{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/t/tiendat.ng.cs/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelWithLMHead, AutoModelForTokenClassification_Soft_NER\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../BERTOverflow_stackoverflow_github\")\n",
    "model = AutoModelForTokenClassification_Soft_NER.from_pretrained(\"../BERTOverflow_stackoverflow_github\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification_Soft_NER(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(82000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (word_GRU): GRU(768, 300, batch_first=True, bidirectional=True)\n",
       "  (w_proj): Linear(in_features=600, out_features=600, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (seq_GRU): GRU(600, 300, batch_first=True, bidirectional=True)\n",
       "  (seq_proj): Linear(in_features=600, out_features=600, bias=True)\n",
       "  (classifier): Linear(in_features=600, out_features=51, bias=True)\n",
       "  (ctc_embed): Embedding(60, 768)\n",
       "  (seg_embed): Embedding(60, 768)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= \"The reference assemblies for framework “.NETFramework,Version=v4.6.2” were not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.type of BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(82000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=82000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_model.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(82000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=82000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = stack_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = stack_tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = stack_model(torch.tensor([ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.6212, -0.6841,  0.0089,  ...,  0.4118,  0.0073, -0.1091],\n",
       "         [ 0.3366, -0.3322,  0.1757,  ...,  0.4519, -0.1361,  0.4731],\n",
       "         [ 0.1994, -0.3740,  0.7420,  ...,  0.8055,  0.3258, -0.5597],\n",
       "         ...,\n",
       "         [ 0.5074, -0.2760,  0.3783,  ...,  1.0345,  0.4022, -0.8400],\n",
       "         [ 0.2656,  0.1407,  0.9754,  ...,  0.3641, -0.3403, -0.3264],\n",
       "         [ 0.3877,  0.2662,  0.0895,  ...,  0.3911,  0.4634, -0.5508]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=(tensor([[[ 0.5826,  1.2725, -2.2040,  ...,  0.0879,  0.6008, -0.1014],\n",
       "         [ 0.0746,  0.6503, -1.0044,  ...,  0.5248, -1.2233,  0.3976],\n",
       "         [ 0.4641,  0.6837, -0.7479,  ...,  1.6925, -0.3316, -0.4140],\n",
       "         ...,\n",
       "         [ 0.3485, -2.3452, -0.1426,  ...,  0.6342, -1.1082,  0.9578],\n",
       "         [-0.1695,  0.9866, -1.3703,  ..., -1.0514, -1.6212,  0.4190],\n",
       "         [ 2.2172,  0.3095, -1.3905,  ..., -0.7270, -1.7038,  0.4484]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.5707e-01,  1.6033e+00, -1.5104e+00,  ...,  4.6506e-01,\n",
       "           7.5264e-01,  3.6938e-01],\n",
       "         [-5.1776e-06,  1.1819e+00, -2.9825e-01,  ...,  1.1768e+00,\n",
       "          -1.1076e+00,  6.8610e-01],\n",
       "         [ 1.5116e-01,  7.9065e-01, -5.8416e-02,  ...,  2.3042e+00,\n",
       "           3.9056e-01, -8.7273e-02],\n",
       "         ...,\n",
       "         [ 3.6619e-01, -1.9992e+00,  2.3644e-01,  ...,  1.2495e+00,\n",
       "          -5.3545e-01,  1.5404e+00],\n",
       "         [-2.0524e-01,  1.2806e+00, -1.1726e+00,  ..., -9.0036e-02,\n",
       "          -1.1462e+00,  8.7234e-01],\n",
       "         [ 1.4592e+00,  4.3789e-01, -8.1127e-01,  ..., -3.9981e-01,\n",
       "          -1.4585e+00,  1.3526e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.5393e-02,  6.0550e-01, -1.3282e+00,  ...,  7.7574e-01,\n",
       "           9.7746e-01,  1.7498e-01],\n",
       "         [ 1.8539e-01,  2.4348e-01,  7.4770e-02,  ...,  1.1296e+00,\n",
       "          -7.6300e-01,  5.9560e-01],\n",
       "         [ 1.9434e-01,  1.4361e-01, -1.7909e-03,  ...,  2.1810e+00,\n",
       "           9.1238e-01, -7.4390e-02],\n",
       "         ...,\n",
       "         [-1.5336e-01, -2.1515e+00,  5.1082e-01,  ...,  8.5225e-01,\n",
       "           2.2918e-01,  1.4860e+00],\n",
       "         [-3.9198e-01,  9.4841e-01, -8.0316e-01,  ...,  1.0997e-01,\n",
       "          -3.4881e-01,  1.0155e+00],\n",
       "         [ 1.2734e+00,  4.2469e-01, -3.3266e-01,  ..., -7.5049e-01,\n",
       "          -1.1508e+00,  1.2147e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3071,  0.5956, -0.8944,  ...,  0.1835,  1.1143, -0.0135],\n",
       "         [ 0.2148,  0.3835,  0.3918,  ...,  0.5824, -0.4088,  0.4489],\n",
       "         [ 0.1976,  0.0189,  0.3449,  ...,  2.4178,  0.6179, -0.2411],\n",
       "         ...,\n",
       "         [-0.0532, -2.2645,  0.8167,  ...,  0.5938, -0.0236,  1.5127],\n",
       "         [ 0.0750,  1.1737, -0.1749,  ...,  0.0785,  0.0573,  0.9293],\n",
       "         [ 1.0706,  0.5384, -0.2097,  ..., -0.6379, -0.7244,  0.9577]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6798,  0.4886, -1.2706,  ...,  0.3774,  1.8559, -0.0251],\n",
       "         [ 0.6215, -0.0374,  0.3274,  ...,  0.6860,  0.2887,  0.7021],\n",
       "         [ 0.5790, -0.0352,  0.0282,  ...,  1.9622,  1.5932,  0.2317],\n",
       "         ...,\n",
       "         [ 0.4086, -2.4418,  0.6056,  ...,  0.8212,  0.8283,  1.2383],\n",
       "         [ 0.2014,  1.1719, -0.1465,  ...,  0.3294,  1.0901,  1.2988],\n",
       "         [ 1.2716, -0.1498, -0.6073,  ...,  0.1112,  0.0901,  0.7965]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1800,  0.2770, -1.3753,  ...,  0.1500,  2.5043, -0.3475],\n",
       "         [ 0.4326, -0.0033,  0.9385,  ...,  0.1273,  0.5183,  0.8928],\n",
       "         [-0.0249,  0.1396,  0.5396,  ...,  1.2968,  1.6561,  0.4118],\n",
       "         ...,\n",
       "         [ 0.2414, -2.1858,  1.2056,  ...,  0.4423,  0.7660,  0.8928],\n",
       "         [-0.0644,  0.9123, -0.1720,  ...,  0.1248,  1.6148,  0.7029],\n",
       "         [ 0.6680, -0.2623,  0.1170,  ..., -0.4130,  0.6534,  0.6180]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3469, -0.1229, -2.0642,  ...,  0.5483,  1.7096, -1.0074],\n",
       "         [ 0.2458, -0.1032,  0.3088,  ...,  0.6613,  0.0994,  0.7779],\n",
       "         [ 0.0307,  0.1205, -0.1196,  ...,  1.5253,  1.0829,  0.1169],\n",
       "         ...,\n",
       "         [-0.2211, -2.2368,  0.5374,  ...,  0.4968, -0.2575,  0.6361],\n",
       "         [ 0.0737,  1.0980, -0.9990,  ...,  0.2585,  1.0527,  0.2956],\n",
       "         [ 0.1311, -0.7544, -0.3219,  ..., -0.0512,  0.1768,  0.3927]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7135,  0.5769, -1.6639,  ...,  0.1750,  0.7845, -0.9054],\n",
       "         [-0.6679,  0.0929,  0.5167,  ...,  0.2052, -0.5884,  0.8991],\n",
       "         [-0.8691,  0.1004,  0.0141,  ...,  1.0366,  0.3472,  0.1010],\n",
       "         ...,\n",
       "         [-0.9300, -2.0164,  1.0040,  ...,  0.3112, -1.0747,  0.4859],\n",
       "         [-0.8233,  1.5923, -0.4814,  ...,  0.1232,  0.1631,  0.3356],\n",
       "         [-0.2122, -0.3077, -0.5381,  ...,  0.0332, -0.3219,  0.7556]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2136,  0.8291, -1.3892,  ...,  1.0448,  0.6657, -0.8633],\n",
       "         [-0.3575,  0.5263,  0.7999,  ...,  1.1366, -0.7850,  0.5523],\n",
       "         [-1.0089,  0.2251, -0.1252,  ...,  1.2892,  0.0524,  0.2608],\n",
       "         ...,\n",
       "         [-0.6861, -1.5252,  0.6952,  ...,  1.3373, -1.0593,  0.1985],\n",
       "         [-0.9984,  1.6028,  0.0203,  ...,  0.6433,  0.2943,  0.4317],\n",
       "         [-0.2675, -0.5040, -0.7158,  ...,  0.5535, -0.3978,  1.0042]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 5.4116e-01,  5.0051e-01, -1.3672e+00,  ...,  1.3981e+00,\n",
       "           1.4669e-01, -1.0929e+00],\n",
       "         [ 6.8982e-01, -2.1233e-02,  9.2119e-01,  ...,  1.4461e+00,\n",
       "          -1.1724e+00,  1.8398e-01],\n",
       "         [-4.7512e-01,  1.0068e-03, -3.9346e-01,  ...,  2.0930e+00,\n",
       "          -3.3908e-01, -3.1376e-02],\n",
       "         ...,\n",
       "         [-7.5604e-01, -1.5983e+00,  6.8133e-01,  ...,  1.5283e+00,\n",
       "          -1.1264e+00,  1.9528e-01],\n",
       "         [-3.3274e-01,  1.0137e+00, -2.2490e-02,  ...,  6.0937e-01,\n",
       "          -9.8054e-02,  3.2609e-01],\n",
       "         [ 1.0086e+00, -1.2000e+00, -5.7578e-01,  ...,  1.0912e+00,\n",
       "          -7.6097e-01,  8.6276e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3531,  0.2221, -0.4690,  ...,  1.8981, -0.2725, -1.3017],\n",
       "         [ 0.8909, -0.3883,  1.3149,  ...,  1.8606, -0.9434, -0.1105],\n",
       "         [-0.6058, -0.1729,  0.1073,  ...,  2.5335, -0.3548,  0.1211],\n",
       "         ...,\n",
       "         [-0.5883, -1.1885,  1.1596,  ...,  2.1777, -1.1202,  0.0792],\n",
       "         [-0.4310,  0.9832,  0.7544,  ...,  1.1678,  0.3252,  0.3565],\n",
       "         [ 0.8983, -0.9804, -0.1301,  ...,  1.8609, -0.8429,  0.7843]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1508,  0.3157, -0.8029,  ...,  1.3022, -0.1830, -1.2819],\n",
       "         [ 0.9705, -0.3765,  1.0771,  ...,  1.6091, -1.0451, -0.1878],\n",
       "         [-0.5748,  0.1340, -0.3187,  ...,  2.0886, -0.3306, -0.0223],\n",
       "         ...,\n",
       "         [-0.2394, -0.4309,  0.9546,  ...,  1.9835, -1.2747,  0.0557],\n",
       "         [-0.1409,  0.7835,  0.3925,  ...,  1.2959,  0.1551,  0.1231],\n",
       "         [ 0.7948, -0.6514, -0.1160,  ...,  1.8846, -0.8159,  0.3710]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1529,  0.7931, -0.9391,  ...,  1.0976, -0.0689, -0.3161],\n",
       "         [ 0.7196,  0.2165,  1.2714,  ...,  1.3918, -0.1290,  0.1834],\n",
       "         [-0.7461,  0.4914, -0.1576,  ...,  1.4515, -0.1405,  0.5162],\n",
       "         ...,\n",
       "         [-0.4684,  0.3063,  1.1283,  ...,  1.1666, -0.8184, -0.0296],\n",
       "         [-0.2146,  0.8334,  0.4995,  ...,  0.7838,  0.4824,  0.5879],\n",
       "         [ 0.4464, -0.3551, -0.1067,  ...,  1.4097, -0.5406,  0.7569]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(prediction[0, masked_index]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.6212, -0.6841,  0.0089,  ...,  0.4118,  0.0073, -0.1091],\n",
       "         [ 0.3366, -0.3322,  0.1757,  ...,  0.4519, -0.1361,  0.4731],\n",
       "         [ 0.1994, -0.3740,  0.7420,  ...,  0.8055,  0.3258, -0.5597],\n",
       "         ...,\n",
       "         [ 0.5074, -0.2760,  0.3783,  ...,  1.0345,  0.4022, -0.8400],\n",
       "         [ 0.2656,  0.1407,  0.9754,  ...,  0.3641, -0.3403, -0.3264],\n",
       "         [ 0.3877,  0.2662,  0.0895,  ...,  0.3911,  0.4634, -0.5508]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=(tensor([[[ 0.5826,  1.2725, -2.2040,  ...,  0.0879,  0.6008, -0.1014],\n",
       "         [ 0.0746,  0.6503, -1.0044,  ...,  0.5248, -1.2233,  0.3976],\n",
       "         [ 0.4641,  0.6837, -0.7479,  ...,  1.6925, -0.3316, -0.4140],\n",
       "         ...,\n",
       "         [ 0.3485, -2.3452, -0.1426,  ...,  0.6342, -1.1082,  0.9578],\n",
       "         [-0.1695,  0.9866, -1.3703,  ..., -1.0514, -1.6212,  0.4190],\n",
       "         [ 2.2172,  0.3095, -1.3905,  ..., -0.7270, -1.7038,  0.4484]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.5707e-01,  1.6033e+00, -1.5104e+00,  ...,  4.6506e-01,\n",
       "           7.5264e-01,  3.6938e-01],\n",
       "         [-5.1776e-06,  1.1819e+00, -2.9825e-01,  ...,  1.1768e+00,\n",
       "          -1.1076e+00,  6.8610e-01],\n",
       "         [ 1.5116e-01,  7.9065e-01, -5.8416e-02,  ...,  2.3042e+00,\n",
       "           3.9056e-01, -8.7273e-02],\n",
       "         ...,\n",
       "         [ 3.6619e-01, -1.9992e+00,  2.3644e-01,  ...,  1.2495e+00,\n",
       "          -5.3545e-01,  1.5404e+00],\n",
       "         [-2.0524e-01,  1.2806e+00, -1.1726e+00,  ..., -9.0036e-02,\n",
       "          -1.1462e+00,  8.7234e-01],\n",
       "         [ 1.4592e+00,  4.3789e-01, -8.1127e-01,  ..., -3.9981e-01,\n",
       "          -1.4585e+00,  1.3526e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.5393e-02,  6.0550e-01, -1.3282e+00,  ...,  7.7574e-01,\n",
       "           9.7746e-01,  1.7498e-01],\n",
       "         [ 1.8539e-01,  2.4348e-01,  7.4770e-02,  ...,  1.1296e+00,\n",
       "          -7.6300e-01,  5.9560e-01],\n",
       "         [ 1.9434e-01,  1.4361e-01, -1.7909e-03,  ...,  2.1810e+00,\n",
       "           9.1238e-01, -7.4390e-02],\n",
       "         ...,\n",
       "         [-1.5336e-01, -2.1515e+00,  5.1082e-01,  ...,  8.5225e-01,\n",
       "           2.2918e-01,  1.4860e+00],\n",
       "         [-3.9198e-01,  9.4841e-01, -8.0316e-01,  ...,  1.0997e-01,\n",
       "          -3.4881e-01,  1.0155e+00],\n",
       "         [ 1.2734e+00,  4.2469e-01, -3.3266e-01,  ..., -7.5049e-01,\n",
       "          -1.1508e+00,  1.2147e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3071,  0.5956, -0.8944,  ...,  0.1835,  1.1143, -0.0135],\n",
       "         [ 0.2148,  0.3835,  0.3918,  ...,  0.5824, -0.4088,  0.4489],\n",
       "         [ 0.1976,  0.0189,  0.3449,  ...,  2.4178,  0.6179, -0.2411],\n",
       "         ...,\n",
       "         [-0.0532, -2.2645,  0.8167,  ...,  0.5938, -0.0236,  1.5127],\n",
       "         [ 0.0750,  1.1737, -0.1749,  ...,  0.0785,  0.0573,  0.9293],\n",
       "         [ 1.0706,  0.5384, -0.2097,  ..., -0.6379, -0.7244,  0.9577]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6798,  0.4886, -1.2706,  ...,  0.3774,  1.8559, -0.0251],\n",
       "         [ 0.6215, -0.0374,  0.3274,  ...,  0.6860,  0.2887,  0.7021],\n",
       "         [ 0.5790, -0.0352,  0.0282,  ...,  1.9622,  1.5932,  0.2317],\n",
       "         ...,\n",
       "         [ 0.4086, -2.4418,  0.6056,  ...,  0.8212,  0.8283,  1.2383],\n",
       "         [ 0.2014,  1.1719, -0.1465,  ...,  0.3294,  1.0901,  1.2988],\n",
       "         [ 1.2716, -0.1498, -0.6073,  ...,  0.1112,  0.0901,  0.7965]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1800,  0.2770, -1.3753,  ...,  0.1500,  2.5043, -0.3475],\n",
       "         [ 0.4326, -0.0033,  0.9385,  ...,  0.1273,  0.5183,  0.8928],\n",
       "         [-0.0249,  0.1396,  0.5396,  ...,  1.2968,  1.6561,  0.4118],\n",
       "         ...,\n",
       "         [ 0.2414, -2.1858,  1.2056,  ...,  0.4423,  0.7660,  0.8928],\n",
       "         [-0.0644,  0.9123, -0.1720,  ...,  0.1248,  1.6148,  0.7029],\n",
       "         [ 0.6680, -0.2623,  0.1170,  ..., -0.4130,  0.6534,  0.6180]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3469, -0.1229, -2.0642,  ...,  0.5483,  1.7096, -1.0074],\n",
       "         [ 0.2458, -0.1032,  0.3088,  ...,  0.6613,  0.0994,  0.7779],\n",
       "         [ 0.0307,  0.1205, -0.1196,  ...,  1.5253,  1.0829,  0.1169],\n",
       "         ...,\n",
       "         [-0.2211, -2.2368,  0.5374,  ...,  0.4968, -0.2575,  0.6361],\n",
       "         [ 0.0737,  1.0980, -0.9990,  ...,  0.2585,  1.0527,  0.2956],\n",
       "         [ 0.1311, -0.7544, -0.3219,  ..., -0.0512,  0.1768,  0.3927]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7135,  0.5769, -1.6639,  ...,  0.1750,  0.7845, -0.9054],\n",
       "         [-0.6679,  0.0929,  0.5167,  ...,  0.2052, -0.5884,  0.8991],\n",
       "         [-0.8691,  0.1004,  0.0141,  ...,  1.0366,  0.3472,  0.1010],\n",
       "         ...,\n",
       "         [-0.9300, -2.0164,  1.0040,  ...,  0.3112, -1.0747,  0.4859],\n",
       "         [-0.8233,  1.5923, -0.4814,  ...,  0.1232,  0.1631,  0.3356],\n",
       "         [-0.2122, -0.3077, -0.5381,  ...,  0.0332, -0.3219,  0.7556]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2136,  0.8291, -1.3892,  ...,  1.0448,  0.6657, -0.8633],\n",
       "         [-0.3575,  0.5263,  0.7999,  ...,  1.1366, -0.7850,  0.5523],\n",
       "         [-1.0089,  0.2251, -0.1252,  ...,  1.2892,  0.0524,  0.2608],\n",
       "         ...,\n",
       "         [-0.6861, -1.5252,  0.6952,  ...,  1.3373, -1.0593,  0.1985],\n",
       "         [-0.9984,  1.6028,  0.0203,  ...,  0.6433,  0.2943,  0.4317],\n",
       "         [-0.2675, -0.5040, -0.7158,  ...,  0.5535, -0.3978,  1.0042]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 5.4116e-01,  5.0051e-01, -1.3672e+00,  ...,  1.3981e+00,\n",
       "           1.4669e-01, -1.0929e+00],\n",
       "         [ 6.8982e-01, -2.1233e-02,  9.2119e-01,  ...,  1.4461e+00,\n",
       "          -1.1724e+00,  1.8398e-01],\n",
       "         [-4.7512e-01,  1.0068e-03, -3.9346e-01,  ...,  2.0930e+00,\n",
       "          -3.3908e-01, -3.1376e-02],\n",
       "         ...,\n",
       "         [-7.5604e-01, -1.5983e+00,  6.8133e-01,  ...,  1.5283e+00,\n",
       "          -1.1264e+00,  1.9528e-01],\n",
       "         [-3.3274e-01,  1.0137e+00, -2.2490e-02,  ...,  6.0937e-01,\n",
       "          -9.8054e-02,  3.2609e-01],\n",
       "         [ 1.0086e+00, -1.2000e+00, -5.7578e-01,  ...,  1.0912e+00,\n",
       "          -7.6097e-01,  8.6276e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3531,  0.2221, -0.4690,  ...,  1.8981, -0.2725, -1.3017],\n",
       "         [ 0.8909, -0.3883,  1.3149,  ...,  1.8606, -0.9434, -0.1105],\n",
       "         [-0.6058, -0.1729,  0.1073,  ...,  2.5335, -0.3548,  0.1211],\n",
       "         ...,\n",
       "         [-0.5883, -1.1885,  1.1596,  ...,  2.1777, -1.1202,  0.0792],\n",
       "         [-0.4310,  0.9832,  0.7544,  ...,  1.1678,  0.3252,  0.3565],\n",
       "         [ 0.8983, -0.9804, -0.1301,  ...,  1.8609, -0.8429,  0.7843]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1508,  0.3157, -0.8029,  ...,  1.3022, -0.1830, -1.2819],\n",
       "         [ 0.9705, -0.3765,  1.0771,  ...,  1.6091, -1.0451, -0.1878],\n",
       "         [-0.5748,  0.1340, -0.3187,  ...,  2.0886, -0.3306, -0.0223],\n",
       "         ...,\n",
       "         [-0.2394, -0.4309,  0.9546,  ...,  1.9835, -1.2747,  0.0557],\n",
       "         [-0.1409,  0.7835,  0.3925,  ...,  1.2959,  0.1551,  0.1231],\n",
       "         [ 0.7948, -0.6514, -0.1160,  ...,  1.8846, -0.8159,  0.3710]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1529,  0.7931, -0.9391,  ...,  1.0976, -0.0689, -0.3161],\n",
       "         [ 0.7196,  0.2165,  1.2714,  ...,  1.3918, -0.1290,  0.1834],\n",
       "         [-0.7461,  0.4914, -0.1576,  ...,  1.4515, -0.1405,  0.5162],\n",
       "         ...,\n",
       "         [-0.4684,  0.3063,  1.1283,  ...,  1.1666, -0.8184, -0.0296],\n",
       "         [-0.2146,  0.8334,  0.4995,  ...,  0.7838,  0.4824,  0.5879],\n",
       "         [ 0.4464, -0.3551, -0.1067,  ...,  1.4097, -0.5406,  0.7569]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
