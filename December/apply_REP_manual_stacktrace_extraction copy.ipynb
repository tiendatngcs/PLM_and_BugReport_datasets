{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created Dec 20th 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/t/tiendat.ng.cs/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "import pynndescent\n",
    "\n",
    "import sqlite3\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import sys; sys.path.insert(0, '..')\n",
    "from itertools import combinations, chain\n",
    "import random\n",
    "import my_utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}  # Dictionary to store parent nodes\n",
    "        self.ranks = {}    # Dictionary to store rank (or size) of each set\n",
    "        self.processed = False\n",
    "        self.project_name = None\n",
    "\n",
    "    def find(self, x):\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.ranks[x] = 1\n",
    "            return x\n",
    "\n",
    "        # Path compression\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "\n",
    "        if root_x != root_y:\n",
    "            if self.ranks[root_x] < self.ranks[root_y]:\n",
    "                self.parent[root_x] = root_y\n",
    "                self.ranks[root_y] += self.ranks[root_x]\n",
    "            else:\n",
    "                self.parent[root_y] = root_x\n",
    "                self.ranks[root_x] += self.ranks[root_y]\n",
    "            \n",
    "    def process_project(self, conn, project_name):\n",
    "        cursor = conn.cursor()\n",
    "        self.project_name = project_name\n",
    "        print(\"Processing\", project_name)\n",
    "        cursor.execute(f\"SELECT * FROM {project_name}\")\n",
    "        for row in cursor.fetchall():\n",
    "            dup_id = int(row[column_names.index(\"dup_id\")])\n",
    "            if dup_id == -1: continue\n",
    "            bug_id = int(row[column_names.index(\"bug_id\")])\n",
    "            assert(dup_id != bug_id)\n",
    "            self.union(bug_id, dup_id)\n",
    "        self.processed = True\n",
    "            \n",
    "    def get_roots(self,):\n",
    "        assert(self.processed)\n",
    "        return list(set(self.parent.values()))\n",
    "    \n",
    "    def get_children(self, parent):\n",
    "        assert(self.processed)\n",
    "        parent = self.find(parent)\n",
    "        children = [key for key, value in self.parent.items() if value == parent]\n",
    "        return children\n",
    "    \n",
    "    def get_all_children(self, ):\n",
    "        return [key for key, value in self.parent.items()]\n",
    "    \n",
    "    def are_dups(this, bug_id1, bug_id2):\n",
    "        if (bug_id1 not in this.parent.keys() or bug_id2 not in this.parent.keys()):\n",
    "            return False\n",
    "        return this.parent[bug_id1] == this.parent[bug_id2]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bug_ids(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    column_name = \"bug_id\"\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    cursor.execute(f\"SELECT DISTINCT {column_name} FROM {table_name} ORDER BY {column_name};\")\n",
    "    distinct_values_sorted = cursor.fetchall()\n",
    "\n",
    "    # Extract table names from the result\n",
    "    return [value[0] for value in distinct_values_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a query to get information about the columns in the specified table\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    columns_info = cursor.fetchall()\n",
    "\n",
    "    # Extract and return the column names\n",
    "    column_names = [column[1] for column in columns_info]\n",
    "    return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_feature(conn, project_name, bug_id):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    query = f\"SELECT * FROM {project_name} WHERE bug_id = {bug_id};\"\n",
    "    # print(query)\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()[0]\n",
    "    return result[column_names.index(\"code_feature\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptions(conn, project_name, bug_id):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    query = f\"SELECT * FROM {project_name} WHERE bug_id = {bug_id};\"\n",
    "    # print(query)\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()[0]\n",
    "    desc = result[column_names.index(\"description\")]\n",
    "    short_desc = result[column_names.index(\"short_desc\")]\n",
    "\n",
    "    # Extract table names from the result\n",
    "    return (desc + \" \\n \" + short_desc).replace(\"\\\\'\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(description, stride_len, chunk_size):\n",
    "    tokens = tokenizer.tokenize(description)\n",
    "    # if len og token array is < 32, we do nothing as there is not enough information\n",
    "    if (len(tokens) < chunk_size // 2): return None\n",
    "\n",
    "    # remember to add cls and sep token at each chunk\n",
    "    token_ids = tokenizer.convert_tokens_to_ids([tokenizer.cls_token]+tokens+[tokenizer.sep_token])\n",
    "\n",
    "    # divide token ids into batche of chunks\n",
    "    chunk_list=[]\n",
    "    for i in range(0, len(token_ids), stride_len):\n",
    "        chunk = token_ids[i:min(i+chunk_size, len(token_ids))]\n",
    "        assert(len(chunk) <= chunk_size)\n",
    "        if len(chunk) < chunk_size:\n",
    "            # keep going\n",
    "            continue\n",
    "            # if (len(chunk) < chunk_size // 2): continue\n",
    "            # pad_length = chunk_size - len(chunk)\n",
    "            # chunk += [tokenizer.pad_token_id]*pad_length\n",
    "        assert(len(chunk) == chunk_size)\n",
    "        # print(chunk)\n",
    "        chunk_list.append(chunk)\n",
    "\n",
    "    if(len(chunk_list) == 0): return None\n",
    "    chunk_arr = np.array(chunk_list)\n",
    "    # print(\"Chunk arr size{}\".format(chunk_arr.shape))\n",
    "    # context_embedding = model(torch.tensor(token_ids[:512])[None, :])[0]\n",
    "    context_embedding = model(torch.tensor(chunk_arr)[:, :])[0]\n",
    "    return context_embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicated_pairs(union_find):\n",
    "    roots = union_find.get_roots()\n",
    "    pairs = []\n",
    "    for root in roots:\n",
    "        group = union_find.get_children(root)\n",
    "        pairs += list(combinations(group, 2))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_duplicated_pairs(union_find, conn, size):\n",
    "    from_dup = union_find.get_all_children()\n",
    "    #sample in some other single reports\n",
    "    assert(union_find.processed)\n",
    "    samples = random.sample(get_bug_ids(conn, union_find.project_name), len(from_dup))\n",
    "    \n",
    "    pairs = []\n",
    "    count = 0\n",
    "    while (count < size):\n",
    "        pair = random.sample(samples, 2)\n",
    "        if pair[0] == pair[1] or union_find.are_dups(pair[0], pair[1]):\n",
    "            continue\n",
    "        pairs += [(pair[0], pair[1]),]\n",
    "        count += 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mislabels(union_find, bug_ids, anchor_bug_id, threshold):\n",
    "    assert(threshold >= 0 and threshold <= 1)\n",
    "    ret = []\n",
    "    for bug_id in tqdm(bug_ids):\n",
    "        if not union_find.are_dups(anchor_bug_id, bug_id):\n",
    "            sim_score = get_similarity_of_pair((anchor_bug_id, bug_id),)\n",
    "            if sim_score > threshold:\n",
    "                ret += [bug_id]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_similarity_of_pair_with_code_feature(conn, project_name, pair):\n",
    "    sent0 = my_utils.get_code_feature(conn, project_name, pair[0])\n",
    "    sent1 = my_utils.get_code_feature(conn, project_name, pair[1])\n",
    "    sent_embedding0 = model.encode(sent0,convert_to_tensor=True)\n",
    "    sent_embedding1 = model.encode(sent1,convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(sent_embedding0, sent_embedding1).numpy()[0, 0]\n",
    "\n",
    "\n",
    "def get_similarity_of_pair_with_desc(conn, project_name, pair):\n",
    "    sent0 = my_utils.get_descriptions(conn, project_name, pair[0])\n",
    "    sent1 = my_utils.get_descriptions(conn, project_name, pair[1])\n",
    "    sent_embedding0 = model.encode(sent0,convert_to_tensor=True)\n",
    "    sent_embedding1 = model.encode(sent1,convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(sent_embedding0, sent_embedding1).numpy()[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stacktrace(text):\n",
    "    # stack_trace_pattern = re.compile(r'\\n(Caused\\sby:\\s)*\\w+\\.(\\w+\\.)+\\w+.*?\\n(?:[ \\t]+[at]*.*\\n)+', re.DOTALL)\n",
    "    # stack_trace_pattern = re.compile(r\"\\n(?:Caused by: )*(?:[a-zA-Z]+\\.)+(?:[a-zA-Z]+:*.*)\\n(.*\\n)*\\n(?:[ \\t]+[at]*.*\\n)*\", re.DOTALL)\n",
    "    \n",
    "    # stack_trace_pattern = re.compile(r\"\\n\\d*\\s*(?:Caused by: )*(?:[a-zA-Z]+\\.)+(?:[a-zA-Z]+:*.*)\\n(.*\\n)*\\n*(?:\\d*\\s*[ \\t]+[at]*.*\\n)*\", re.DOTALL)\n",
    "    \n",
    "    caused_by_pattern = re.compile(r\"\\n\\d*\\s*(?:Caused by: )*(?:[a-zA-Z]+\\.)+[a-zA-Z]+.*\", re.DOTALL)\n",
    "    at_pattern = re.compile(r\"\\s*\\t*at (?:.+\\.)+.+.*\", re.DOTALL)\n",
    "    \n",
    "    result_text = caused_by_pattern.sub('', text)\n",
    "    result_text = at_pattern.sub('', text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_java_path(word):\n",
    "    is_long = len(word) >= 15\n",
    "    has_dots = word.count('.') >= 4\n",
    "    return is_long or has_dots\n",
    "\n",
    "def contains_java_path(line):\n",
    "    for word in line.split(\" \"):\n",
    "        if is_java_path(word): return True\n",
    "    return False\n",
    "\n",
    "def java_path_is_majority(line):\n",
    "    total_java_path_length = 0\n",
    "    for word in line.split(\" \"):\n",
    "        if is_java_path(word): total_java_path_length += len(word)\n",
    "    return total_java_path_length / len(line) > 0.5\n",
    "\n",
    "def startswith_datetime(line):\n",
    "    first_word = line.split(\" \")[0]\n",
    "    match1 = re.match(r\"\\d+-\\d+-\\d+\", first_word) is not None\n",
    "    match2 = re.match(r\"\\d+:\\d+:\\d+.*\", first_word) is not None\n",
    "    return match1 or match2\n",
    "\n",
    "def get_tag_name(line):\n",
    "    pattern = re.compile(r'^\\s*<([a-zA-Z][^\\s>]*)\\s*[^>]*>')\n",
    "    match = pattern.match(line)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def is_stacktrace_more(line):\n",
    "    match = re.match(r\"... \\d+ more\", line.strip()) is not None\n",
    "    return match\n",
    "\n",
    "def segregate_log_and_stacktrace(text):\n",
    "    eng = \"\"\n",
    "    log_and_stacktrace = \"\"\n",
    "    \n",
    "    for line in text.split(\"\\n\"):\n",
    "        if get_tag_name(line) is not None:\n",
    "            eng += line + \"\\n\"\n",
    "            continue\n",
    "        if len(line) != 0\\\n",
    "            and (line.startswith(\"at \")\\\n",
    "            or line.startswith(\"Caused by: \")\\\n",
    "            or java_path_is_majority(line)\\\n",
    "            or startswith_datetime(line)\n",
    "            or is_stacktrace_more(line)):\n",
    "            log_and_stacktrace += line + \"\\n\"\n",
    "        else:\n",
    "            eng += line + \"\\n\"\n",
    "    return eng.strip(), log_and_stacktrace.strip()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_of_pair_with_desc(conn, project_name, pair):\n",
    "    sent0 = my_utils.get_descriptions(conn, project_name, pair[0])\n",
    "    sent1 = my_utils.get_descriptions(conn, project_name, pair[1])\n",
    "    sent_embedding0 = model.encode(sent0,convert_to_tensor=True)\n",
    "    sent_embedding1 = model.encode(sent1,convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(sent_embedding0, sent_embedding1).numpy()[0, 0]\n",
    "\n",
    "def get_similarity_of_pair_with_desc_no_stacktrace(conn, project_name, pair):\n",
    "    sent0 = remove_stacktrace(my_utils.get_descriptions(conn, project_name, pair[0]))\n",
    "    sent1 = remove_stacktrace(my_utils.get_descriptions(conn, project_name, pair[1]))\n",
    "    sent_embedding0 = model.encode(sent0,convert_to_tensor=True)\n",
    "    sent_embedding1 = model.encode(sent1,convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(sent_embedding0, sent_embedding1).numpy()[0, 0]\n",
    "\n",
    "def load_content_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_similarity_of_pair_with_desc_no_stacktrace_from_file(folder, pair):\n",
    "    file_name1 = str(pair[0]) + \".txt\"\n",
    "    file_name2 = str(pair[1]) + \".txt\"\n",
    "    file_path1 = os.path.join(folder, file_name1)\n",
    "    file_path2 = os.path.join(folder, file_name2)\n",
    "    \n",
    "    sent0 = load_content_from_file(file_path1)\n",
    "    sent1 = load_content_from_file(file_path2)\n",
    "    sent_embedding0 = model.encode(sent0,convert_to_tensor=True)\n",
    "    sent_embedding1 = model.encode(sent1,convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(sent_embedding0, sent_embedding1).numpy()[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"../dbrd_processed.db\"\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hadoop_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24083/24083 [00:00<00:00, 69365.02it/s]\n"
     ]
    }
   ],
   "source": [
    "table = \"hadoop_old\"\n",
    "union_find = my_utils.UnionFind()\n",
    "union_find.process_project(conn, table, min_desc_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of bug_ids that has stacktrace\n",
    "bug_ids = my_utils.get_bug_ids(conn, table)\n",
    "bug_ids_w_stacktrace = [bug_id for bug_id in bug_ids if len(my_utils.get_stacktrace(conn, table, bug_id)) != 0]\n",
    "\n",
    "# bug ids what has duplicates\n",
    "bug_ids_w_duplicates = union_find.get_all_children()\n",
    "\n",
    "# intersection, bug_ids that has duplicates and stactrace\n",
    "bug_ids_w_duplicates_and_stacktrace = list(set(bug_ids_w_duplicates).intersection(set(bug_ids_w_stacktrace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bug_ids_w_duplicates_and_stacktrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3771"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bug_ids_w_stacktrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs_with_stacktrace = []\n",
    "seen = []\n",
    "for bug_id in bug_ids_w_duplicates_and_stacktrace:\n",
    "    if bug_id in seen: continue\n",
    "    children = union_find.get_children(bug_id)\n",
    "    assert(bug_id in children)\n",
    "    pairs = list(combinations(children, 2))\n",
    "    for bug_id1, bug_id2 in pairs:\n",
    "        if bug_id1 in bug_ids_w_duplicates_and_stacktrace and bug_id2 in bug_ids_w_duplicates_and_stacktrace:\n",
    "            positive_pairs_with_stacktrace.append((bug_id1, bug_id2))\n",
    "    seen += children\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_pairs_with_stacktrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bug_ids_w_duplicates_and_stacktrace = list(set(chain.from_iterable(positive_pairs_with_stacktrace)))\n",
    "final_bug_ids_w_duplicates_and_stacktrace.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_bug_ids_w_duplicates_and_stacktrace = list(set(chain.from_iterable(positive_pairs_with_stacktrace[:20])))\n",
    "partial_bug_ids_w_duplicates_and_stacktrace.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_pairs(union_find, bug_ids, sample_size):\n",
    "    all_pairs = combinations(bug_ids, 2)\n",
    "    negative_pairs = []\n",
    "    for pair in all_pairs:\n",
    "        if (not union_find.are_dups(pair[0], pair[1])):\n",
    "            negative_pairs.append(pair)\n",
    "    if (sample_size == -1):\n",
    "        return negative_pairs\n",
    "    return random.sample(negative_pairs, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_positive_pairs_with_stacktrace = positive_pairs_with_stacktrace[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_negative_pairs_with_stacktrace = get_negative_pairs(union_find, partial_bug_ids_w_duplicates_and_stacktrace, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partial_positive_pairs_with_stacktrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partial_negative_pairs_with_stacktrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPREDUCE-3617 addresses the default values of yarn.nodemanager.principal and yarn.resourcemanager.principal\n",
      "I have enabled authorization with simple authentication. NM <=> RM still attempts kerberos authentication. If simple authentication is enabled yarn.nodemanager.principal and yarn.resourcemanager.principal values should be ignored and simple authentication should be used.\n",
      "core-site.xml snippet\n",
      "  <property>\n",
      "    <name>hadoop.security.authentication</name>\n",
      "    <value>simple</value>\n",
      "    <description></description>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hadoop.security.authorization</name>\n",
      "    <value>true</value>\n",
      "    <description></description>\n",
      "  </property>\n",
      "\n",
      "\n",
      "yarn-site.xml snippet\n",
      "<property>\n",
      "  <description>The Kerberos principal for the resource manager.</description>\n",
      "  <name>yarn.resourcemanager.principal</name>\n",
      "  <value>rm/sightbusy-lx@LOCALHOST</value>\n",
      "</property>\n",
      "<property>\n",
      "  <description>The kerberos principal for the node manager.</description>\n",
      "  <name>yarn.nodemanager.principal</name>\n",
      "  <value>nm/sightbusy-lx@LOCALHOST</value>\n",
      "</property>\n",
      "\n",
      "\n",
      "nodemanager.out snippet\n",
      "2012-01-03 16:40:00,793 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:registerWithRM(176)) - Connected to ResourceManager at machine.example.com:8025\n",
      "2012-01-03 16:40:00,845 ERROR service.CompositeService (CompositeService.java:start(72)) - Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException\n",
      "    at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)\n",
      "    at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n",
      "    at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)\n",
      "    at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)\n",
      "Caused by: java.lang.reflect.UndeclaredThrowableException\n",
      "    at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)\n",
      "    at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)\n",
      "    at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)\n",
      "    ... 3 more\n",
      "Caused by: com.google.protobuf.ServiceException: org.apache.hadoop.security.authorize.AuthorizationException: User user (auth:SIMPLE) is not authorized for protocol interface org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, expected client Kerberos principal is nm/sightbusy-lx@LOCALHOST\n",
      "    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)\n",
      "    at $Proxy24.registerNodeManager(Unknown Source)\n",
      "    at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)\n",
      "    ... 5 more\n",
      "Caused by: org.apache.hadoop.security.authorize.AuthorizationException: User user (auth:SIMPLE) is not authorized for protocol interface org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, expected client Kerberos principal is nm/sightbusy-lx@LOCALHOST\n",
      "    at org.apache.hadoop.ipc.Client.call(Client.java:1085)\n",
      "    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)\n",
      "    ... 7 more\n",
      "2012-01-03 16:40:00,846 WARN  event.AsyncDispatcher (AsyncDispatcher.java:run(78)) - AsyncDispatcher thread interrupted\n",
      "java.lang.InterruptedException\n",
      "    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1961)\n",
      "    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1996)\n",
      "    at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)\n",
      "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:76)\n",
      "    at java.lang.Thread.run(Thread.java:662)\n",
      "2012-01-03 16:40:00,846 INFO  service.AbstractService (AbstractService.java:stop(75)) - Service:Dispatcher is stopped.\n",
      "\n",
      " \n",
      " Authorization of NM <=> RM with simple authentication mistakenly attempts kerberos when yarn.nodemanager.principal is defined\n"
     ]
    }
   ],
   "source": [
    "print(my_utils.get_descriptions(conn, table, 12537362))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have enabled authorization with simple authentication. NM <=> RM still attempts kerberos authentication. If simple authentication is enabled yarn.nodemanager.principal and yarn.resourcemanager.principal values should be ignored and simple authentication should be used.\n",
      "core-site.xml snippet\n",
      "  <property>\n",
      "    <name>hadoop.security.authentication</name>\n",
      "    <value>simple</value>\n",
      "    <description></description>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hadoop.security.authorization</name>\n",
      "    <value>true</value>\n",
      "    <description></description>\n",
      "  </property>\n",
      "\n",
      "\n",
      "yarn-site.xml snippet\n",
      "<property>\n",
      "  <description>The Kerberos principal for the resource manager.</description>\n",
      "  <name>yarn.resourcemanager.principal</name>\n",
      "  <value>rm/sightbusy-lx@LOCALHOST</value>\n",
      "</property>\n",
      "<property>\n",
      "  <description>The kerberos principal for the node manager.</description>\n",
      "  <name>yarn.nodemanager.principal</name>\n",
      "  <value>nm/sightbusy-lx@LOCALHOST</value>\n",
      "</property>\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " Authorization of NM <=> RM with simple authentication mistakenly attempts kerberos when yarn.nodemanager.principal is defined\n"
     ]
    }
   ],
   "source": [
    "print(segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, 12537362))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPREDUCE-3617 addresses the default values of yarn.nodemanager.principal and yarn.resourcemanager.principal\n",
      "I have enabled authorization with simple authentication. NM <=> RM still attempts kerberos authentication. If simple authentication is enabled yarn.nodemanager.principal and yarn.resourcemanager.principal values should be ignored and simple authentication should be used.\n",
      "core-site.xml snippet\n",
      "  <property>\n",
      "    <name>hadoop.security.authentication</name>\n",
      "    <value>simple</value>\n",
      "    <description></description>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>hadoop.security.authorization</name>\n",
      "    <value>true</value>\n",
      "    <description></description>\n",
      "  </property>\n",
      "\n",
      "\n",
      "yarn-site.xml snippet\n",
      "<property>\n",
      "  <description>The Kerberos principal for the resource manager.</description>\n",
      "  <name>yarn.resourcemanager.principal</name>\n",
      "  <value>rm/sightbusy-lx@LOCALHOST</value>\n",
      "</property>\n",
      "<property>\n",
      "  <description>The kerberos principal for the node manager.</description>\n",
      "  <name>yarn.nodemanager.principal</name>\n",
      "  <value>nm/sightbusy-lx@LOCALHOST</value>\n",
      "</property>\n",
      "\n",
      " Authorization of NM <=> RM with simple authentication mistakenly attempts kerberos when yarn.nodemanager.principal is defined\n"
     ]
    }
   ],
   "source": [
    "print(remove_stacktrace(my_utils.get_descriptions(conn, table, 12537362)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12639198, 12537362]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_find.get_children(12537362)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During BigTop 0.6.0 release test cycle, Roman Shaposhnik came around the following problem:\n",
      "\n",
      "013-03-26 15:37:03,573 FATAL\n",
      "org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting\n",
      "NodeManager\n",
      "org.apache.hadoop.yarn.YarnException: Failed to Start\n",
      "org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n",
      "        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:199)\n",
      "        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:322)\n",
      "        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:359)\n",
      "Caused by: org.apache.avro.AvroRuntimeException:\n",
      "java.lang.reflect.UndeclaredThrowableException\n",
      "        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:162)\n",
      "        at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n",
      "        ... 3 more\n",
      "Caused by: java.lang.reflect.UndeclaredThrowableException\n",
      "        at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:128)\n",
      "        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:61)\n",
      "        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:199)\n",
      "        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:158)\n",
      "        ... 4 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException):\n",
      "User yarn/ip-10-46-37-244.ec2.internal@BIGTOP (auth:KERBEROS) is not\n",
      "authorized for protocol interface\n",
      "org.apache.hadoop.yarn.server.api.ResourceTrackerPB, expected client\n",
      "Kerberos principal is yarn/ip-10-46-37-244.ec2.internal@BIGTOP\n",
      "        at org.apache.hadoop.ipc.Client.call(Client.java:1235)\n",
      "        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)\n",
      "        at $Proxy26.registerNodeManager(Unknown Source)\n",
      "        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)\n",
      "        ... 6 more\n",
      "\n",
      "\n",
      "\n",
      "The most significant part is \n",
      "User yarn/ip-10-46-37-244.ec2.internal@BIGTOP (auth:KERBEROS) is not authorized for protocol interface  org.apache.hadoop.yarn.server.api.ResourceTrackerPB indicating that ResourceTrackerPB hasn't been annotated with @KerberosInfo nor @TokenInfo \n",
      " $var shell substitution in properties are not expanded in hadoop-policy.xml\n"
     ]
    }
   ],
   "source": [
    "print(my_utils.get_descriptions(conn, table, 12639198))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During BigTop 0.6.0 release test cycle, Roman Shaposhnik came around the following problem:\n",
      "\n",
      "NodeManager\n",
      "authorized for protocol interface\n",
      "\n",
      "\n",
      "\n",
      "The most significant part is \n",
      "User yarn/ip-10-46-37-244.ec2.internal@BIGTOP (auth:KERBEROS) is not authorized for protocol interface  org.apache.hadoop.yarn.server.api.ResourceTrackerPB indicating that ResourceTrackerPB hasn't been annotated with @KerberosInfo nor @TokenInfo \n",
      " $var shell substitution in properties are not expanded in hadoop-policy.xml\n"
     ]
    }
   ],
   "source": [
    "print(segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, 12639198))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During BigTop 0.6.0 release test cycle, Roman Shaposhnik came around the following problem:\n",
      "\n",
      "013-03-26 15:37:03,573 FATAL $var shell substitution in properties are not expanded in hadoop-policy.xml\n"
     ]
    }
   ],
   "source": [
    "print(remove_stacktrace(my_utils.get_descriptions(conn, table, 12639198)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores_pos = np.array([get_similarity_of_pair_with_desc(conn, table, pair) for pair in partial_positive_pairs_with_stacktrace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [00:18<00:00, 20.45it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_scores_neg = np.array([get_similarity_of_pair_with_desc(conn, table, pair) for pair in tqdm(partial_negative_pairs_with_stacktrace)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74316394 0.17393094\n"
     ]
    }
   ],
   "source": [
    "print(sim_scores_pos.mean(), sim_scores_pos.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35438177 0.13968097\n"
     ]
    }
   ],
   "source": [
    "print(sim_scores_neg.mean(), sim_scores_neg.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 25.07it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_scores_pos_wo_stacktrace = np.array([get_similarity_of_pair_with_desc_no_stacktrace_from_file(\"spark_wo_stacktrace\", pair) for pair in tqdm(partial_positive_pairs_with_stacktrace)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [00:14<00:00, 25.84it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_scores_neg_wo_stacktrace = np.array([get_similarity_of_pair_with_desc_no_stacktrace_from_file(\"spark_wo_stacktrace\", pair) for pair in tqdm(partial_negative_pairs_with_stacktrace)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7414124 0.1926197\n"
     ]
    }
   ],
   "source": [
    "print(sim_scores_pos_wo_stacktrace.mean(), sim_scores_pos_wo_stacktrace.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28094012 0.1169239\n"
     ]
    }
   ],
   "source": [
    "print(sim_scores_neg_wo_stacktrace.mean(), sim_scores_neg_wo_stacktrace.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partial_bug_ids_w_duplicates_and_stacktrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34981698, 0.15760064, 0.41249198, 0.41249198, 0.29618025,\n",
       "       0.29618025, 0.3864901 , 0.09716301, 0.08086804, 0.36586148,\n",
       "       0.34407666, 0.62588763, 0.59807384, 0.22106531, 0.46586314,\n",
       "       0.4237856 , 0.5828459 , 0.4907854 , 0.52060944, 0.37772298],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores_neg[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37045205, 0.15760064, 0.37853605, 0.37690347, 0.29618025,\n",
       "       0.29618025, 0.270958  , 0.04622083, 0.07302547, 0.41119844,\n",
       "       0.34407666, 0.5406481 , 0.4741854 , 0.22106531, 0.46841183,\n",
       "       0.34125957, 0.5409507 , 0.45556837, 0.52060944, 0.21469279],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores_neg_wo_stacktrace[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12537362, 12658852)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_negative_pairs_with_stacktrace[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_find.are_dups(12537362, 12658852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12660224, 12658852]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_find.get_children(12658852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5406481"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity_of_pair_with_desc_no_stacktrace_from_file(\"spark_wo_stacktrace\", partial_negative_pairs_with_stacktrace[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6042371"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity_of_pair_with_desc_no_stacktrace_from_file(\"spark_wo_stacktrace\", (12660224, 12658852))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12537362 12658852 0.5406481\n",
      "12540965 12658852 0.25529\n",
      "12545575 12658852 0.13150539\n",
      "12602372 12658852 0.36987168\n",
      "12602373 12658852 0.37431195\n",
      "12602374 12658852 0.20421995\n",
      "12602377 12658852 0.20421995\n",
      "12624404 12658852 0.21704678\n",
      "12639198 12658852 0.54223627\n",
      "12644379 12658852 0.115709916\n",
      "12644449 12658852 0.15631133\n",
      "12649623 12658852 0.32205003\n",
      "12654402 12658852 0.31348854\n",
      "12658852 12658852 1.0\n",
      "12660224 12658852 0.6042371\n",
      "12672191 12658852 0.2360063\n",
      "12679949 12658852 0.34648585\n",
      "12680074 12658852 0.26270974\n",
      "12688395 12658852 0.37964186\n",
      "12689278 12658852 0.38430497\n",
      "12690959 12658852 0.5425202\n",
      "12700876 12658852 0.12126547\n",
      "12703250 12658852 0.29401445\n",
      "12710926 12658852 0.19538037\n",
      "12722180 12658852 0.36400878\n",
      "12723052 12658852 0.2703671\n",
      "12733025 12658852 0.37316945\n",
      "12743684 12658852 0.20649233\n",
      "12745107 12658852 0.20893043\n"
     ]
    }
   ],
   "source": [
    "for bug_id in partial_bug_ids_w_duplicates_and_stacktrace:\n",
    "    bug_id1 = 12658852\n",
    "    sim_score = get_similarity_of_pair_with_desc_no_stacktrace_from_file(\"spark_wo_stacktrace\", (bug_id, 12658852))\n",
    "    print(bug_id, bug_id1, sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65443754, 0.78060997, 0.99999994, 0.80272967, 0.80272967,\n",
       "       0.80272967, 0.80272967, 1.        , 0.74767995, 0.8932073 ,\n",
       "       0.920344  , 0.22961201, 0.48272163, 0.6399281 , 0.8248001 ,\n",
       "       0.82780427, 0.7060792 , 0.61524755, 0.59533626, 0.7345528 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6042371 , 0.78060997, 0.9995721 , 0.9205553 , 0.9205553 ,\n",
       "       0.92103803, 0.92103803, 1.        , 0.6384648 , 0.87005013,\n",
       "       0.7594098 , 0.22961201, 0.67306983, 0.4193321 , 0.7903035 ,\n",
       "       0.7834386 , 0.71531075, 0.58781797, 0.5592795 , 0.7345528 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores_pos_wo_stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12710926, 12649623)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_positive_pairs_with_stacktrace[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at pos pairs: (12710926, 12649623)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12639198, 12537362)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_positive_pairs_with_stacktrace[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'spark_w_stacktrace' created.\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"spark_w_stacktrace\"\n",
    "if not os.path.exists(folder_name):\n",
    "    # If it doesn't exist, create the folder\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")\n",
    "    \n",
    "for bug_id in partial_bug_ids_w_duplicates_and_stacktrace:\n",
    "    file_name = str(bug_id)+\"w_stacktrace.txt\"\n",
    "    sent = my_utils.get_descriptions(conn, table, bug_id)\n",
    "    file_path = os.path.join(folder_name, file_name)\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write the content to the file\n",
    "        file.write(sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicates within a search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hadoop_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24083/24083 [00:00<00:00, 68599.06it/s]\n"
     ]
    }
   ],
   "source": [
    "table = \"hadoop_old\"\n",
    "union_find = my_utils.UnionFind()\n",
    "union_find.process_project(conn, table, min_desc_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of bug_ids that has stacktrace\n",
    "bug_ids = my_utils.get_bug_ids(conn, table)\n",
    "bug_ids_w_stacktrace = [bug_id for bug_id in bug_ids if len(my_utils.get_stacktrace(conn, table, bug_id)) != 0]\n",
    "\n",
    "# bug ids what has duplicates\n",
    "bug_ids_w_duplicates = union_find.get_all_children()\n",
    "\n",
    "# intersection, bug_ids that has duplicates and stactrace\n",
    "bug_ids_w_duplicates_and_stacktrace = list(set(bug_ids_w_duplicates).intersection(set(bug_ids_w_stacktrace)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 12376.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# finalize search space by adding duplicates of bug_ids_w_duplicates_and_stacktrace\n",
    "search_space = bug_ids_w_stacktrace.copy()\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    dups = union_find.get_children(bug_id)\n",
    "    for dup in dups:\n",
    "        if dup != bug_id and dup not in search_space:\n",
    "            search_space.append(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3891"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3891/3891 [01:09<00:00, 55.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to vectors\n",
    "\n",
    "search_space_vects = {}\n",
    "for bug_id in tqdm(search_space):\n",
    "    eng = segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, bug_id))[0]\n",
    "    vect = model.encode(eng,convert_to_tensor=True).numpy()\n",
    "    search_space_vects[bug_id] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pynndescent.NNDescent(np.array(list(search_space_vects.values())), n_neighbors=100, metric=\"cosine\")\n",
    "index.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 1085048.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q_vects = []\n",
    "Q_indices = [search_space.index(bug_id) for bug_id in bug_ids_w_duplicates_and_stacktrace]\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    # eng = segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, bug_id))[0]\n",
    "    vect = search_space_vects[bug_id]\n",
    "    Q_vects.append(vect)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = index.query(np.array(Q_vects), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q =  12722180\n",
      "Index of query  2911\n",
      "Index of neighbors  [2911  367   53 3113 1938 1509  999  703  470 2882 1649]\n",
      "Duplicates  [12733025, 12722180]\n",
      "Index of duplicates  [3113, 2911]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "print(\"Q = \", q)\n",
    "print(\"Index of query \", Q_indices[i])\n",
    "print(\"Index of neighbors \", neighbors[0][i])\n",
    "print(\"Duplicates \", union_find.get_children(q))\n",
    "print(\"Index of duplicates \", [search_space.index(id) for id in union_find.get_children(q)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 8369.34it/s]\n"
     ]
    }
   ],
   "source": [
    "found_in_top_k_wo_stacktrace = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "for i in tqdm(range(len(bug_ids_w_duplicates_and_stacktrace))):\n",
    "    q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "    # print(\"Q = \", q)\n",
    "    # print(\"Index of query \", Q_indices[i])\n",
    "    # print(\"Index of neighbors \", neighbors[0][i])\n",
    "    # print(\"Duplicates \", union_find.get_children(q))\n",
    "    index_of_duplicates = [search_space.index(id) for id in union_find.get_children(q)]\n",
    "    # print(\"Index of duplicates \", index_of_duplicates)\n",
    "    for result_k in range(1, len(neighbors[0][i][1:])):\n",
    "        if neighbors[0][i][result_k] in index_of_duplicates:\n",
    "            # increment from k to 10\n",
    "            for f in range(result_k, len(found_in_top_k_wo_stacktrace)):\n",
    "                found_in_top_k_wo_stacktrace[f] += 1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.35714286, 0.42857143, 0.4789916 , 0.5       ,\n",
       "       0.51680672, 0.55042017, 0.55462185, 0.56302521, 0.57563025,\n",
       "       0.57563025])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_in_top_k_wo_stacktrace / 238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 13761.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# finalize search space by adding duplicates of bug_ids_w_duplicates_and_stacktrace\n",
    "search_space = bug_ids_w_stacktrace.copy()\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    dups = union_find.get_children(bug_id)\n",
    "    for dup in dups:\n",
    "        if dup != bug_id and dup not in search_space:\n",
    "            search_space.append(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3891"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3891/3891 [00:33<00:00, 116.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to vectors\n",
    "\n",
    "search_space_vects = {}\n",
    "for bug_id in tqdm(search_space):\n",
    "    eng = my_utils.get_descriptions(conn, table, bug_id)[0]\n",
    "    vect = model.encode(eng,convert_to_tensor=True).numpy()\n",
    "    search_space_vects[bug_id] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/t/tiendat.ng.cs/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "index = pynndescent.NNDescent(np.array(list(search_space_vects.values())), n_neighbors=100, metric=\"cosine\")\n",
    "index.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 791629.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q_vects = []\n",
    "Q_indices = [search_space.index(bug_id) for bug_id in bug_ids_w_duplicates_and_stacktrace]\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    # eng = segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, bug_id))[0]\n",
    "    vect = search_space_vects[bug_id]\n",
    "    Q_vects.append(vect)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = index.query(np.array(Q_vects), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q =  12722180\n",
      "Index of query  2911\n",
      "Index of neighbors  [224 304  82 223 225 256  42  80 159 204 382]\n",
      "Duplicates  [12733025, 12722180]\n",
      "Index of duplicates  [3113, 2911]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "print(\"Q = \", q)\n",
    "print(\"Index of query \", Q_indices[i])\n",
    "print(\"Index of neighbors \", neighbors[0][i])\n",
    "print(\"Duplicates \", union_find.get_children(q))\n",
    "print(\"Index of duplicates \", [search_space.index(id) for id in union_find.get_children(q)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 8009.86it/s]\n"
     ]
    }
   ],
   "source": [
    "found_in_top_k_w_stacktrace = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "for i in tqdm(range(len(bug_ids_w_duplicates_and_stacktrace))):\n",
    "    q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "    # print(\"Q = \", q)\n",
    "    # print(\"Index of query \", Q_indices[i])\n",
    "    # print(\"Index of neighbors \", neighbors[0][i])\n",
    "    # print(\"Duplicates \", union_find.get_children(q))\n",
    "    index_of_duplicates = [search_space.index(id) for id in union_find.get_children(q)]\n",
    "    # print(\"Index of duplicates \", index_of_duplicates)\n",
    "    for result_k in range(1, len(neighbors[0][i][1:])):\n",
    "        if neighbors[0][i][result_k] in index_of_duplicates:\n",
    "            # increment from k to 10\n",
    "            for f in range(result_k, len(found_in_top_k_w_stacktrace)):\n",
    "                found_in_top_k_w_stacktrace[f] += 1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00840336, 0.01260504, 0.02521008, 0.02521008,\n",
       "       0.02941176, 0.02941176, 0.03361345, 0.04201681, 0.04621849,\n",
       "       0.04621849])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_in_top_k_w_stacktrace / 238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  3,  6,  6,  7,  7,  8, 10, 11, 11])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_in_top_k_w_stacktrace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eclipse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 7878/27583 [00:00<00:00, 78771.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27583/27583 [00:00<00:00, 59810.36it/s]\n"
     ]
    }
   ],
   "source": [
    "table = \"eclipse\"\n",
    "union_find = my_utils.UnionFind()\n",
    "union_find.process_project(conn, table, min_desc_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of bug_ids that has stacktrace\n",
    "bug_ids = my_utils.get_bug_ids(conn, table)\n",
    "bug_ids_w_stacktrace = [bug_id for bug_id in bug_ids if len(my_utils.get_stacktrace(conn, table, bug_id)) != 0]\n",
    "\n",
    "# bug ids what has duplicates\n",
    "bug_ids_w_duplicates = union_find.get_all_children()\n",
    "\n",
    "# intersection, bug_ids that has duplicates and stactrace\n",
    "bug_ids_w_duplicates_and_stacktrace = list(set(bug_ids_w_duplicates).intersection(set(bug_ids_w_stacktrace)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 7648.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# finalize search space by adding duplicates of bug_ids_w_duplicates_and_stacktrace\n",
    "search_space = bug_ids_w_stacktrace.copy()\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    dups = union_find.get_children(bug_id)\n",
    "    for dup in dups:\n",
    "        if dup != bug_id and dup not in search_space:\n",
    "            search_space.append(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3020"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 691/3020 [00:14<00:47, 49.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bug_id \u001b[38;5;129;01min\u001b[39;00m tqdm(search_space):\n\u001b[1;32m      5\u001b[0m     eng \u001b[38;5;241m=\u001b[39m segregate_log_and_stacktrace(my_utils\u001b[38;5;241m.\u001b[39mget_descriptions(conn, table, bug_id))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     vect \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m     search_space_vects[bug_id] \u001b[38;5;241m=\u001b[39m vect\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:325\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    328\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert to vectors\n",
    "\n",
    "search_space_vects = {}\n",
    "for bug_id in tqdm(search_space):\n",
    "    eng = segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, bug_id))[0]\n",
    "    vect = model.encode(eng,convert_to_tensor=True).numpy()\n",
    "    search_space_vects[bug_id] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pynndescent.NNDescent(np.array(list(search_space_vects.values())), n_neighbors=100, metric=\"cosine\")\n",
    "index.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 1109420.27it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q_vects = []\n",
    "Q_indices = [search_space.index(bug_id) for bug_id in bug_ids_w_duplicates_and_stacktrace]\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    # eng = segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, bug_id))[0]\n",
    "    vect = search_space_vects[bug_id]\n",
    "    Q_vects.append(vect)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = index.query(np.array(Q_vects), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q =  532492\n",
      "Index of query  286\n",
      "Index of neighbors  [ 286 2724 1120 2774 1916 2583  153 1362  239 1342 1530]\n",
      "Duplicates  [531749, 529367, 531870, 532492]\n",
      "Index of duplicates  [232, 12, 239, 286]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "print(\"Q = \", q)\n",
    "print(\"Index of query \", Q_indices[i])\n",
    "print(\"Index of neighbors \", neighbors[0][i])\n",
    "print(\"Duplicates \", union_find.get_children(q))\n",
    "print(\"Index of duplicates \", [search_space.index(id) for id in union_find.get_children(q)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 5912.60it/s]\n"
     ]
    }
   ],
   "source": [
    "found_in_top_k_wo_stacktrace = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "for i in tqdm(range(len(bug_ids_w_duplicates_and_stacktrace))):\n",
    "    q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "    # print(\"Q = \", q)\n",
    "    # print(\"Index of query \", Q_indices[i])\n",
    "    # print(\"Index of neighbors \", neighbors[0][i])\n",
    "    # print(\"Duplicates \", union_find.get_children(q))\n",
    "    index_of_duplicates = [search_space.index(id) for id in union_find.get_children(q)]\n",
    "    # print(\"Index of duplicates \", index_of_duplicates)\n",
    "    for result_k in range(1, len(neighbors[0][i][1:])):\n",
    "        if neighbors[0][i][result_k] in index_of_duplicates:\n",
    "            # increment from k to 10\n",
    "            for f in range(result_k, len(found_in_top_k_wo_stacktrace)):\n",
    "                found_in_top_k_wo_stacktrace[f] += 1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.55042017, 0.68487395, 0.75630252, 0.77310924,\n",
       "       0.81092437, 0.84033613, 0.85294118, 0.86554622, 0.88655462,\n",
       "       0.88655462])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_in_top_k_wo_stacktrace / 238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 7701.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# finalize search space by adding duplicates of bug_ids_w_duplicates_and_stacktrace\n",
    "search_space = bug_ids_w_stacktrace.copy()\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    dups = union_find.get_children(bug_id)\n",
    "    for dup in dups:\n",
    "        if dup != bug_id and dup not in search_space:\n",
    "            search_space.append(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3020"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3020/3020 [00:25<00:00, 117.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to vectors\n",
    "\n",
    "search_space_vects = {}\n",
    "for bug_id in tqdm(search_space):\n",
    "    eng = my_utils.get_descriptions(conn, table, bug_id)[0]\n",
    "    vect = model.encode(eng,convert_to_tensor=True).numpy()\n",
    "    search_space_vects[bug_id] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/t/tiendat.ng.cs/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "index = pynndescent.NNDescent(np.array(list(search_space_vects.values())), n_neighbors=100, metric=\"cosine\")\n",
    "index.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 878925.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Q_vects = []\n",
    "Q_indices = [search_space.index(bug_id) for bug_id in bug_ids_w_duplicates_and_stacktrace]\n",
    "for bug_id in tqdm(bug_ids_w_duplicates_and_stacktrace):\n",
    "    # eng = segregate_log_and_stacktrace(my_utils.get_descriptions(conn, table, bug_id))[0]\n",
    "    vect = search_space_vects[bug_id]\n",
    "    Q_vects.append(vect)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = index.query(np.array(Q_vects), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q =  532492\n",
      "Index of query  286\n",
      "Index of neighbors  [900 874 234 557 579 808 146 179 249 290 906]\n",
      "Duplicates  [531749, 529367, 531870, 532492]\n",
      "Index of duplicates  [232, 12, 239, 286]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "print(\"Q = \", q)\n",
    "print(\"Index of query \", Q_indices[i])\n",
    "print(\"Index of neighbors \", neighbors[0][i])\n",
    "print(\"Duplicates \", union_find.get_children(q))\n",
    "print(\"Index of duplicates \", [search_space.index(id) for id in union_find.get_children(q)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:00<00:00, 5025.86it/s]\n"
     ]
    }
   ],
   "source": [
    "found_in_top_k_w_stacktrace = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "for i in tqdm(range(len(bug_ids_w_duplicates_and_stacktrace))):\n",
    "    q = bug_ids_w_duplicates_and_stacktrace[i]\n",
    "    # print(\"Q = \", q)\n",
    "    # print(\"Index of query \", Q_indices[i])\n",
    "    # print(\"Index of neighbors \", neighbors[0][i])\n",
    "    # print(\"Duplicates \", union_find.get_children(q))\n",
    "    index_of_duplicates = [search_space.index(id) for id in union_find.get_children(q)]\n",
    "    # print(\"Index of duplicates \", index_of_duplicates)\n",
    "    for result_k in range(1, len(neighbors[0][i][1:])):\n",
    "        if neighbors[0][i][result_k] in index_of_duplicates:\n",
    "            # increment from k to 10\n",
    "            for f in range(result_k, len(found_in_top_k_w_stacktrace)):\n",
    "                found_in_top_k_w_stacktrace[f] += 1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00840336, 0.02521008, 0.04621849, 0.05042017,\n",
       "       0.06302521, 0.07563025, 0.07983193, 0.10504202, 0.12605042,\n",
       "       0.12605042])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_in_top_k_w_stacktrace / 238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  6, 11, 12, 15, 18, 19, 25, 30, 30])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_in_top_k_w_stacktrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
