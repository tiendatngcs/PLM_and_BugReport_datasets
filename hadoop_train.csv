bug_id1,bug1,bug_id2,bug2,label
13155202,"Support pushing arrow batches through org.apache.arrow.vector.ipc.ArrowOutputStream in LllapOutputFormatService. 
 Support ArrowOutputStream in LlapOutputFormatService",13158906,"Arrow batch serializer doesn't handle null values well in complex nested data types. 
 Null value error with complex nested data type in Arrow batch serializer",yes
13247213,"YARN UI2 shows ""Pending, Allocated, Reserved Containers"" information for fair scheduler. In here, pending container information is not printed. UI2 shows "",0,0"" instead ""0,0,0"".
In UI1, This same information is displayed as Num of active Application & Pending applications.
Num Active Applications:	0
Num Pending Applications:	0
It's not clear from UI2 what do we intend to show in ""Pending, Allocated, Reserved Containers""? Is it really containers or apps? 
 [UI2] Fix Pending, Allocated, Reserved Containers information for Fair Scheduler",13239486,"There are a few issues with the apps page for a queue when Fair Scheduler is used.

Labels like configured capacity, configured max capacity etc. (marked in the attached image) are not needed as they are specific to Capacity Scheduler.
Steady fair memory,used memoryandmaximum memory are actual values but are shown as percentages.
Formatting ofPending, Allocated, Reserved Containers values is not correct (shown in the attached screenshot)

 
 UI2 - Fair scheduler queue apps page issues",yes
13204875,"For example, if we define a aux service with name A, and it's class is pkg.class.A
It should be loading from yarn.nodemanager.aux-services.A.system-classes, but currently it is loading from yarn.nodemanager.aux-services.pkg.class.A.system-classes now. 
 Aux service system-class is loading from a wrong property",13201705,"It would be useful to support adding, removing, or updating auxiliary services without requiring a restart of NMs. 
 Dynamically add or remove auxiliary services",yes
13324802,"unit test failed of org.apache.hadoop.fs.contract.AbstractContractMultipartUploaderTest#testConcurrentUploads.
Exception:


java.lang.IllegalArgumentExceptionjava.lang.IllegalArgumentException at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127) at org.apache.hadoop.test.LambdaTestUtils$ProportionalRetryInterval.<init>(LambdaTestUtils.java:907) at org.apache.hadoop.fs.contract.AbstractContractMultipartUploaderTest.testConcurrentUploads(AbstractContractMultipartUploaderTest.java:815)

Reason:


public ProportionalRetryInterval(int intervalMillis,
    int maxIntervalMillis) {
  Preconditions.checkArgument(intervalMillis > 0);
  Preconditions.checkArgument(maxIntervalMillis > 0);
  this.intervalMillis = intervalMillis;
  this.current = intervalMillis;
  this.maxIntervalMillis = maxIntervalMillis;
}

The constructor of ProportionalRetryInterval requires maxIntervalMillis> 0. But TestHDFSContractMultipartUploader does not override the timeToBecomeConsistentMillis method, so maxIntervalMillis = 0 
 Fix unit test of HDFS-13934",13316762,"TestHDFSContractMultipartUploader fails on trunk with IllegalArgumentException


[ERROR] testConcurrentUploads(org.apache.hadoop.fs.contract.hdfs.TestHDFSContractMultipartUploader)  Time elapsed: 0.127 s  <<< ERROR!
java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.test.LambdaTestUtils$ProportionalRetryInterval.<init>(LambdaTestUtils.java:907)
	at org.apache.hadoop.fs.contract.AbstractContractMultipartUploaderTest.testConcurrentUploads(AbstractContractMultipartUploaderTest.java:815)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)


 
 TestHDFSContractMultipartUploader fails on trunk",yes
13164188,"

Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener; locked 75% of time Bug type IS2_INCONSISTENT_SYNC (click for details) In class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService Field org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener Synchronized 75% of the time Unsynchronized access at AllocationFileLoaderService.java:[line 117] Synchronized access at AllocationFileLoaderService.java:[line 212] Synchronized access at AllocationFileLoaderService.java:[line 228] Synchronized access at AllocationFileLoaderService.java:[line 269]

 
 Findbugs warning IS2_INCONSISTENT_SYNC in AllocationFileLoaderService.reloadListener",13165507,"This is reported by findbugs. See: https://builds.apache.org/job/PreCommit-YARN-Build/21007/artifact/out/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-warnings.html

Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener; locked 75% of time

 
 Multithreaded correctness Warning occurs in AllocationFileLoaderService",yes
13210940,"Nodemanager will not start
1. If Autodiscovery is enabled:

If nvidia-smi path is misconfigured or the file does not exist.
There is 0 GPU found
If the file exists but it is not pointing to an nvidia-smi
if the binary is ok but there is an IOException

2. If the manuallyconfiguredGPU devices are misconfigured

Any index:minornumber format failure will cause a problem
0 configured device will cause a problem
NumberFormatException is not handled

It would be a better option to add warnings about the configuration, set 0 available GPUs and let the node work and run non-gpujobs. 
 Backport HBASE-21279 (Split TestAdminShell into several tests) to branch-2",13279620,"

insert into b values(1);


List<LockComponent> lockComponents = AcidUtils.makeLockComponents(plan.getOutputs(), plan.getInputs(), conf);
plan.getInputs() contains single entry <_dummy_database@_dummy_table>
 
 Do not acquire read lock for dummy input",No
13210940,"Nodemanager will not start
1. If Autodiscovery is enabled:

If nvidia-smi path is misconfigured or the file does not exist.
There is 0 GPU found
If the file exists but it is not pointing to an nvidia-smi
if the binary is ok but there is an IOException

2. If the manuallyconfiguredGPU devices are misconfigured

Any index:minornumber format failure will cause a problem
0 configured device will cause a problem
NumberFormatException is not handled

It would be a better option to add warnings about the configuration, set 0 available GPUs and let the node work and run non-gpujobs. 
 Backport HBASE-21279 (Split TestAdminShell into several tests) to branch-2",13160335,"Add some useful stuff and some refinement to PE tool
1. Add multiPut support
Though we have BufferedMutator, sometimes we need to benchmark batch put in a certain number.
Set --multiPut=number to enable batchput(meanwhile, --autoflush need be set to false)
2. Add Connection Number support
Before, there is only on parameter to control the connection used by threads. oneCon=true means all threads use one connection, false means each thread has it own connection.
When thread number is high and oneCon=false, we noticed high context switch frequency in the machine which PE run on, disturbing the benchmark results(each connection has its own netty worker threads, 2*CPU IIRC).  
So, added a new parameter connCount to PE. set --connCount=2 means all threads will share 2 connections.
3. Add avg RT and avg TPS/QPS statstic for all threads
Useful when we want to meansure the total throughtput of the cluster
4. Delete some redundant code
Now RandomWriteTest is inherited from SequentialWrite. 
 Add multiPut support and other miscellaneous to PE",No
13185672,"In JDK10, sun.net.dns.ResolverConfiguration is encapsulated and not accessible from unnamed modules. This issue is to remove the usage of ResolverConfiguration. 
 [JDK10] Migrate from sun.net.dns.ResolverConfiguration to the replacement",13185726,"./dev-support/bin/hadoop.sh
PATCH_NAMING_RULE=""https://wiki.apache.org/hadoop/HowToContribute""


https://wiki.apache.org/hadoop/HowToContribute was moved to https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute. Let's update the personality. 
 Update PATCH_NAMING_RULE in the personality file",No
13180937,"There is no directory created as ""DS_APP_ATTEMPT"" so
#Assert.assertTrue(outputDirForEntity.isDirectory()) returning false


org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2
Stacktrace
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628)

org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow
 Failing for the past 10 builds (Since#29 )
 
Stacktrace
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628)

org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2DefaultFlow
 
 
 Stacktrace
 java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628) 
 TestDistributedShell test cases failing giving assertion error",13172117,"These tests have been failing for a while in trunk:



testDSShellWithoutDomainV2
1 min 20 sec
Failed


testDSShellWithoutDomainV2CustomizedFlow
1 min 20 sec
Failed


testDSShellWithoutDomainV2DefaultFlow
1 min 20 sec
Failed



The root causes are the same:


java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.checkTimelineV2(TestDistributedShell.java:546)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShell(TestDistributedShell.java:451)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShell(TestDistributedShell.java:310)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2(TestDistributedShell.java:306)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
 
 Fix DistributedShell unit tests",yes
13216663,"before HIVE-18908; there was a baseForward (non vectorized) and vectorForward (vectorized); and both of them have accounted for the number of rows correctly - after HIVE-18908 this have changed to count vectorized batches in case of vectorized execution.
relevant part of Operator.java
[counters are dropping to 1|
https://github.com/apache/hive/commit/a37827ecd557c7f7d69f3b2ccdbf6535908b1461#diff-e70a9d33150346fe8b9b7d719d677b97L356] 
 Runtime rowcount calculation is incorrect in vectorized executions",13200233,"see history of ./ql/src/test/results/clientpositive/stat_estimate_drill.q.out 
 Drilldown estimates are highly off after HIVE-18908",yes
13254526,"When trying to launch the infrastructure application to begin the startup of the internal HDFS cluster as shown in the Manual Workload Launch section in here



$ ./dynamometer-infra/bin/start-dynamometer-cluster.sh \
 -hadoop_binary_path hadoop-3.0.2.tar.gz \
 -conf_path my-hadoop-conf \
 -fs_image_dir hdfs:///fsimage \
 -block_list_path hdfs:///dyno/blocks



 its usage is always shown even if correct arguments are given, if `-hadoop_binary_path` is placed as a first argument for the script. 
 [Dynamometer] start-dynamometer-cluster.sh shows its usage even if correct arguments are given.",13140688,"The assignment of version,suite and keyName should happen lazily, right before it's used in case the fileXAttr is null 
 Change the code order in getFileEncryptionInfo to avoid unnecessary call of assignment",No
13219452,"When implementing HBASE-21717, I found that AccessController will use the Admin interface to get table descriptors on master. We can expose the read only methods in TableDescritors to CPs so user can get the table descriptor directly. 
 Expose TableDescriptors to master coprocessor",13136371,"Error is:


java.lang.AssertionError: found exception: java.lang.NullPointerException via CODE-BUG: Uncaught runtime exception: pid=154, state=RUNNABLE:SPLIT_TABLE_REGION_CREATE_DAUGHTER_REGIONS; SplitTableRegionProcedure table=testRecoveryAndDoubleExecution, parent=3d8d459ba395c2cf6b1e5c71aca92cfd, daughterA=c6531c10effa8e542159ab82a87bd75e, daughterB=ee34a9af88273b6c06e1a688fc50ed6e:java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testRecoveryAndDoubleExecution(TestSplitTableRegionProcedure.java:411)

Exception from the output file:


2018-02-05 18:00:48,205 ERROR [PEWorker-1] procedure2.ProcedureExecutor(1480): CODE-BUG: Uncaught runtime exception: pid=19, state=RUNNABLE:SPLIT_TABLE_REGION_CREATE_DAUGHTER_REGIONS; SplitTableRegionProcedure table=testSplitWithoutPONR, parent=57114194fb486a3988b232bcf10eb177, daughterA=749aa83c03b8f7c6b642cd73c5b51e43, daughterB=a53ec69e8dd2cfa6c0be2b9a7eb271bb
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.splitStoreFiles(SplitTableRegionProcedure.java:617)
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.createDaughterRegions(SplitTableRegionProcedure.java:541)
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.executeFromState(SplitTableRegionProcedure.java:241)
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.executeFromState(SplitTableRegionProcedure.java:89)
	at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:180)
	at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:845)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1455)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1224)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$800(ProcedureExecutor.java:78)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1734)

Value of 'htd' is null asitis initialized in the constructor but when the object is deserialized its null.
 
 TestSplitTableRegionProcedure#testSplitWithoutPONR() and testRecoveryAndDoubleExecution() are failing with NPE",No
13220838,"As we have already deprecated the same methods in our Admin interface. 
 Deprecated isTableAvailableWithSplit method in thrift",13209330,"The one in the Admin interface has already been marked as deprecated but in thrift it is still there. 
 Deprecated isTableAvailableWithSplit method in thrift module",yes
13255898,"https://orc.apache.org/docs/releases.html
 
 Update ORC to version 1.6",13206641,"

  public static String generatePath(Path baseURI, String filename) {
    String path = new String(baseURI + Path.SEPARATOR + filename);
    return path;
  }

  public static String generateFileName(Byte tag, String bigBucketFileName) {
    String fileName = new String(""MapJoin-"" + tag + ""-"" + bigBucketFileName + suffix);
    return fileName;
  }


It's a bit odd to be performing string concatenation and then wrapping the results in a new string.  This is creating superfluous String objects.  
 Remove Extra String Object",No
13254661,"Is doing the release. We need to update the release year from 2018 to 2019.


$ find . -name ""pom.xml"" | xargs grep -n 2018
./hadoop-project/pom.xml:34:    <release-year>2018</release-year>

 
 Update the release year to 2019",13151249,"Missed in HIVE-18859 
 Update golden files for negative tests",No
13184385,"

mvn test -Dtest=TestHDFSContractAppend#testAppendDirectory,TestRouterWebHDFSContractAppend#testAppendDirectory

In case ofTestHDFSContractAppend the test excepts FileAlreadyExistsException but HDFS sends the exception wrapped into a RemoteException.
In case of TestRouterWebHDFSContractAppend the append does not even throw exception.
Steve Loughran, Thomas Marqardt, any thoughts? 
 AbstractContractAppendTest fails against HDFS on HADOOP-15407 branch",13130273,"Another good find by our Romil.


hbase(main):001:0> list
TABLE
a
1 row(s)
Took 0.8385 seconds
hbase(main):002:0> tables=list
TABLE
a
1 row(s)
Took 0.0267 seconds
hbase(main):003:0> puts tables

hbase(main):004:0> p tables
nil


The list command should be returning ['a'] but is not.
The command class itself appears to be doing the right thing – maybe the retval is getting lost somewhere else?
FYI Michael Stack. 
 Add '--return-values' option to Shell to print return values of commands in interactive mode",No
13160803,"The jobSubmitDir directory is owned by root and is being cleaned up as the submitting user, which appears to be why it is failing to clean up.

2018-05-21 19:46:15,124 WARN  [DeletionService #0] privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 255. Privileged Execution Operation Stderr:

Stdout: main : command provided 3
main : run as user is ebadger
main : requested yarn user is ebadger
failed to unlink /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/jobSubmitDir/job.split: Permission denied
failed to unlink /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/jobSubmitDir/job.splitmetainfo: Permission denied
failed to rmdir jobSubmitDir: Directory not empty
Error while deleting /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001: 39 (Directory not empty)

Full command array for failed execution:
[/hadoop-3.2.0-SNAPSHOT/bin/container-executor, ebadger, ebadger, 3, /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001]
2018-05-21 19:46:15,124 ERROR [DeletionService #0] nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(848)) - DeleteAsUser for /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001 returned with exit code: 255
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=255:
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:206)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.deleteAsUser(LinuxContainerExecutor.java:844)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask.run(FileDeletionTask.java:135)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: ExitCodeException exitCode=255:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        ... 10 more



[foo@bar hadoop]$ ls -l /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/
total 4
drwxr-sr-x 2 root users 4096 May 21 19:45 jobSubmitDir

 
 Privileged docker containers' jobSubmitDir does not get successfully cleaned up",13279500,"CVE-2019-20330 is reported and fixed in jackson-databind 2.9.10.2.
https://nvd.nist.gov/vuln/detail/CVE-2019-20330 
 Upgrade jackson-databind to 2.9.10.2",No
13325693,"Hi there,
 I'm using Spark to read some data from S3 and I encountered an error when reading from a bucket that contains a period (e.g. `s3a://okokes-test-v1.1/foo.csv`). I have close to zero Java experience, but I've tried to trace this as well as I can. Apologies for any misunderstanding on my part.
Edit: the title is a little misleading - buckets can contain dots and s3a will work, but only if these bucket names conform to hostname restrictions - e.g. `s3a://foo.bar/bak.csv` would work, but my case - `okokes-test-v1.1` does not, because `1` is not conform to a top level domain pattern.
Using hadoop-aws:3.2.0, I get the following:


java.lang.NullPointerException: null uri host.
 at java.base/java.util.Objects.requireNonNull(Objects.java:246)
 at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:71)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:470)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:235)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
 at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:535)
 ... 47 elided

hadoop-aws:2.7.4 did lead to a similar outcome


java.lang.IllegalArgumentException: The bucketName parameter must be specified.
 at com.amazonaws.services.s3.AmazonS3Client.assertParameterNotNull(AmazonS3Client.java:2816)
 at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1026)
 at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
 at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
 at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:535)
 ... 47 elided

I investigated the issue a little bit and found buildFSURI to require the host to be not null - see S3xLoginHelper.java - but in my case the host is null and the authority part of the URL should be used. When I checked AWS' handling of this case, they seem to be using authority for all s3:// paths - https://github.com/aws/aws-sdk-java/blob/master/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3URI.java#L85.
I verified this URI in a Scala shell (openjdk 1.8.0_252)



scala> (new URI(""s3a://okokes-test-v1.1/foo.csv"")).getHost()
val res1: String = null
scala> (new URI(""s3a://okokes-test-v1.1/foo.csv"")).getAuthority()
val res2: String = okokes-test-v1.1



Oh and this is indeed a bucket name. Not only did I create it in the console, but there's also enough documentation on the topic - https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules 
 s3a: bucket names which aren't parseable hostnames unsupported",13238730,"I am experiencing very old issue appearing now again on Cloudera cluster 6.2. I use following libraries with pyspark job:

/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hadoop/hadoop-common-3.0.0-cdh6.2.0.jar
/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/hadoop/hadoop-aws-3.0.0-cdh6.2.0.jar

While trying to write DF to S3 as CSV I get following error:


java.lang.NullPointerException: null uri host. This can be caused by unencoded / in the password string
	at java.util.Objects.requireNonNull(Objects.java:228)
	at org.apache.hadoop.fs.s3native.S3xLoginHelper.buildFSURI(S3xLoginHelper.java:69)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.setUri(S3AFileSystem.java:467)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:234)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3288)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3337)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3305)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:423)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:523)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:281)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
// code placeholder


I specify secret key via configuration, not via path (as older bugs reported) and on top of that my Secret key doesn't have any slash, but access key has dash '-' character and
AWS_HOST_BASE I define with 'http://host.domain.suffix/'form
My code doesn't use secret key in s3 path, but as follows:


sparkSession = SparkSession.builder.getOrCreate() 
sparkContext = sparkSession.sparkContext 
#sparkContext._jsc.hadoopConfiguration().set(""fs.s3a.multipart.size"", ""1000000"") 
sparkContext._jsc.hadoopConfiguration().set(""fs.s3a.access.key"", AWS_ACCESS_KEY_ID) sparkContext._jsc.hadoopConfiguration().set(""fs.s3a.secret.key"", AWS_SECRET_ACCESS_KEY) sparkContext._jsc.hadoopConfiguration().set(""fs.s3a.endpoint"", AWS_HOST_BASE) sparkContext._jsc.hadoopConfiguration().set(""fs.s3.access.key"", AWS_ACCESS_KEY_ID) sparkContext._jsc.hadoopConfiguration().set(""fs.s3.secret.key"", AWS_SECRET_ACCESS_KEY) sparkContext._jsc.hadoopConfiguration().set(""fs.s3.endpoint"", AWS_HOST_BASE) sparkContext._jsc.hadoopConfiguration().set(""fs.s3n.access.key"", AWS_ACCESS_KEY_ID) sparkContext._jsc.hadoopConfiguration().set(""fs.s3n.secret.key"", AWS_SECRET_ACCESS_KEY) sparkContext._jsc.hadoopConfiguration().set(""fs.s3n.endpoint"", AWS_HOST_BASE) 

sqlContext = SQLContext(sparkSession.sparkContext) 
# log4j = sparkContext._jvm.org.apache.log4j 
# pylint: disable=W0212

 logger = sparkContext._jvm.org.apache.log4j.LogManager.getLogger(""OracleToS3"") 
# logger = log4j.LogManager.getlogger(__name__) 
sparkContext.setLogLevel('INFO') 
logger.info(""Going to process Oracle tables..."") 

for table in Source.table_list:
    logger.info(""Reading oracle table into dataframe"")
    oracle_table = sparkContext.read \
        .format(""jdbc"") \
        .option(""url"", Source.jdbc_string) \
        .option(""dbtable"", table) \
        .option(""user"", Source.user) \
        .option(""password"", Source.password) \
        .option(""driver"", ""oracle.jdbc.driver.OracleDriver"") \
        .load()

    # Display schema
    logger.info(""Display table schema"")
    oracle_table.show()
    logger.info(""Display table top 5"")
    oracle_table.head(5)
    output_file = ""s3a://201906/"" + ""11/"" + table + ""_"" + time.strftime(""%Y%m%d_%H%M%S"") +"".csv""
    logger.info(""Writing table into S3 to file: "" + output_file)
    oracle_table\
        .repartition(1)\
        .write \
        .mode(""overwrite"")\
        .format(""csv"")\
        .option(""header"",""true"") \
        .save(""s3a://201906/"" + ""11/"" + table + ""_"" + time.strftime(""%Y%m%d_%H%M%S"") +"".csv"")

 
 S3A NullPointerException: null uri host. This can be caused by unencoded / in the password string",yes
13346846,"See details the JiraHADOOP-17439 comments.

 
 Spin RCs",13316175,"
If you want the deletion of a persistent object to cause the deletion of related objects then you need to mark the related fields in the mapping to be ""dependent"".
http://www.datanucleus.org/products/accessplatform/jdo/persistence.html#dependent_fields
http://www.datanucleus.org/products/datanucleus/jdo/persistence.html#_deleting_an_object
The database won't do it:
Derby Schema

ALTER TABLE ""APP"".""COLUMNS_V2"" ADD CONSTRAINT ""COLUMNS_V2_FK1"" FOREIGN KEY (""CD_ID"") REFERENCES ""APP"".""CDS"" (""CD_ID"") ON DELETE NO ACTION ON UPDATE NO ACTION;


https://github.com/apache/hive/blob/65cf6957cf9432277a096f91b40985237274579f/standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql#L452 
 Make ""cols"" dependent so that it cascade deletes",No
13196411,"
2018-11-06,12:55:25,980 WARN [RpcServer.default.FPBQ.Fifo.handler=251,queue=11,port=17100] org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure: Refresh peer TestPeer for TRANSIT_SYNC_REPLICATION_STATE on c4-hadoop-tst-st54.bj,17200,1541479922465 failed
java.lang.NullPointerException via c4-hadoop-tst-st54.bj,17200,1541479922465:java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.procedure2.RemoteProcedureException.fromProto(RemoteProcedureException.java:124)
	at org.apache.hadoop.hbase.master.MasterRpcServices.lambda$reportProcedureDone$4(MasterRpcServices.java:2303)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
	at org.apache.hadoop.hbase.master.MasterRpcServices.reportProcedureDone(MasterRpcServices.java:2298)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:13149)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:338)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:318)
Caused by: java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.wal.SyncReplicationWALProvider.peerSyncReplicationStateChange(SyncReplicationWALProvider.java:303)
	at org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandlerImpl.transitSyncReplicationPeerState(PeerProcedureHandlerImpl.java:216)
	at org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.call(RefreshPeerCallable.java:74)
	at org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.call(RefreshPeerCallable.java:34)
	at org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler.process(RSProcedureHandler.java:47)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

 
 NPE if RS restarts between REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN and TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE",13131631,"The release year needs to be updated.

$ find . -name ""pom.xml"" | xargs grep -n 2017
./hadoop-project/pom.xml:34:  <release-year>2017</release-year>
 
 Update the release year to 2018",No
13176609,"Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService. 
 VectorFileSinkArrowOperator",13224819,"Use FilterHooks for checking dbs/tables/partitions for showCompactions 
 Use FilterHooks for show compactions",No
13278403,"PUT requests in HttpFS with path as ""/"" were not supported .
 
 HttpFS: put requests are not supported for path ""/""",13224777,"When move namespace region start unassign produre, after unassign procedure finished namespace region will be offline. At the same time master crashed then reboot will stuck, because master init is block by waiting namespace table online ,at same time master init not finish so move region procedure can not go on, make deadlock. 
 Move namespace region then master crashed make deadlock",No
13271775,"If you try dropping a table which is part of the definition of a created materialized view, the table is not dropped, which is the desired state as it is part of the materialized view.
However, there was a ""drop"" call to the table, so it tried to drop it but did not succeed, leaving it in an inconsistent state.

Repro:
-------
1) Create tables:



CREATE TABLE emps (empid INT,deptno INT,name VARCHAR(256),salary FLOAT,hire_date TIMESTAMP)STORED AS ORC TBLPROPERTIES ('transactional'='true');

CREATE TABLE depts (deptno INT,deptname VARCHAR(256),locationid INT)STORED AS ORC TBLPROPERTIES ('transactional'='true');



2) Create the VM:



CREATE MATERIALIZED VIEW mv1 AS SELECT empid, deptname, hire_date FROM emps JOIN deptsON (emps.deptno = depts.deptno) WHERE hire_date >='2016-01-01';



3) Following is in backend database at this point:



mysql> select TBL_ID, DB_ID, SD_ID, TBL_NAME, TBL_TYPE from TBLS where DB_ID=16;
+--------+-------+-------+----------+-------------------+
| TBL_ID | DB_ID | SD_ID | TBL_NAME | TBL_TYPE     |
+--------+-------+-------+----------+-------------------+
|   81 |  16 |  81 | emps   | MANAGED_TABLE   |
|   83 |  16 |  83 | depts  | MANAGED_TABLE   |
|   84 |  16 |  84 | mv1   | MATERIALIZED_VIEW |
+--------+-------+-------+----------+-------------------+
3 rows in set (0.00 sec)



4) Let's drop the 'emps' table:



0: jdbc:hive2://c1122-node2.squadron.support.> drop table emps;
INFO : Compiling command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b): drop table emps
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO : Completed compiling command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b); Time taken: 0.05 seconds
INFO : Executing command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b): drop table emps
INFO : Starting task [Stage-0:DDL] in serial mode
INFO : Completed executing command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b); Time taken: 10.281 seconds
INFO : OK
No rows affected (16.949 seconds)


No issue displayed

5) List tables:



0: jdbc:hive2://c1122-node2.squadron.support.> show tables;
INFO  : Compiling command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413): show tables
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
INFO  : Completed compiling command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413); Time taken: 0.041 seconds
INFO  : Executing command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413): show tables
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413); Time taken: 0.016 seconds
INFO  : OK
+-----------+
| tab_name  |
+-----------+
| depts     |
| emps      |
+-----------+
2 rows selected (0.08 seconds)



6) Now, from the backend-db point of view:



mysql> select TBL_ID, DB_ID, SD_ID, TBL_NAME, TBL_TYPE from TBLS where DB_ID=16;
+--------+-------+-------+----------+-------------------+
| TBL_ID | DB_ID | SD_ID | TBL_NAME | TBL_TYPE     |
+--------+-------+-------+----------+-------------------+
|   81 |  16 | NULL | emps   | MANAGED_TABLE   |
|   83 |  16 |  83 | depts  | MANAGED_TABLE   |
|   84 |  16 |  84 | mv1   | MATERIALIZED_VIEW |
+--------+-------+-------+----------+-------------------+
3 rows in set (0.00 sec)


The table is left with NULL in SD_ID, making it not available.

7) From Metastore.log



2019-12-02T20:00:25,545 INFO [pool-6-thread-195]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(907)) - 196: source:172.25.34.150 drop_table : tbl=hive.mvs.emps
2019-12-02T20:00:25,545 INFO [pool-6-thread-195]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(349)) - ugi=hive	ip=172.25.34.150	cmd=source:172.25.34.150 drop_table : tbl=hive.mvs.emps	
2019-12-02T20:00:25,580 INFO [pool-6-thread-195]: metastore.ObjectStore$RetryingExecutor (ObjectStore.java:run(9966)) - Attempting to acquire the DB log notification lock: 0 out of 10 retries
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""NEXT_EVENT_ID"" from ""NOTIFICATION_SEQUENCE"" for update"".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:391) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:216) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.lambda$lockForUpdate$0(ObjectStore.java:9936) ~[hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:9963) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:9938) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:10002) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at com.sun.proxy.$Proxy28.addNotificationEvent(Unknown Source) [?:?]
	at org.apache.hive.hcatalog.listener.DbNotificationListener.process(DbNotificationListener.java:968) [hive-hcatalog-server-extensions-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropTable(DbNotificationListener.java:198) [hive-hcatalog-server-extensions-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier$19.notify(MetaStoreListenerNotifier.java:99) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:273) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:335) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:2670) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:2842) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at com.sun.proxy.$Proxy30.drop_table_with_environment_context(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:15533) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:15517) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_112]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_112]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) [hadoop-common-3.1.1.3.1.0.0-78.jar:?]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.sql.BatchUpdateException: Cannot delete or update a parent row: a foreign key constraint fails (""hive"".""MV_TABLES_USED"", CONSTRAINT ""MV_TABLES_USED_FK2"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS"" (""TBL_ID""))
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2058) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1471) ~[mysql-connector-java.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:125) ~[HikariCP-2.6.1.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-2.6.1.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:319) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:211) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:633) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 37 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Cannot delete or update a parent row: a foreign key constraint fails (""hive"".""MV_TABLES_USED"", CONSTRAINT ""MV_TABLES_USED_FK2"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS"" (""TBL_ID""))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_112]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_112]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_112]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_112]
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.Util.getInstance(Util.java:386) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1041) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4187) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4119) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2570) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2731) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2820) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2159) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2462) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2010) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1471) ~[mysql-connector-java.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:125) ~[HikariCP-2.6.1.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-2.6.1.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:319) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:211) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:633) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 37 more





8) If you try to query the table:



0: jdbc:hive2://c1122-node2.squadron.support.> select * from emps;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table emps. null (state=42000,code=40000)



It fails as expected.
9) If you try to query the MV:


0: jdbc:hive2://c1122-node2.squadron.support.> select * from mv1; INFO : Compiling command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2): select * from mv1 INFO : Semantic Analysis Completed (retrial = false) INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:mv1.empid, type:int, comment:null), FieldSchema(name:mv1.deptname, type:varchar(256), comment:null), FieldSchema(name:mv1.hire_date, type:timestamp, comment:null)], properties:null) INFO : Completed compiling command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2); Time taken: 0.229 seconds INFO : Executing command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2): select * from mv1 INFO : Completed executing command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2); Time taken: 0.01 seconds INFO : OK +------------+---------------+----------------+ | mv1.empid | mv1.deptname | mv1.hire_date | +------------+---------------+----------------+ +------------+---------------+----------------+ No rows selected (0.276 seconds)


It does not fail, as the underlying data has not changed, and the table is still being shown as valid.

10) Insert data into ""depts"" table and rebuild the mv.


$ INSERT INTO TABLE depts VALUES (101,'IT',25);
$ INSERT INTO TABLE depts VALUES (102,'Eng',11);

0: jdbc:hive2://c1122-node2.squadron.support.> ALTER MATERIALIZED VIEW mvs.mv1 REBUILD;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table emps. null (state=42000,code=40000)


This fails as expected.
 
 Drop table involved in materialized view leaves the table in inconsistent state",13330352,"I have discovered that it's possible to drop a table used by a materialized view. When I drop this table, the result is OK while I think this action should be refused. When I check in the metastore database, I can see that the table has been partially deleted (ie : the reference of the table still exists inTBLS and in MV_TABLES_USED). This introduces an inconsistency in the metastore.
Steps to reproduced :



jdbc:hive2://localhost.> use use ptest2_db_dev;
No rows affected (0.067 seconds)
0: jdbc:hive2://localhost.> create table table_blocked (id string);
No rows affected (0.97 seconds)
0: jdbc:hive2://localhost.> desc table_blocked;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+
1 row selected (0.171 seconds)
0: jdbc:hive2://localhost.> create materialized view table_blocked_mv as select * from table_blocked;
No rows affected (18.055 seconds)
0: jdbc:hive2://localhost.> desc table_blocked_mv;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+
1 row selected (0.316 seconds)
0: jdbc:hive2://localhost.> drop table table_blocked;
No rows affected (10.803 seconds)
0: jdbc:hive2://localhost.> desc table_blocked_mv;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+
1 row selected (0.222 seconds)
0: jdbc:hive2://localhost.> desc table_blocked;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table table_blocked. null (state=42000,code=40000)
0: jdbc:hive2://localhost.> select * from table_blocked_mv;
Error: Error while compiling statement: FAILED: SemanticException Table ptest2_db_dev.table_blocked not found when trying to obtain it to check masking/filtering policies (state=42000,code=40000)

 
 Drop table used by a materialized view",yes
13195203,"Currently, Hadoop allows passing files containing tokens.
WebHDFS provides base64 delegation tokens that can be used directly.
This JIRA adds the option to pass base64 tokens directly without using files. 
 sysdb test is not updated and fails on update",13188096,"Test case attached. The following query fail:


SELECT * FROM ext_auth1 JOIN ext_auth2 ON ext_auth1.ikey = ext_auth2.ikey


Error message:


2018-09-28T00:36:23,860 DEBUG [17b954d9-3250-45a9-995e-1b3f8277a681 main] dao.GenericJdbcDatabaseAccessor: Query to execute is [SELECT *
FROM (SELECT *
FROM ""SIMPLE_DERBY_TABLE1""
WHERE ""ikey"" IS NOT NULL) AS ""t""
INNER JOIN (SELECT *
FROM ""SIMPLE_DERBY_TABLE2""
WHERE ""ikey"" IS NOT NULL) AS ""t0"" ON ""t"".""ikey"" = ""t0"".""ikey"" {LIMIT 1}]
2018-09-28T00:36:23,864 ERROR [17b954d9-3250-45a9-995e-1b3f8277a681 main] dao.GenericJdbcDatabaseAccessor: Error while trying to get column names.
java.sql.SQLSyntaxErrorException: Table/View 'SIMPLE_DERBY_TABLE2' does not exist.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement42.<init>(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.jdbc.Driver42.newEmbedPreparedStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.commons.dbcp.DelegatingConnection.prepareStatement(DelegatingConnection.java:281) ~[commons-dbcp-1.4.jar:1.4]
        at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.prepareStatement(PoolingDataSource.java:313) ~[commons-dbcp-1.4.jar:1.4]
        at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.getColumnNames(GenericJdbcDatabaseAccessor.java:74) [hive-jdbc-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.storage.jdbc.JdbcSerDe.initialize(JdbcSerDe.java:78) [hive-jdbc-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) [hive-serde-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:540) [hive-serde-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:90) [hive-metastore-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:77) [hive-metastore-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:295) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:277) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genTablePlan(SemanticAnalyzer.java:11100) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11468) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11427) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:525) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12319) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:356) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:289) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:669) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1872) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1819) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1814) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335) [hive-cli-4.0.0-SNAPSHOT.jar:?]


Hive is pushing the join into jdbc driver though the table refer to different data source.
 
 Schema change in HIVE-19166 should also go to hive-schema-4.0.0.hive.sql",yes
13202193,"I was looking into some compile profiles for tables with lots of columns; and it turned out that thrift 0.9.3 is allocating a List during every hashcode calculation; but luckily THRIFT-2877 is improving on that - so I propose to upgrade to at least 0.10.0  
 Upgrade thrift to at least 0.10.0",13223590,"Upgrade to consider security fixes.
Especiallyhttps://issues.apache.org/jira/browse/THRIFT-4506 
 Upgrade Thrift to 0.13.0",yes
13198035,"A vendor might need a customized scheduling policy for their devices. It could be scheduled based on topology, resource utilization, virtualization, device attribute and so on.
We'll provide another optional interface ""DevicePluginScheduler"" for the vendor device plugin to implement. Once it's implemented, the framework will prefer it to the default scheduler.
Thiswould bring more flexibility to the framework's scheduling mechanism. 
 DataNode runs async disk checks  maybe  throws NullPointerException In ThrottledAsyncChecker.java  ",13198047,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",yes
13268967,"When an application is unregistered there is a chance that there are still containers running on a node for that application. In all cases we handle the application missing from the RM gracefully (log a message and continue) except for the FS pre-emption thread.
In case the application is removed but some containers are still linked to a node the FSPreemptionThread will crash with a NPE when it tries to retrieve the application id for the attempt:


FSAppAttempt app =
    scheduler.getSchedulerApp(container.getApplicationAttemptId());
ApplicationId appId = app.getApplicationId();
 
 FSPreemptionThread can cause NullPointerException while app is unregistered with containers running on a node",13168721,"I ran into the following exception with sls:
2018-06-26 13:34:04,358 ERROR resourcemanager.ResourceManager: Received RMFatalEvent of type CRITICAL_THREAD_CRASH, caused by a critical thread, FSPreemptionThread, that exited unexpectedly: java.lang.NullPointerException
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreemptOnNode(FSPreemptionThread.java:207)
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreemptForOneContainer(FSPreemptionThread.java:161)
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreempt(FSPreemptionThread.java:121)
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.run(FSPreemptionThread.java:81) 
 Fair scheduler exception with SLS",yes
13257941,"When mvn install is required in hbase-connectors the build fails to find the parent. Need to run mvn install in root in this case. 
 Run mvn install for root in precommit",13230761,"On hbase-spark-it module the mvn install command fails with dependency issue if hbase-spark wasn't built previously. As a result precommit job fails if a file is changed only in hbase-spark-it.

[ERROR] Failed to execute goal on project hbase-spark-it: Could not resolve dependencies for project org.apache.hbase.connectors.spark:hbase-spark-it:jar:1.0.0: Could not find artifact org.apache.hbase.connectors.spark:hbase-spark:jar:1.0.0 in central (https://repo.maven.apache.org/maven2) -> [Help 1]
 
 Compile fails on hbase-spark-it module",yes
13258734,"I found a problem, it could be a mistake..
reproduce steps in hbase shell:
1.create 't11', {NAME => 'f1', VERSIONS => 1}
2.put 't11','r1','f1:q1','f1'
3.flush 't11'
4.put 't11','r1','f1:q1','f2'
5.flush 't11'
6.scan 't11', {RAW => true, VERSIONS => 10, FILTER => ""(QualifierFilter (>=, 'binary:f1'))""}

the result:
 1. 1.3.1 version
 hbase(main):011:0> scan 't11', {RAW => true, VERSIONS => 10, FILTER => ""(QualifierFilter (>=, 'binary:q1'))""}
 ROW COLUMN+CELL 
 r1 column=f1:q1, timestamp=1569400085570, value=f2 
 r1 column=f1:q1, timestamp=1569400068958, value=f1
 2. in 2.1.1 version
 hbase(main):023:0> scan 't11', {RAW => true, VERSIONS => 10, FILTER => ""(QualifierFilter (>=, 'binary:q1'))""}
 ROW COLUMN+CELL 
 r1 column=f1:q1, timestamp=1569400122280, value=f2 
 1 row(s)
 Took 0.0800 seconds

 
 scan#setVersion is invalid.",13245687,"create 'testScanRaw',{NAME => 'f', VERSIONS => 1}

put 'testScanRaw','r1','f:q','1'
put 'testScanRaw','r1','f:q','2'
put 'testScanRaw','r1','f:q','3'

hbase(main):005:0> scan 'testScanRaw',{RAW => true, STARTROW => 'r1', STOPROW=>'r1',VERSIONS=>2}
ROW COLUMN+CELL
r1 column=f:q, timestamp=1563430154757, value=3
r1 column=f:q, timestamp=1563430153120, value=2

hbase(main):006:0> scan 'testScanRaw',{RAW => true, STARTROW => 'r1', STOPROW=>'r1',VERSIONS=>2,FILTER => ""(QualifierFilter (=, 'binary:q'))""}
ROW COLUMN+CELL
r1 column=f:q, timestamp=1563430154757, value=3

BTW,the result is right in hbase1.2. 
 Wrong result in one case of scan that use  raw and versions and filter together",yes
13209224,"While the attempt fails, theREGISTERED comes, hence theInvalidStateTransitionException happens.



2019-01-13 00:41:57,127 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: App attempt: appattempt_1547311267249_0001_000002 can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: REGISTERED at FAILED
at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)
at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)
at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:913)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:121)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:1073)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:1054)
at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)
at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)
at java.lang.Thread.run(Thread.java:745)


 
 Invalid event: REGISTERED and LAUNCH_FAILED at FAILED, and NullPointerException happens in RM while shutdown a NM",13209222,"while shutdown a NodeManager, the RM occurs a null point exception



2019-01-13 08:52:20,299 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type CONTAINER_ALLOCATED for applicationAttempt appattempt_1547340702286_0001_000001
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMContainerAllocatedTransition.transition(RMAppAttemptImpl.java:1210)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMContainerAllocatedTransition.transition(RMAppAttemptImpl.java:1180)
	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:913)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:1073)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:1054)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)
	at java.lang.Thread.run(Thread.java:745)


 
 NullPointerException happens in RM while shutdown a NM",yes
13176635,"When a new service is submitted through the YARN Services UI and an error occurs, the only message in the UI is


Error: Adapter operation failed


Even though the underlying REST API is returning one of the following messages, it is completely hidden from the end user


- \{""diagnostics"":""Artifact tarball does not exist hbase-2.0.0.3.0.0.0.tar.gz""}
- \{""diagnostics"":""Specified src_file does not exist on hdfs: hdfs-site.xml""}
- \{""diagnostics"":""Failed to create service sleeper-service, because it already exists.""}

 
 [UI2] YARN Services UI new submission failures are not debuggable",13130540,"
$ bin/stop-hbase.sh
stopping hbasecat: /tmp/hbase-mdrob-master.pid: No such file or directory

 
 stop-hbase gives unfriendly message when local hbase isn't running",No
13191247,"The namenode supports FSCK.
The Router should be able to forward FSCK to the right Namenode and aggregate the results. 
 RBF: Add FSCK to the Router",13291490,"We sometimesmount different disks for different storage types as the storage location. It's important to check the volume is mounted rightly beforeinitializing storage locations. 
 Add checking of effective filesystem during initializing storage locations",No
13201019,"In https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node, there are two configuration tags in mapred-site.xml. 
 mapred-site.xml is misformatted in single node setup document",13247345,"Yarn Daemon Logs displays the URL instead of log name.
 
 [UI2] Yarn Daemon Logs displays the URL instead of log name",No
13210284,"All branch-2 builds are failing with docker problems right now
https://builds.apache.org/job/PreCommit-HADOOP-Build/15791/console

npm ERR! TypeError: Cannot read property 'latest' of undefined
npm ERR!     at next (/usr/share/npm/lib/cache.js:687:35)
npm ERR!     at /usr/share/npm/lib/cache.js:675:5
npm ERR!     at saved (/usr/share/npm/node_modules/npm-registry-client/lib/get.js:142:7)
npm ERR!     at /usr/lib/nodejs/graceful-fs/polyfills.js:133:7
npm ERR!     at Object.oncomplete (fs.js:107:15)
npm ERR! If you need help, you may report this log at:
npm ERR!     <http://github.com/isaacs/npm/issues>
npm ERR! or email it to:
npm ERR!     <npm-@googlegroups.com>

npm ERR! System Linux 4.4.0-138-generic
npm ERR! command ""/usr/bin/nodejs"" ""/usr/bin/npm"" ""install"" ""-g"" ""ember-cli""
npm ERR! cwd /root
npm ERR! node -v v0.10.25
npm ERR! npm -v 1.3.10
npm ERR! type non_object_property_load


Reported by Steve Loughran. 
 branch-2 build is failing by npm error",13173052,"

RUN apt-get -y install nodejs && \
    ln -s /usr/bin/nodejs /usr/bin/node && \
    apt-get -y install npm && \
    npm install npm@latest -g && \
    npm install -g bower && \
    npm install -g ember-cli


should get reduced to


RUN apt-get -y install nodejs && \
    ln -s /usr/bin/nodejs /usr/bin/node && \
    apt-get -y install npm && \
    npm install npm@latest -g


The locally installed versions of bower and ember-cli aren't being used anymore.  Removing these cuts the docker build time significantly. 
 Node.js and npm package loading in the Dockerfile failing on branch-2",yes
13271036,"The wait timeout for blocking compaction is hardcoded to 5 minutes.


public class AlterTableCompactOperation extends DDLOperation<AlterTableCompactDesc> {
  private static final int FIVE_MINUTES_IN_MILLIES = 5*60*1000;

...
}

This should be configurable via a Hive Configuration parameter. 
 ACID: Wait timeout for blocking compaction should be configurable",13241999,"I observed the issue in Impala development environment when (major) compacting insert_only transactional tables in Hive. The compaction could take ~10 minutes even when it only had to merge 2 rows from 2 inserts. The actual work was done much earlier, the new base file was correctly written to HDFS, and Hive seemed to wait without doing any work.
The compactions are started manually, hive.compactor.initiator.on=false to avoid ""surprise compaction"" during tests.


hive.compactor.abortedtxn.threshold=1000
hive.compactor.check.interval=300s
hive.compactor.cleaner.run.interval=5000ms
hive.compactor.compact.insert.only=true
hive.compactor.crud.query.based=false
hive.compactor.delta.num.threshold=10
hive.compactor.delta.pct.threshold=0.1
hive.compactor.history.reaper.interval=2m
hive.compactor.history.retention.attempted=2
hive.compactor.history.retention.failed=3
hive.compactor.history.retention.succeeded=3
hive.compactor.initiator.failed.compacts.threshold=2
hive.compactor.initiator.on=false
hive.compactor.max.num.delta=500
hive.compactor.worker.threads=4
hive.compactor.worker.timeout=86400s

 
 Slow compaction for tiny tables",yes
13159234,"TestResolveHdfsSymlink#testFcResolveAfs fails on Windowswith error message:
[INFO] Running org.apache.hadoop.fs.TestResolveHdfsSymlink
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 28.574 s <<< FAILURE! - in org.apache.hadoop.fs.TestResolveHdfsSymlink
[ERROR] testFcResolveAfs(org.apache.hadoop.fs.TestResolveHdfsSymlink) Time elapsed: 0.039 s <<< ERROR!
java.io.IOException: Mkdirs failed to create file:/E:/OSS/hadoop/hadoop-hdfs-project/hadoop-hdfs/file:/E:/OSS/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/n014HnmeeA
 at org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:360)
 at org.apache.hadoop.fs.TestResolveHdfsSymlink.testFcResolveAfs(TestResolveHdfsSymlink.java:88)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)
[INFO]
[INFO] Results:
[INFO]
[ERROR] Errors:
[ERROR] TestResolveHdfsSymlink.testFcResolveAfs:88 ╗ IO Mkdirs failed to create file:/...
[INFO]
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0 
 merge master-txnstats branch",13165950,"Test failures due to this issue include any table access operations to 
tables that are modified by hive-schema-4.0.0.xxx.sql 
 One item in pom.xml still has 3.1.0 causing Metastore releated tables related test failures. ",yes
13194576,"When I learned the code of HBase-RPC,I found a spelling error in the comment.
Is the word ""initilize"" should be ""initialize""? 
 correct spelling error of 'initilize' in comment",13217361,"Currently, we use JsonMessageEncoder as the default message factory for Notification events. As the size of some of the events are really huge and cause OOM issues in RDBMS. So, it is needed to enable GzipJSONMessageEncoder as default message factory to optimise the memory usage. 
 Need to set GzipJSONMessageEncoder as default config for EVENT_MESSAGE_FACTORY.",No
13261139,"Currently the leaf queue's name must be unique regardless of its position in the queue hierarchy.
Design doc and first proposal is being made, I'll attach it as soon as it's done. 
 Allow multiple leaf queues with the same name in CapacityScheduler",13263635,"CapacitySchedulerQueueManager allows unsupported Queue hierarchy. When creating a queue with same name as an existing parent queue name - it has to fail with below.


Caused by: java.io.IOException: A is moved from:root.A to:root.B.A after refresh, which is not allowed.Caused by: java.io.IOException: A is moved from:root.A to:root.B.A after refresh, which is not allowed. at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.validateQueueHierarchy(CapacitySchedulerQueueManager.java:335) at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.reinitializeQueues(CapacitySchedulerQueueManager.java:180) at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:762) at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:473) ... 70 more 


In Some cases, the error is not thrown while creating the queue but thrown at submission of job ""Failed to submit application_1571677375269_0002 to YARN : Application application_1571677375269_0002 submitted by user : systest to non-leaf queue : B""
Below scenarios are allowed but it should not


It allows root.A.A1.B when root.B.B1 already exists.
   
1. Add root.A
2. Add root.A.A1
3. Add root.B
4. Add root.B.B1
5. Allows Add of root.A.A1.B 

It allows two root queues:
   
1. Add root.A
2. Add root.B
3. Add root.A.A1
4. Allows Add of root.A.A1.root
	 


Below scenario is handled properly:


It does not allow root.B.A when root.A.A1 already exists.
     
1. Add root.A
2. Add root.B
3. Add root.A.A1
4. Does not Allow Add of root.B.A


This error handling has to be consistent in all scenarios. 
 CapacitySchedulerQueueManager allows unsupported Queue hierarchy",yes
13191247,"The namenode supports FSCK.
The Router should be able to forward FSCK to the right Namenode and aggregate the results. 
 RBF: Add FSCK to the Router",13289370,"When load FSImage, it will sort sections in FileSummary and load Section's in SectionName enum sequence. But the sort method is wrong , when I use branch-2.6.0 to load fsimage write by branch-2 with patchhttps://issues.apache.org/jira/browse/HDFS-14771, it will throw NPE because it load INODE first 


2020-03-03 14:33:26,618 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.lang.NullPointerException
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadPermission(FSImageFormatPBINode.java:101)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectory(FSImageFormatPBINode.java:148)
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadRootINode(FSImageFormatPBINode.java:332)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:218)
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:254)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:180)
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:226)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1036)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1020)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:741)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:677)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:290)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1092)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:780)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:609)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:666)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:838)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:817)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1538)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1606)



I print the load  order:


2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE,  offset = 37, length = 11790829 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 37, length = 826591 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 826628, length = 828192 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 1654820, length = 835240 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 2490060, length = 833630 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 3323690, length = 909445 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 4233135, length = 866147 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 5099282, length = 1272751 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 6372033, length = 1311876 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 7683909, length = 1251510 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 8935419, length = 1296120 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 10231539, length = 770082 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 11001621, length = 789245 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 11790866, length = 67038 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 11857904, length = 84692 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 11942596, length = 71759 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = NS_INFO,  offset = 8, length = 29 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = STRING_TABLE,  offset = 12567596, length = 440 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_REFERENCE,  offset = 12566380, length = 0 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SNAPSHOT,  offset = 12566191, length = 83 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR,  offset = 11790866, length = 774068 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = FILES_UNDERCONSTRUCTION,  offset = 12564934, length = 1257 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SNAPSHOT_DIFF,  offset = 12566274, length = 106 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SECRET_MANAGER,  offset = 12566380, length = 1209 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = CACHE_MANAGER,  offset = 12567589, length = 7 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12014355, length = 84629 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12098984, length = 65215 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12164199, length = 64496 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12228695, length = 68122 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12296817, length = 53417 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12350234, length = 51455 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12401689, length = 80305 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12481994, length = 82940 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SNAPSHOT_DIFF_SUB,  offset = 12566274, length = 106 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Current loadin


The order is wrong 
 Release 2.2.4",No
13246631,"Currently the --hiveconf arguments get added to the System properties. While this allows official HiveConf variables to be set in the conf that is loaded by the HiveStrictManagedMigration utility, there are utility-specific configuration settings which we would want to be set from the command line. For example since Ambari knows what the Hive system user name is it would make sense to be able to set strict.managed.tables.migration.owner on the command line when running this utility. 
 HiveStrictManagedMigration settings do not always get set with --hiveconf arguments",13227334,"The dist.apache.org server is only intended for use by developers in staging releases.
It must not be used on public download pages.
Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead.
The current download page has lots of references to dist.a.o; please replace thes. 
 dist.apache.org must not be used for public downloads",No
13148738,"something changed in the stack of docker image definitions we rely on and we don't have git now, which is causing yetus to fail once we relaunch in the container.
add git, try not to look too hard at what this implies about reproducibility of our builds. 
 test Dockerfile needs to include git",13177787,"When attempting to create table from SELECT with LOCATION (managed tables)

To reproduce CREATE source table CLASS with some data
then use the code below to CREATE table TABLE42 from select on CLASS and LOCATION /tmp/test1



CREATE TABLE TABLE42 ROW FORMAT SERDE
'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe' STORED AS RCFILE
LOCATION '/tmp/test1'
AS SELECT * FROM
CLASS;



it fails with ERROR:


ERROR : Job Commit failed with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(The following files were committed but not found: [/tmp/test1/delta_0000001_0000001_0000/000000_0])'
org.apache.hadoop.hive.ql.metadata.HiveException: The following files were committed but not found: [/tmp/test1/delta_0000001_0000001_0000/000000_0]
at org.apache.hadoop.hive.ql.exec.Utilities.handleMmTableFinalPath(Utilities.java:4329)
at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1393)



If the same is attempted for an EXTERNAL table will work OK


CREATE EXTERNAL TABLE test3 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe' STORED AS RCFILE LOCATION '/tmp/test2' AS SELECT * FROM test;


if we CREATE table on statement 1, and INSERT from SELECT on statement 2 , will work OK.
(Step1 CREATE TABLE)


CREATE TABLE `TABLE42`(
`COL1` double,
`COL2` varchar(8),
`COL3` varchar(1),
`COL4` double,
`COL5` double)
ROW FORMAT SERDE
'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS INPUTFORMAT
'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.RCFileOutputFormat'
LOCATION '/tmp/test1';



(Step2 INSERT from SELECT)


INSERT into TABLE42 select * FROM CLASS;


will work as expected.

Thanks
Pablo

 
 Create table from CTAS with location fails for managed tables",No
13160435,"Similar to HIVE-19572, with -- MASK_LINEAGE. 
 Add option to mask lineage in q files",13160127,"Fix debug log statement introduced in HADOOP-15250. 
 CLONE - NOOP jira to see which tests are flaky on HiveQA",yes
13172824,"From a local UT check against 2.1.0-RC1, HMaster failed to initialize before time out. Checking the test log we could see below message:

2018-07-17 20:06:37,142 DEBUG [Thread-4003] client.RpcRetryingCallerImpl(131): Call exception, tries=6, retries=6, started=4173 ms ago, cancelled=false, msg=java.io.IOException: Inject error
        at org.apache.hadoop.hbase.master.procedure.TestProcedurePriority$MyCP.preGetOp(TestProcedurePriority.java:92)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:841)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:838)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult.callObserver(CoprocessorHost.java:540)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.execOperation(CoprocessorHost.java:614)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGet(RegionCoprocessorHost.java:838)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2520)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2460)
        at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:41998)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:409)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
, details=row 'hbase:namespace' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=hdpdevm1.et2sqa.tbsite.net,59254,1531829189215, seqNum=-1, exception=java.io.IOException: java.io.IOException: Inject error
        at org.apache.hadoop.hbase.master.procedure.TestProcedurePriority$MyCP.preGetOp(TestProcedurePriority.java:92)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:841)
        ...
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:386)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:360)
        at org.apache.hadoop.hbase.MetaTableAccessor.getTableState(MetaTableAccessor.java:1078)
        at org.apache.hadoop.hbase.MetaTableAccessor.tableExists(MetaTableAccessor.java:403)
        at org.apache.hadoop.hbase.master.TableNamespaceManager.start(TableNamespaceManager.java:94)


In current test code we will set FAIL to true w/o checking whether namespace manager is already up, and if not lucky we will run into the above case and get a timeout.
The fix will be quite straight forward. 
 Fix Intermittent failure on TestProcedurePriority",13231747,"We do failover test and throw a leak error, this is hard to reproduce.



2019-05-06 02:30:27,781 ERROR [AsyncFSWAL-0] util.ResourceLeakDetector: LEAK: ByteBuf.release() was not called before it's garbage-collected. See http://netty.io/wiki/reference-counted-objects.html for more information.
Recent access records:
Created at:
 org.apache.hbase.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:334)
 org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
 org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
 org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.flush0(FanOutOneBlockAsyncDFSOutput.java:494)
 org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.flush(FanOutOneBlockAsyncDFSOutput.java:513)
 org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter.sync(AsyncProtobufLogWriter.java:144)
 org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.sync(AsyncFSWAL.java:353)
 org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.consume(AsyncFSWAL.java:536)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 java.lang.Thread.run(Thread.java:748)



If FanOutOneBlockAsyncDFSOutput#endBlock throw Exception before call ""buf.release();"", this buf has not chance to release.
In CallRunner if the call skipped or Dropping timed out call, the call do not call cleanup. 
 ByteBuf LEAK ERROR",No
13177782,"Druid timestamp expression functions returns numeric values in form of millis since epoch. 
Functions that use the output of the timestamp functions as String return different values for tables stored in HIVE and Druid.


SELECT SUBSTRING(to_date(datetime0),4) FROM tableau_orc.calcs;
| 4-07-25  |

SELECT SUBSTRING(to_date(datetime0),4) FROM druid_tableau.calcs;
| 0022400000  |

SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM tableau_orc.calcs;
| 2004-07-17 00:00:00  |

SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM druid_tableau.calcs;
| 1090454400000 00:00:00  |


We need to add explicit CAST to String before generating Druid expressions.
 
 Druid Needs Explicit CASTs from Timestamp to STRING when the output of timestamp function is used as String",13177775,"Druid time expressions return numeric values in form of ms (instead of formatted timestamp). Because of this expressions/function which expects its argument as string type ended up returning different values for time expressions input.
e.g.


SELECT SUBSTRING(to_date(datetime0),4) FROM tableau_orc.calcs;
| 4-07-25  |

SELECT SUBSTRING(to_date(datetime0),4) FROM druid_tableau.calcs;
| 0022400000  |

SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM tableau_orc.calcs;
| 2004-07-17 00:00:00  |

SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM druid_tableau.calcs;
| 1090454400000 00:00:00  |


Druid needs explicit cast to make this work 
 Druid Needs Explicit CASTs from Timestamp to STRING when the output of timestamp function is used as String",yes
13165018,"In RM-HA env, kill ZK leader and then perform RM failover.
Sometimes, active RM gets NPE and fail to come up successfully



2018-06-08 10:31:03,007 INFO client.ZooKeeperSaslClient (ZooKeeperSaslClient.java:run(289)) - Client will use GSSAPI as SASL mechanism.

2018-06-08 10:31:03,008 INFO zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server xxx/xxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'

2018-06-08 10:31:03,009 WARN zookeeper.ClientCnxn (ClientCnxn.java:run(1146)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused

at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)

at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)

at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)

at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)

2018-06-08 10:31:03,344 INFO service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService failed in state INITED

java.lang.NullPointerException

at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)

at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)

at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)

at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)

at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)

at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)

at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)

at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)

at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)

at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)

at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)

at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)

2018-06-08 10:31:03,345 INFO ha.ActiveStandbyElector (ActiveStandbyElector.java:quitElection(409)) - Yielding from election
 
 ActiveStandbyElectorBasedElectorService is failing with NPE",13161059,"Add unit tests for feature that was implemented in HIVE-18788.
The integration tests are present, but it will be useful to catch errors during module build. 
 Clean up inputs in JDBC PreparedStatement. Add unit tests.",No
13210940,"Nodemanager will not start
1. If Autodiscovery is enabled:

If nvidia-smi path is misconfigured or the file does not exist.
There is 0 GPU found
If the file exists but it is not pointing to an nvidia-smi
if the binary is ok but there is an IOException

2. If the manuallyconfiguredGPU devices are misconfigured

Any index:minornumber format failure will cause a problem
0 configured device will cause a problem
NumberFormatException is not handled

It would be a better option to add warnings about the configuration, set 0 available GPUs and let the node work and run non-gpujobs. 
 Backport HBASE-21279 (Split TestAdminShell into several tests) to branch-2",13129304,"
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(java.io.IOException): java.io.IOException: Need clean namespaces or table-cfs config firstly when replicate_all flag is true
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:463)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: org.apache.hadoop.hbase.replication.ReplicationException: Need clean namespaces or table-cfs config firstly when replicate_all flag is true
  at org.apache.hadoop.hbase.master.replication.ReplicationManager.checkPeerConfig(ReplicationManager.java:134)
  at org.apache.hadoop.hbase.master.replication.ReplicationManager.addReplicationPeer(ReplicationManager.java:74)
  at org.apache.hadoop.hbase.master.HMaster.addReplicationPeer(HMaster.java:3325)
  at org.apache.hadoop.hbase.master.MasterRpcServices.addReplicationPeer(MasterRpcServices.java:1891)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:404)
  ... 3 more




      // setup the replication on the source
      if (!source.equals(sink)) {
        ReplicationAdmin replicationAdmin = new ReplicationAdmin(source.getConfiguration());
        // remove any old replication peers
        for (String oldPeer : replicationAdmin.listPeerConfigs().keySet()) {
          replicationAdmin.removePeer(oldPeer);
        }

        // set the sink to be the target
        ReplicationPeerConfig peerConfig = new ReplicationPeerConfig();
        peerConfig.setClusterKey(sink.toString());

        // set the test table to be the table to replicate
        HashMap<TableName, ArrayList<String>> toReplicate = new HashMap<>();
        toReplicate.put(tableName, new ArrayList<>(0));

        replicationAdmin.addPeer(""TestPeer"", peerConfig, toReplicate);


It seems like some change in semantics of ReplicationPeerConfig weren't correctly updated after HBASE-16868 (or a related change maybe). IntegrationTestReplication is trying to replicate a single table but we get into a state where we actually construct a Config object that says ""replicate all tables"" and ""replicate just this one table"".
1. We should catch invalid config objects when we construct it via the builder
2. Some Builder interface methods are missing Javadoc
3. We should update IntegrationTestReplication to use the new API.
FYI [~zghaobac], Michael Stack 
 IntegrationTestReplication broken w/ separate clusters",No
13230476,"Fails intermittently with diff:


Client Execution succeeded but contained differences (error code = 1) after executing cbo_rp_limit.q 
11c11
<  1  4 2
---
>  1 4 2

 
 Use special pause for CallQueueTooBigException",13220103,"GC thrash from an unexpected source in ReduceSinkOperator.


org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinarySerializeWrite.resetWithoutOutput(LazyBinarySerializeWrite.java:136)
        at org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinarySerializeWrite.reset(LazyBinarySerializeWrite.java:132)
        at org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.process(VectorReduceSinkUniformHashOperator.java:180)


GC space is getting thrashed by the 


    root = new Field(STRUCT);


for every row. 
 Vectorization: LazyBinarySerializeWrite allocates Field() within the loop",No
13272549,"When using Hive with Sentry, at the moment is not possible to delete a temporary tables without granting ALL privileges on database/table level.
Temporary tables are strongly bound to the session, and it is only visible/accessible to the user who owns the session, and the table exists until the session is not closed.
Since temporary tables only exists on session level, and they are not connected to any database, checking the privileges with Sentry is unnecessary. 
 Temporary table delete fails with ""SemanticException No valid privileges""",13169515,"pushed to branch-3 as well. 
 Skip authorization for temp tables",yes
13286361,"This fix went in to 2.2+. But I feel like it's applicable and could be used in 2.1 as well. 
 Backport HBASE-22040 to branch-2.1",13172824,"From a local UT check against 2.1.0-RC1, HMaster failed to initialize before time out. Checking the test log we could see below message:

2018-07-17 20:06:37,142 DEBUG [Thread-4003] client.RpcRetryingCallerImpl(131): Call exception, tries=6, retries=6, started=4173 ms ago, cancelled=false, msg=java.io.IOException: Inject error
        at org.apache.hadoop.hbase.master.procedure.TestProcedurePriority$MyCP.preGetOp(TestProcedurePriority.java:92)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:841)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:838)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult.callObserver(CoprocessorHost.java:540)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.execOperation(CoprocessorHost.java:614)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGet(RegionCoprocessorHost.java:838)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2520)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2460)
        at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:41998)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:409)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
, details=row 'hbase:namespace' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=hdpdevm1.et2sqa.tbsite.net,59254,1531829189215, seqNum=-1, exception=java.io.IOException: java.io.IOException: Inject error
        at org.apache.hadoop.hbase.master.procedure.TestProcedurePriority$MyCP.preGetOp(TestProcedurePriority.java:92)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:841)
        ...
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:386)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:360)
        at org.apache.hadoop.hbase.MetaTableAccessor.getTableState(MetaTableAccessor.java:1078)
        at org.apache.hadoop.hbase.MetaTableAccessor.tableExists(MetaTableAccessor.java:403)
        at org.apache.hadoop.hbase.master.TableNamespaceManager.start(TableNamespaceManager.java:94)


In current test code we will set FAIL to true w/o checking whether namespace manager is already up, and if not lucky we will run into the above case and get a timeout.
The fix will be quite straight forward. 
 Fix Intermittent failure on TestProcedurePriority",No
13152597,"Upgrade to Hadoop 3.1.0 
 Upgrade to Hadoop 3.1.0",13153641,"Given that Hadoop 3.1.0 has been released, we need to upgrade hadoop.version to 3.1.0. This change is requiredforHIVE-18037sinceit depends on YARN Service which had its first release in 3.1.0 (and is non-existent in 3.0.0). 
 Upgrade hadoop.version to 3.1.0",yes
13171631,"Hbase version 1.2.0 OldWals are getting filled and showing as below
7.2 K 21.5 K /hbase/.hbase-snapshot
0 0 /hbase/.tmp
0 0 /hbase/MasterProcWALs
18.3 G 60.2 G /hbase/WALs
28.7 G 86.1 G /hbase/archive
0 0 /hbase/corrupt
1.7 T 5.2 T /hbase/data
42 126 /hbase/hbase.id
7 21 /hbase/hbase.version
7.2 T 21.6 T /hbase/oldWALs

It;s not getting purged by Hmaster asoldWals are supposed to be cleaned in master background chore,HBASE-20352(for 1.x version) is created to speed up cleaning oldWals, in our case it's not happening.
hbase.master.logcleaner.ttlis 1 minutes 
 Hbase-1.2.0 OldWals age getting filled and not purged by Hmaster",13150219,"Using multiple threads to scan directory and to clean old WALs 
 [Chore] Backport HBASE-18309 to branch-1",yes
13227334,"The dist.apache.org server is only intended for use by developers in staging releases.
It must not be used on public download pages.
Please use www.apache.org/dist (for KEYS, hashes and sigs) and the mirror system instead.
The current download page has lots of references to dist.a.o; please replace thes. 
 dist.apache.org must not be used for public downloads",13283135,"I see this test fail a lot in my environments. It also uses such a large array that it seems particularly memory wasteful and difficult to get good contention in the test as well. 
 TestSyncTimeRangeTracker fails quite easily and allocates a very expensive array.",No
13167128,"In failure testing, we stopped the KMS and then tried to run some encryption related commands.
hdfs crypto -createZonewill complain with a short ""RemoteException: Connection refused."" This message could be improved to explain that we cannot connect to the KMSClientProvier.
For example,hadoop key listwhile KMS is down will error:


 -bash-4.1$ hadoop key list
 Cannot list keys for KeyProvider: KMSClientProvider[http://hdfs-cdh5-vanilla-1.vpc.cloudera.com:16000/kms/v1/]: Connection refusedjava.net.ConnectException: Connection refused
 at java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
 at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
 at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
 at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at java.net.Socket.connect(Socket.java:579)
 at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
 at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
 at sun.net.www.http.HttpClient.New(HttpClient.java:308)
 at sun.net.www.http.HttpClient.New(HttpClient.java:326)
 at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
 at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
 at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:186)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:125)
 at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:312)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:397)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:392)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:392)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.getKeys(KMSClientProvider.java:479)
 at org.apache.hadoop.crypto.key.KeyShell$ListCommand.execute(KeyShell.java:286)
 at org.apache.hadoop.crypto.key.KeyShell.run(KeyShell.java:79)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
 at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:513)

 
 Improve error message when creating encryption zone while KMS is unreachable",13159520,"Implement custom test runner that retries failed tests as a workaround for flakiness. Also a test rule for retrying failed tests (for cases where custom test runner is not possible, e.g ParametrizedTests which already is a customer TestRunner).  
 Retry test runner and retry rule for flaky tests",No
13154307,"To let resource manager run long running applications you must set the property yarn.webapp.api-service.enable to true, as described in http://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html.
By the way, on the documentation, it is indicated only in the REST API section.
I think it is useful to add this configuration also in the first section of the quick start guide 
 yarn.webapp.api-service.enable should be highlighted in the quickstart",13342183,"This JIRA proposes to backport HIVE-19662 to branch-3.1 and upgrade Avro to 1.8.2. 
 Backport HIVE-19662 to branch-3.1",No
13276478,"Fails while running scripts @Before, the actual problem is  something like this:


2019-12-24T06:11:16,065 DEBUG [main] exec.Utilities: HDFS dir: /tmp/hive with schema null, permission: rwxr-xr-x
2019-12-24T06:11:16,065 ERROR [main] cli.TestHiveCli: Failed due to the error:Connecting to jdbc:hive2://
Hive Session ID = 92e1327d-4267-4efc-af7d-6bd2deeba7d7
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/hiveptest/34.70.195.43-hiveptest-0/apache-github-source-source/conf/ivysettings.xml will be used
Error applying authorization policy on hive configuration: The dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x
Hive Session ID = 5d752eea-9bb5-42b4-953d-750f766fefaa
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/hiveptest/34.70.195.43-hiveptest-0/apache-github-source-source/conf/ivysettings.xml will be used
Error applying authorization policy on hive configuration: The dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x
Connection is already closed.


locally it passes:


2019-12-26T06:23:45,535 DEBUG [main] cli.CLIService: SessionHandle [ee1f45b2-cb74-4b0e-9c4e-5ddbc47a94ae]: closeSession()
2019-12-26T06:23:45,540 DEBUG [main] cli.TestHiveCli: Connecting to jdbc:hive2://
Hive Session ID = feba5d41-796f-406e-a017-bce19c28fe09
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/abstractdog/apache/hive/conf/ivysettings.xml will be used
ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/home/abstractdog/apache/hive/conf/ivysettings.xml will be used
Connected to: Apache Hive (version 4.0.0-SNAPSHOT)
Driver: Hive JDBC (version 4.0.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
hive> create database if not exists test;

 
 TestHiveCli is flaky",13274661,"Error applying authorization policy on hive configuration: The dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x
SessionState.java


  private Path createRootHDFSDir(HiveConf conf) throws IOException {
    Path rootHDFSDirPath = new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR));
    *Utilities.ensurePathIsWritable(rootHDFSDirPath, conf);*
    return rootHDFSDirPath;
  }

 
 Fix TestHiveCli: scratchdir should be writable",yes
13215034,"If you run WASB in secure mode, it doesn't set connectingUsingSAS to true, which can break things 
 WASB in secure mode does not set connectingUsingSAS",13337736,"We have observed some failures when launching containers with runc. We have not yet identified the root cause of those failures, but a side-effect of these failures was the Nodemanager marked itself unhealthy. Since these are rare failures that only affect a single launch, they should not cause the Nodemanager to be marked unhealthy.
Here is an example RM log:

resourcemanager.log.2020-10-02-03.bz2:2020-10-02 03:20:10,255 [RM Event dispatcher] INFO rmnode.RMNodeImpl: Node node:8041 reported UNHEALTHY with details: Linux Container Executor reached unrecoverable exception


And here is an example of the NM log:

2020-10-02 03:20:02,033 [ContainersLauncher #434] INFO runtime.RuncContainerRuntime: Launch container failed for container_e25_1601602719874_10691_01_001723
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=24: OCI command has bad/missing local dire
ctories


The problem is that the runc code in container-executor is re-using exit code 24 (INVALID_CONFIG_FILE) which is intended for problems with the container-executor.cfg file, and those failures are fatal for the NM.  We should use a different exit code for these. 
 runc launch failure should not cause nodemanager to go unhealthy",No
13180212,"Not sure which commit broke it but it is on the top of the flaky list. We use a method in TestEndToEndSplitTransition, but the method will use its own HBaseTestingUtility which is not initialized so NPE. Need to rewrite. 
 NPE in TestTableResource",13180578,"TestTableResource fails consistently with NPE, only in the branch-2s. Both master and branch-1 is fine. 

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 54.397 s <<< FAILURE! - in org.apache.hadoop.hbase.rest.TestTableResource
[ERROR] org.apache.hadoop.hbase.rest.TestTableResource  Time elapsed: 54.395 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.hbase.rest.TestTableResource.setUpBeforeClass(TestTableResource.java:134)

 
 NPE in TestTableResource.setUpBeforeClass (TestTableResource.java:134)",yes
13206348,"In FSImageFormatProtobuf.SectionName#fromString(), as follows:


public static SectionName fromString(String name) {
  for (SectionName n : values) {
    if (n.name.equals(name))
      return n;
  }
  return null;
}


When the code meets an unknown section from the fsimage, the function will return null. Callers always operates the return value with a ""switch"" clause, like FSImageFormatProtobuf.Loader#loadInternal(), as:


switch (SectionName.fromString(n))


NPE will be thrown here. 
 how to ask a question about hdfs？ I don't find the page",13289924,"Continuation of HADOOP-15686
Add the same log4j property to disable error log in hadoop-hdfs. 
 Suppress bogus AbstractWadlGeneratorGrammarGenerator in KMS stderr in hdfs",No
13171244,"After Kerberos ticket expires, RegistryDNS throws NPE error:


2018-07-06 01:26:25,025 ERROR yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(68)) - Thread Thread[TGT Renewer for rm/host1.example.com@EXAMPLE.COM,5,main] threw an Exception.

java.lang.NullPointerException

    at javax.security.auth.kerberos.KerberosTicket.getEndTime(KerberosTicket.java:482)

    at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:894)

    at java.lang.Thread.run(Thread.java:745)
 
 YARN RegistryDNS throws NPE when Kerberos tgt expires",13244776,"As discussed in the following thread, we can deprecate / remove OfflineMetaRepair.
https://lists.apache.org/thread.html/f122efdc79be541d678e22cf8cf573352ab468159596d820c38bf84b@%3Cdev.hbase.apache.org%3E
Maybe we can deprecate in 2.x and remove in 3.0. 
 Deprecate / Remove OfflineMetaRepair in hbase-2+",No
13221827,"The transactional files written in hive have each row decorated with ROW_ID column. However, when we bring in files using LOAD DATA... command to the transactional tables, they do not have these metadata columns (in Hive ACID parlance, these are called original files). These original files are decorated with an inferred ROWID generated while reading these. However, after these are compacted, the ROW_ID metadata column, becomes part of the file itself.
To determine if a file is original or not, currently we use check for the presence of hive.acid.key.index. For query based compaction, currently we do not write hive.acid.key.index (HIVE-21165). This means, there is a possibility that that even after compaction, they get treated as original files.
Irrespective of HIVE-21165, we should avoid hive.acid.key.index to decide whether the file is original or not, and instead look for the presence of ROW__ID to do that. hive.acid.key.index should be treated as a performance optimization, as it was seemingly meant to be. 
 ACID: Avoid using hive.acid.key.index to determine if the file is original or not",13185609,"org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.isOriginal() is checking for presence of hive.acid.key.index in the footer.  This is only created when the file is written by OrcRecordUpdater.  It should instead check for presence of Acid metadata columns so that a file can be produced by something other than OrcRecordUpater.
Also, hive.acid.key.index counts number of different type of events which is not really useful for Acid V2 (as of Hive 3) since each file only has 1 type of event. 
 OrcInputFormat.isOriginal() should not rely on hive.acid.key.index",yes
13286898,"Add row format OpenCSVSerde to the metastore column managed list 
 Scale *MiniCluster config for the environment it runs in.",13290844,"Our tests can create thousands of threads all up in the one JVM. Using less means less memory, less contention, likelier passes, and later, more possible parallelism.
I've been studying the likes of TestNamespaceReplicationWithBulkLoadedData to see what it does as it runs (this test puts up 4 clusters with replication between). It peaks at 2k threads. After some configuration and using less HDFS, its possible to get it down to ~800 threads and about 1/2 the memory-used. HDFS is a main offender. DataXceivers (Server and Client), jetty threads, Volume threads (async disk 'worker' then another for cleanup...), image savers, ipc clients – new thread per incoming connection w/o bound (or reuse), block responder threads, anonymous threads, and so on. Many are not configurable or boundable or are hard-coded; e.g. each volume gets 4 workers regardless. Biggest impact was just downing the count of data nodes. TODO: a follow-on that turns down DN counts in all tests.
I've been using Java Flight Recorder during this study. Here is how you get a flight recorder for the a single test run: {code:java} MAVEN_OPTS="" -XX:StartFlightRecording=disk=true,dumponexit=true,filename=recording.jfr,settings=profile,path-to-gc-roots=true,maxsize=1024m"" mvn test -Dtest=TestNamespaceReplicationWithBulkLoadedData -Dsurefire.firstPartForkCount=0 -Dsurefire.secondPartForkCount=0 {code} i.e. start recording on mvn launch, bound the size of the recording, and have the test run in the mvn context (DON'T fork). Useful is connecting to the running test at the same time from JDK Mission Control. We do the latter because the thread reporting screen is overwhelmed by the count of running threads and if you connect live, you can at least get a 'live threads' graph w/ count as the test progresses. Useful. When the test finishes, it dumps a .jfr file which can be opened in JDK MC.
I've been compiling w/ JDK8 and then running w/ JDK11 so I can use JDK MC Version 7, the non-commercial latest. Works pretty well. Let me put up a patch for tests that cuts down thread counts where we can.
Let me put up a patch that does first pass on curtailing resource usage. 
 Use less resources running tests",yes
13252400,"The section of 108. Basic Spark contains typo.
HBaseContext doesn't have hBaseRDD method;it has hbaseRDD method. 
 [DOC] HBaseContext doesn't have hBaseRDD method",13196322,"From https://builds.apache.org/job/HBase-Flaky-Tests/job/master/1863/testReport/org.apache.hadoop.hbase.client/TestAdmin2/testGetProcedures/ :


Mon Nov 05 04:52:13 UTC 2018, RpcRetryingCaller{globalStartTime=1541393533029, pause=250, maxAttempts=7}, org.apache.hadoop.hbase.procedure2.BadProcedureException: org.apache.hadoop.hbase.procedure2.BadProcedureException: The procedure class org.apache.hadoop.hbase.procedure2.FailedProcedure must be accessible and have an empty constructor
 at org.apache.hadoop.hbase.procedure2.ProcedureUtil.validateClass(ProcedureUtil.java:82)
 at org.apache.hadoop.hbase.procedure2.ProcedureUtil.convertToProtoProcedure(ProcedureUtil.java:162)
 at org.apache.hadoop.hbase.master.MasterRpcServices.getProcedures(MasterRpcServices.java:1249)
 at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
 at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
 at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
 at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
 at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)

 
 TestAdmin2#testGetProcedures fails due to FailedProcedure inaccessible",No
13268007,"RunningDU across lots of disks is very expensive . We applied the patchHDFS-14313 to getused space from ReplicaInfo in memory.However, new du threads throw the exception


// 2019-11-08 18:07:13,858 ERROR [refreshUsed-/home/vipshop/hard_disk/7/dfs/dn/current/BP-1203969992-XXXX-1450855658517] org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed: ReplicaCachingGetSpaceUsed refresh error
java.util.ConcurrentModificationException: Tree has been modified outside of iterator
at org.apache.hadoop.hdfs.util.FoldedTreeSet$TreeSetIterator.checkForModification(FoldedTreeSet.java:311)
at org.apache.hadoop.hdfs.util.FoldedTreeSet$TreeSetIterator.hasNext(FoldedTreeSet.java:256)
at java.util.AbstractCollection.addAll(AbstractCollection.java:343)
at java.util.HashSet.<init>(HashSet.java:120)
at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.deepCopyReplica(FsDatasetImpl.java:1052)
at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed.refresh(ReplicaCachingGetSpaceUsed.java:73)
at org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:178)
at java.lang.Thread.run(Thread.java:748)

 
 ReplicaCachingGetSpaceUsed throws  ConcurrentModificationException",13196326,"Currently, we instantiate /static with the default settings.
However, if this folder is behind a symbolic link, this won't load.l
This is exactly the same issue and solution as described in GEODE-5445. 
 Allow HttpServer2 to discover resources in /static when symlinks are used",No
13273630,"Distcp over S3A always copies all source files no matter the files are changed or not. This is opposite to the statement in the doc below.
http://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html

And to use -update to only copy changed files.


CopyMapper compares file length as well as block size before copying. While the file length should match, the block size does not. This is apparently because the returned block size from S3A is always 32MB.
https://github.com/apache/hadoop/blob/release-3.2.0-RC1/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L348
I'd suppose we should update the documentation or make code change. 
 Replace Base64 in accumulo-handler Package",13197949,"It is OK to send the same open/close region request to a RS multiple times but if we persist the dispatched field, after master restarts the procedure will finish immediately, even if the reportRegionStateTransition has not been called yet. This may lead to assign a region to multiple region servers... 
 Should not persist the dispatched field for RegionRemoteProcedureBase",No
13244875,"In secure HDFS, during Namenode loading fsimage, when hitting Namenode through the REST API, below exception would be thrown out. (This is in version 2.8.2)
org.apache.hadoop.hdfs.web.resources.ExceptionHandler: INTERNAL_SERVER_ERROR
 java.lang.NullPointerException
 at org.apache.hadoop.hdfs.server.common.JspHelper.getTokenUGI(JspHelper.java:283)
 at org.apache.hadoop.hdfs.server.common.JspHelper.getUGI(JspHelper.java:226)
 at org.apache.hadoop.hdfs.web.resources.UserProvider.getValue(UserProvider.java:54)
 at org.apache.hadoop.hdfs.web.resources.UserProvider.getValue(UserProvider.java:42)
 at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:203)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
 at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
 at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:87)
 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
 at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1353)
 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
 at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
 at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
 at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
 at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
 at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
 at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
 at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
 at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
 at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
 at org.mortbay.jetty.Server.handle(Server.java:326)
 at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
 at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
 at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
 at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
 at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
 at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
 at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
This is because during this phase, namesystem hasn't been initialized. In non-HA context, it can throw a RetriableException to let the client retry or simply throw other exception to make the client fail. In HA context, one possible solution is to throw StandbyException to let the client fail over to another namenode. 
 NPE during secure namenode startup",13201162,"In 2.8.2 we saw this exception when a security-enabled NameNode is still loading edits:


2018-11-28 00:21:02,909 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2068ms GC pool 'ParNew' had collection(s): count=1 time=2325ms 2018-11-28 00:21:05,768 WARN org.apache.hadoop.hdfs.web.resources.ExceptionHandler: INTERNAL_SERVER_ERROR java.lang.NullPointerException at org.apache.hadoop.hdfs.server.common.JspHelper.getTokenUGI(JspHelper.java:283) at org.apache.hadoop.hdfs.server.common.JspHelper.getUGI(JspHelper.java:226) at org.apache.hadoop.hdfs.web.resources.UserProvider.getValue(UserProvider.java:54) at org.apache.hadoop.hdfs.web.resources.UserProvider.getValue(UserProvider.java:42) at com.sun.jersey.server.impl.inject.InjectableValuesProvider.getInjectableValues(InjectableValuesProvider.java:46) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$EntityParamInInvoker.getParams(AbstractResourceMethodDispatchProvider.java:153) at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:203) at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75) at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288) at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108) at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147) at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469) at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400) at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349) at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339) at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416) at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537) at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:87) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1353) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399) at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230) at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) at org.mortbay.jetty.Server.handle(Server.java:326) at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)


Looking at the code, this is where the NPE happened (the line with verifyToken):


    if (context != null) {
      final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);
      if (nn != null) {
        // Verify the token.
        nn.getNamesystem().verifyToken(id, token.getPassword());
      }
    }


In NameNode::initialize, the namesystem is initialized after the http server is started, and usually it takes a while before fsimage is loaded and namesystem becomes non-null:


    if (NamenodeRole.NAMENODE == role) {
      startHttpServer(conf);
    }

    loadNamesystem(conf);


Perhaps it should throw a proper exception instead of failing with NPE. 
 NPE when serving http requests while NameNode is starting up",yes
13217415,"The FairScheduler's configuration has the following defaults (from the code: javadoc):

In new style resources, any resource that is not specified will be set to missing or 0%, as appropriate. Also, in the new style resources, units are not allowed. Units are assumed from the resource manager's settings for the resources when the value isn't a percentage. The missing parameter is only used in the case of new style resources without percentages. With new style resources with percentages, any missing resources will be assumed to be 100% because percentages are only used with maximum resource limits.


This is not documented in the hadoop yarn site FairScheduler.html. It is quite intuitive, but still need to be documented though. 
 Fair Scheduler configuration defaults are not documented in case of min and maxResources",13138410,"(col1) IN (col2) can be transformed to (col1) = (col2), to avoid the hash-set implementation. 
 Optimize: Transform IN clauses to = when there's only one element",No
13155200,"Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches. 
 Arrow batch serializer",13155194,"Allows external clients to consume output from LLAP daemons in Arrow stream format. 
 Arrow format for LlapOutputFormatService (umbrella)",yes
13129304,"
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(java.io.IOException): java.io.IOException: Need clean namespaces or table-cfs config firstly when replicate_all flag is true
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:463)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: org.apache.hadoop.hbase.replication.ReplicationException: Need clean namespaces or table-cfs config firstly when replicate_all flag is true
  at org.apache.hadoop.hbase.master.replication.ReplicationManager.checkPeerConfig(ReplicationManager.java:134)
  at org.apache.hadoop.hbase.master.replication.ReplicationManager.addReplicationPeer(ReplicationManager.java:74)
  at org.apache.hadoop.hbase.master.HMaster.addReplicationPeer(HMaster.java:3325)
  at org.apache.hadoop.hbase.master.MasterRpcServices.addReplicationPeer(MasterRpcServices.java:1891)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:404)
  ... 3 more




      // setup the replication on the source
      if (!source.equals(sink)) {
        ReplicationAdmin replicationAdmin = new ReplicationAdmin(source.getConfiguration());
        // remove any old replication peers
        for (String oldPeer : replicationAdmin.listPeerConfigs().keySet()) {
          replicationAdmin.removePeer(oldPeer);
        }

        // set the sink to be the target
        ReplicationPeerConfig peerConfig = new ReplicationPeerConfig();
        peerConfig.setClusterKey(sink.toString());

        // set the test table to be the table to replicate
        HashMap<TableName, ArrayList<String>> toReplicate = new HashMap<>();
        toReplicate.put(tableName, new ArrayList<>(0));

        replicationAdmin.addPeer(""TestPeer"", peerConfig, toReplicate);


It seems like some change in semantics of ReplicationPeerConfig weren't correctly updated after HBASE-16868 (or a related change maybe). IntegrationTestReplication is trying to replicate a single table but we get into a state where we actually construct a Config object that says ""replicate all tables"" and ""replicate just this one table"".
1. We should catch invalid config objects when we construct it via the builder
2. Some Builder interface methods are missing Javadoc
3. We should update IntegrationTestReplication to use the new API.
FYI [~zghaobac], Michael Stack 
 IntegrationTestReplication broken w/ separate clusters",13140108,"Left & Right outer joins without keys are valid in SQL and they have different semantics from cross-products


create temporary table foo(x int) stored as orc;
insert into foo values(1),(2);
create temporary table bar(y int) stored as orc;
select count(*) from bar right outer join foo; -- = 2
select count(*) from bar, foo; -- = 0 


canSpecializeMapJoin should bail on these cases. 
 Vectorization: Disable vectorization of key-less outer joins",No
13244315,"There are 4 recurring test failures based on AbstractITCommitMRJob.testMRJob in S3A:

[ERROR]   ITestMagicCommitMRJob>AbstractITCommitMRJob.testMRJob:146->AbstractFSContractTestBase.assertIsDirectory:327 ? FileNotFound
[ERROR]   ITestDirectoryCommitMRJob>AbstractITCommitMRJob.testMRJob:146->AbstractFSContractTestBase.assertIsDirectory:327 ? FileNotFound
[ERROR]   ITestPartitionCommitMRJob>AbstractITCommitMRJob.testMRJob:146->AbstractFSContractTestBase.assertIsDirectory:327 ? FileNotFound
[ERROR]   ITestStagingCommitMRJob>AbstractITCommitMRJob.testMRJob:146->AbstractFSContractTestBase.assertIsDirectory:327 ? FileNotFound


The full stack traces:

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 58.639 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.staging.integration.ITestPartitionCommitMRJob
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.staging.integration.ITestPartitionCommitMRJob)  Time elapsed: 34.236 s  <<< ERROR!
java.io.FileNotFoundException: Path s3a://gabota-versioned-bucket-ireland/test/DELAY_LISTING_ME/testMRJob is recorded as deleted by S3Guard at 2019-07-11T12:06:20.159Z
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2634)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2601)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:561)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.assertIsDirectory(AbstractFSContractTestBase.java:327)
	at org.apache.hadoop.fs.s3a.commit.AbstractITCommitMRJob.testMRJob(AbstractITCommitMRJob.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 70.31 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.staging.integration.ITestStagingCommitMRJob
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.staging.integration.ITestStagingCommitMRJob)  Time elapsed: 29.917 s  <<< ERROR!
java.io.FileNotFoundException: Path s3a://gabota-versioned-bucket-ireland/test/DELAY_LISTING_ME/testMRJob is recorded as deleted by S3Guard at 2019-07-11T12:07:17.791Z
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2634)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2601)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:561)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.assertIsDirectory(AbstractFSContractTestBase.java:327)
	at org.apache.hadoop.fs.s3a.commit.AbstractITCommitMRJob.testMRJob(AbstractITCommitMRJob.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 67.665 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.staging.integration.ITestDirectoryCommitMRJob
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.staging.integration.ITestDirectoryCommitMRJob)  Time elapsed: 29.81 s  <<< ERROR!
java.io.FileNotFoundException: Path s3a://gabota-versioned-bucket-ireland/test/DELAY_LISTING_ME/testMRJob is recorded as deleted by S3Guard at 2019-07-11T12:08:25.533Z
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2634)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2601)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:561)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.assertIsDirectory(AbstractFSContractTestBase.java:327)
	at org.apache.hadoop.fs.s3a.commit.AbstractITCommitMRJob.testMRJob(AbstractITCommitMRJob.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 55.787 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.commit.magic.ITestMagicCommitMRJob
[ERROR] testMRJob(org.apache.hadoop.fs.s3a.commit.magic.ITestMagicCommitMRJob)  Time elapsed: 34.97 s  <<< ERROR!
java.io.FileNotFoundException: Path s3a://gabota-versioned-bucket-ireland/test/testMRJob is recorded as deleted by S3Guard at 2019-07-11T12:09:32.970Z
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2634)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2601)
	at org.apache.hadoop.fs.contract.ContractTestUtils.assertIsDirectory(ContractTestUtils.java:561)
	at org.apache.hadoop.fs.contract.AbstractFSContractTestBase.assertIsDirectory(AbstractFSContractTestBase.java:327)
	at org.apache.hadoop.fs.s3a.commit.AbstractITCommitMRJob.testMRJob(AbstractITCommitMRJob.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

 
 AbstractITCommitMRJob.testMRJob test failures",13224031,"Reported failure of ITestDirectoryCommitMRJob in validation runs of HADOOP-16186; assertIsDirectory with s3guard enabled and a parallel test run: Path ""is recorded as deleted by S3Guard""


    waitForConsistency();
    assertIsDirectory(outputPath) /* here */


The file is there but there's a tombstone. Possibilities

some race condition with another test
tombstones aren't timing out
committers aren't creating that base dir in a way which cleans up S3Guard's tombstones.

Remember: we do have to delete that dest dir before the committer runs unless overwrite==true, so at the start of the run there will be a tombstone. It should be overwritten by a success. 
 Improved S3A MR tests",yes
13176635,"When a new service is submitted through the YARN Services UI and an error occurs, the only message in the UI is


Error: Adapter operation failed


Even though the underlying REST API is returning one of the following messages, it is completely hidden from the end user


- \{""diagnostics"":""Artifact tarball does not exist hbase-2.0.0.3.0.0.0.tar.gz""}
- \{""diagnostics"":""Specified src_file does not exist on hdfs: hdfs-site.xml""}
- \{""diagnostics"":""Failed to create service sleeper-service, because it already exists.""}

 
 [UI2] YARN Services UI new submission failures are not debuggable",13201551,"See this in the outout and then the test hang

2018-11-29 20:47:50,061 WARN  [MockRSProcedureDispatcher-pool5-t10] assignment.AssignmentManager(894): The region server localhost,102,1 is already dead, skip reportRegionStateTransition call

 
 TestAssignmentManager is flakey",No
13140750,"precommit attempts at mvnsite fail with odd looking results:



[INFO] 
[INFO] --- maven-compiler-plugin:3.6.1:testCompile (default-testCompile) @ hbase-it ---
[INFO] Compiling 101 source files to /testptch/hbase/hbase-it/target/test-classes
[INFO] /testptch/hbase/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java: Some input files use or override a deprecated API.
[INFO] /testptch/hbase/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java: Recompile with -Xlint:deprecation for details.
[INFO] /testptch/hbase/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/factories/MonkeyFactory.java: Some input files use unchecked or unsafe operations.
[INFO] /testptch/hbase/hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/factories/MonkeyFactory.java: Recompile with -Xlint:unchecked for details.
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /testptch/hbase/hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:[250,48] cannot find symbol
  symbol:   method readEntity(java.lang.Class<java.lang.String>)
  location: variable response of type javax.ws.rs.core.Response
[INFO] 1 error





[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.4:site (default-site) on project hbase-spark: failed to get report for org.apache.maven.plugins:maven-javadoc-plugin: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:testCompile (default-testCompile) on project hbase-it: Compilation failure
[ERROR] /testptch/hbase/hbase-it/src/test/java/org/apache/hadoop/hbase/RESTApiClusterManager.java:[250,48] cannot find symbol
[ERROR]   symbol:   method readEntity(java.lang.Class<java.lang.String>)
[ERROR]   location: variable response of type javax.ws.rs.core.Response
[ERROR] 


this is at least true on master. haven't checked other branches 
 Maven site generation fails",13140732,"website generation has been failing since Feb 20th


Checking out files: 100% (68971/68971), done.
Usage: grep [OPTION]... PATTERN [FILE]...
Try 'grep --help' for more information.
PUSHED is 2
 is not yet mentioned in the hbase-site commit log. Assuming we don't have it yet. 2
Building HBase
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0
Failure: mvn clean site
Build step 'Execute shell' marked build as failure


The status email says


Build status: Still Failing

The HBase website has not been updated to incorporate HBase commit ${CURRENT_HBASE_COMMIT}.


Looking at the code where that grep happens, it looks like the env variable CURRENT_HBASE_COMMIT isn't getting set. That comes from some git command. I'm guessing the version of git changed on the build hosts and upended our assumptions.
we should fix this to 1) rely on git's porcelain interface, and 2) fail as soon as that git command fails 
 website generation is failing",yes
13244817,"ViewFileSystem uses superclass's close in current implementation.
It removes from FileSystem.CACHE without closing the child FileSystems.
To close properly, when FileSystem is closing, its child FileSystems also should be closed. 
 ViewFileSystem should close the child FileSystems in close()",13168518,"ViewFileSystem.close() does nothing but remove itself from FileSystem.CACHE. It's children filesystems are cached in FileSystem.CACHE and shared by all the ViewFileSystem instances. We could't simply close all the children filesystems because it will break the semantic of FileSystem.newInstance().
We might add an inner cache to ViewFileSystem, let it cache all the children filesystems. The children filesystems are not shared any more. When ViewFileSystem is closed we close all the children filesystems in the inner cache. The ViewFileSystem is still cached by FileSystem.CACHE so there won't be too many FileSystem instances.
The FileSystem.CACHE caches the ViewFileSysem instance and the other instances(the children filesystems) are cached in the inner cache. 
 ViewFileSystem.close doesn't close child filesystems and causes FileSystem objects leak.",yes
13134065,"Steps:
1) Enable Ats v2
2) Start Httpd Yarn service
3) Go to UI2 attempts page for yarn service
4) Click on setting icon
5) Click on stop service
6) This action will pop up a box to confirm stop. click on ""Yes""
Expected behavior:
Yarn service should be stopped
Actual behavior:
Yarn UI is not notifying on whether Yarn service is stopped or not.
On checking network stack trace, the PUT request failed with HTTP error 404


Sorry, got error 404
Please consult RFC 2616 for meanings of the error code.
Error Details
org.apache.hadoop.yarn.webapp.WebAppException: /v1/services/httpd-hrt-qa-n: controller for v1 not found
	at org.apache.hadoop.yarn.webapp.Router.resolveDefault(Router.java:247)
	at org.apache.hadoop.yarn.webapp.Router.resolve(Router.java:155)
	at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)
	at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)
	at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)
	at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)
	at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)
	at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)
	at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)
	at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)
	at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:98)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1578)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
 
 Stop and Delete Yarn Service from RM UI fails with HTTP ERROR 404",13226109,"There's an unused local variable and findbugs doesn't like it.

Bug type DLS_DEAD_LOCAL_STORE (click for details)
In class org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Saver
In method org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Saver.save(OutputStream, INodeSymlink)
Local variable named state
At FSImageFormatPBINode.java:[line 623]

File this jira to clean it up. 
 Clean up findbugs warning in branch-2",No
13286053,"HDFS-10756 added the getTrashRoot() support for WebHdfs. However, it was done by creating a FileSystem instance in the namenode. This is unacceptable for many reasons and also the implementation is not correct.  The current implementation only works when security is off. When security is on, the internal client received AccessControlException and does not work.
A similar bug was preset in HDFS-11156. Again, this is not merely a ""performance bug"".  These don't work with security on.  Fortunately HDFS-11156 was reverted and reworked.  I've recently reverted it and ported the rework to branch-2.10.
Unless HDFS-10756 can be remedied quickly, it needs to be reverted. 
 Webhdfs getTrashRoot() causes internal AccessControlException",13273935,"Quoting Daryn Sharp inHDFS-10756 :
Surprised nobody has discovered this will lead to an inevitable OOM in the NN. The NN should not be creating filesystems to itself, and must never create filesystems in a remote user's context or the cache will explode.
I guess the problem lies in side NamenodeWebHdfsMethods#getTrashRoot


	  private static String getTrashRoot(String fullPath,
	      Configuration conf) throws IOException {
	    FileSystem fs = FileSystem.get(conf != null ? conf : new Configuration());
	    return fs.getTrashRoot(
	        new org.apache.hadoop.fs.Path(fullPath)).toUri().getPath();
	  }

 
 WebHDFS getTrashRoot leads to OOM due to FileSystem object creation",yes
13127983,"Add a FunctionBuilder class 
 Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder",13127964,"Add a FunctionBuilder class 
 Add builder for metastore Thrift classes missed in the first pass - FunctionBuilder",yes
13301827,"Find out why the result set is different forcorrelationoptimizer14.q after moving to TestMiniLlapLocalCliDriver. Checkhttps://reviews.apache.org/r/72421/#comment308835for details. 
 Investigate why the results have changed for correlationoptimizer14.q",13223557,"Metastore runs background thread out of which one is partition discovery. While removing expired partitions following exception is thrown


2019-03-24 04:24:59.583 WARN [PartitionDiscoveryTask-0] metastore.MetaStoreDirectSql: Failed to execute [select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS"" inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID"" and ""TBLS"".""TBL_NAME"" = ? inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID"" and ""DBS"".""NAME"" = ? inner join ""PARTITION_KEY_VALS"" ""FILTER0"" on ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 inner join ""PARTITION_KEY_VALS"" ""FILTER1"" on ""FILTER1"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER1"".""INTEGER_IDX"" = 1 inner join ""PARTITION_KEY_VALS"" ""FILTER2"" on ""FILTER2"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER2"".""INTEGER_IDX"" = 2 where ""DBS"".""CTLG_NAME"" = ? and ( ( (((case when ""FILTER0"".""PART_KEY_VAL"" <> ? and ""TBLS"".""TBL_NAME"" = ? and ""DBS"".""NAME"" = ? and ""DBS"".""CTLG_NAME"" = ? and ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 then cast(""FILTER0"".""PART_KEY_VAL"" as date) else null end) = ?) and (""FILTER1"".""PART_KEY_VAL"" = ?)) and (""FILTER2"".""PART_KEY_VAL"" = ?)) )] with parameters [logs, sys, hive, __HIVE_DEFAULT_PARTITION__, logs, sys, hive, 2019-03-23, warehouse-1553300821-692w, metastore-db-create-job]
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS"" inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID"" and ""TBLS"".""TBL_NAME"" = ? inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID"" and ""DBS"".""NAME"" = ? inner join ""PARTITION_KEY_VALS"" ""FILTER0"" on ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 inner join ""PARTITION_KEY_VALS"" ""FILTER1"" on ""FILTER1"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER1"".""INTEGER_IDX"" = 1 inner join ""PARTITION_KEY_VALS"" ""FILTER2"" on ""FILTER2"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER2"".""INTEGER_IDX"" = 2 where ""DBS"".""CTLG_NAME"" = ? and ( ( (((case when ""FILTER0"".""PART_KEY_VAL"" <> ? and ""TBLS"".""TBL_NAME"" = ? and ""DBS"".""NAME"" = ? and ""DBS"".""CTLG_NAME"" = ? and ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 then cast(""FILTER0"".""PART_KEY_VAL"" as date) else null end) = ?) and (""FILTER1"".""PART_KEY_VAL"" = ?)) and (""FILTER2"".""PART_KEY_VAL"" = ?)) )"".
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)
at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:391)
at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:267)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:2042)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionIdsViaSqlFilter(MetaStoreDirectSql.java:621)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:487)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3426)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3418)
at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:3702)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:3453)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:3406)
at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
at com.sun.proxy.$Proxy33.getPartitionsByExpr(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:4521)
at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
at com.sun.proxy.$Proxy34.drop_partitions_req(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:1288)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:474)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:435)
at org.apache.hadoop.hive.metastore.utils.RetryUtilities$ExponentiallyDecayingBatchWork.run(RetryUtilities.java:91)
at org.apache.hadoop.hive.metastore.Msck.dropPartitionsInBatches(Msck.java:499)
at org.apache.hadoop.hive.metastore.Msck.repair(Msck.java:226)
at org.apache.hadoop.hive.metastore.PartitionManagementTask$MsckThread.run(PartitionManagementTask.java:213)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
NestedThrowablesStackTrace:
org.postgresql.util.PSQLException: ERROR: operator does not exist: date = character varying
Hint: No operator matches the given name and argument type(s). You might need to add explicit type casts.
Position: 886
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200)
at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424)
at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:161)
at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:114)
at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52)
at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java)
at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:375)
at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:552)
at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:645)
at org.datanucleus.store.query.Query.executeQuery(Query.java:1855)
at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807)
at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:368)
at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:267)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:2042)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionIdsViaSqlFilter(MetaStoreDirectSql.java:621)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:487)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3426)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3418)
at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:3702)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:3453)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:3406)
at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
at com.sun.proxy.$Proxy33.getPartitionsByExpr(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:4521)
at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
at com.sun.proxy.$Proxy34.drop_partitions_req(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:1288)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:474)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:435)
at org.apache.hadoop.hive.metastore.utils.RetryUtilities$ExponentiallyDecayingBatchWork.run(RetryUtilities.java:91)
at org.apache.hadoop.hive.metastore.Msck.dropPartitionsInBatches(Msck.java:499)
at org.apache.hadoop.hive.metastore.Msck.repair(Msck.java:226)
at org.apache.hadoop.hive.metastore.PartitionManagementTask$MsckThread.run(PartitionManagementTask.java:213)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)

 
 Backport HBASE-21895 ""Error prone upgrade"" to branch-2",No
13169048,"HTableDescriptor is deprecated as of release 2.0.0, and will be removed in 3.0.0. This patch replaces all usages of HTableDescriptor and HColumnDescriptor in the hbase-shell module so thatHTableDescriptor can be removed.
There a few other consequences of this change:

Ruby methods relating to HTableDescriptor and HColumnDescriptor have been removed. This is noted in ""Release Note"" on this issue.
We no longer import constants from HTableDescriptor and HColumnDescriptor into the ruby HBaseConstants module. Instead, we import them fromColumnFamilyDescriptorBuilder and TableDescriptorBuilder.

 
 Use TableDescriptor to replace HTableDescriptor in hbase-shell module",13169047,"HTableDescriptor is deprecated as of release 2.0.0.  
 Use TableDescriptor to replace HTableDescriptor in admin.rb",yes
13203172,"YarnConfiguration.NM_LINUX_CONTAINER_CGROUPS_MOUNT and
YarnConfiguration.NM_LINUX_CONTAINER_CGROUPS_MOUNT_PATH are used in conjunction many places in the code, so for the sake of readabilty and simplicity, it is better to wrap the values of these configs to an object and use it instead of having 2 fields in
CGroupsHandlerImpl and in CgroupsLCEResourcesHandler as well. 
 Create an object for cgroups mount enable and cgroups mount path as they belong together",13296008,"CompactionTool on non-HDFS filesystems does not work because the used filesystems are mixed up. To collect the StoreFiles the Filesystem.get(conf) method is used instead of relying on the root dir filesystem.
With YARN the delegation tokens are not handled correctly with a different filesystem and the mappers only get delegation token for the staging directory's filesystem. 
Another issue is in MapReduce mode when YARN is used. In this case the TMP directory (hbase.tmp.dir) is created under yarn.nodemanager.local-dirs but this directory is created (even on HDFS a regular user can't create this directory) on the Storefile's filesystem where a local filesystem temp directory shouldn't be used.
In this ticket I plan to clean up the used filesystems, remove the custom temp directory (use the regions' .tmp directory) and collect delegation tokens for staging dir + store dir paths' FileSystems. 
 Enable CompactionTool executions on non-HDFS filesystems",No
13260189,"I am trying to bring commits from trunk/branch-3.2 to branch-3.1, but some of them do not compile because of the commons-logging to slf4j migration. 
One of the issue is GenericTestUtils.DelayAnswer do not accept slf4j logger API.
Backport HADOOP-14624 to branch-3.1 to make backport easier. It updates the DelayAnswer signature, but it's in the test scope, so we're not really breaking backward compat. 
 Backport HADOOP-14624 to branch-3.1",13190008,"When testing namespace not enabled account using Oauth, some tests were skipped. So need to update the tests. 
 ABFS: Enable some tests for namespace not enabled account using OAuth",No
13298527,"Admin.getRegionServers() returns all live RS of the cluster. It should consider optionally excluding decommissioned RS for operators to get live non-decommissioned RS list from single API. 
 Admin.getRegionServers() should return live servers excluding decom RS optionally",13325501,"
add or improve some logs for adding local & global deadnodes
logic improve
fix typo

 
 Tiny Improve for DeadNode detector",No
13191611,"Miscellaneous additions to hbck2 over in the hbase-operator-tools project.

Emit version
Fail if going against an hbase that doesn't support hbck2.
Add override to assigns and unassigns
Have bypass match changing its force option to be override
Add recursive to bypass so can pass a parent and it will go find all child procedures.

 
 [hbck2] Add version, version handling, and misc override to assigns/unassigns",13245654,"Only for 3.0.0. Remove the methods which mark deprecated inHBASE-22673. 
 Remove the deprecated methods in Hbck interface",No
13178752,"Fails 30% of the time in list_procedures. Fails creating a Procedure then trying to capture shell output to confirm it sees the just-queued Procedure only it looks like the Procedure finishes too quickly. It works for a while then there are a spate of failures. Then it works again.
Here is how it looks in test output:


Took 5.6355 secondsTook 0.0561 seconds...........F
===============================================================================
Failure: test_list_procedures(Hbase::ListProceduresTest)
src/test/ruby/shell/list_procedures_test.rb:65:in `block in test_list_procedures'
     62:         end
     63:       end
     64: 
  => 65:       assert_equal(1, matching_lines)
     66:     end
     67:   end
     68: end
<1> expected but was
<0>


Then in the log output for the test, I see this for the running of the Procedure:


2018-08-14 00:42:50,381 DEBUG [Time-limited test] procedure2.ProcedureExecutor(948): Stored pid=27, state=RUNNABLE, hasLock=false; org.apache.hadoop.hbase.client.procedure.ShellTestProcedure
2018-08-14 00:42:50,397 INFO  [RS-EventLoopGroup-1-10] ipc.ServerRpcConnection(556): Connection from 67.195.81.150:50597, version=2.0.2-SNAPSHOT, sasl=false, ugi=jenkins (auth:SIMPLE), service=MasterService
F
===============================================================================
Failure: test_list_procedures(Hbase::ListProceduresTest)
src/test/ruby/shell/list_procedures_test.rb:65:in `block in test_list_procedures'
2018-08-14 00:42:50,586 INFO  [PEWorker-16] procedure2.ProcedureExecutor(1316): Finished pid=27, state=SUCCESS, hasLock=false; org.apache.hadoop.hbase.client.procedure.ShellTestProcedure in 234msec
     62:         end
     63:       end
     64: 
  => 65:       assert_equal(1, matching_lines)
     66:     end
     67:   end
     68: end
<1> expected but was
<0>
===============================================================================


The Procedure runs successfully but the regex test on the other end of the Admin is not finding what it expects – the Procedure ran in 234ms.
Will disable in a subprocedure for now till someone has time to play w/ this. 
 TestShell list_procedures is flakey",13175338,"From test output against hadoop3:


2018-07-28 12:04:24,838 DEBUG [Time-limited test] procedure2.ProcedureExecutor(948): Stored pid=12, state=RUNNABLE, hasLock=false; org.apache.hadoop.hbase.client.procedure.      ShellTestProcedure
2018-07-28 12:04:24,864 INFO  [RS-EventLoopGroup-1-3] ipc.ServerRpcConnection(556): Connection from 172.18.128.12:46918, version=3.0.0-SNAPSHOT, sasl=false, ugi=hbase (auth:     SIMPLE), service=MasterService
2018-07-28 12:04:24,900 DEBUG [Thread-114] master.MasterRpcServices(1157): Checking to see if procedure is done pid=11
^[[38;5;196mF^[[0m
===============================================================================
Failure: ^[[48;5;124;38;5;231;1mtest_list_procedures(Hbase::ListProceduresTest)^[[0m
src/test/ruby/shell/list_procedures_test.rb:65:in `block in test_list_procedures'
     62:         end
     63:       end
     64:
^[[48;5;124;38;5;231;1m  => 65:       assert_equal(1, matching_lines)^[[0m
     66:     end
     67:   end
     68: end
<^[[48;5;34;38;5;231;1m1^[[0m> expected but was
<^[[48;5;124;38;5;231;1m0^[[0m>
===============================================================================
...
2018-07-28 12:04:25,374 INFO  [PEWorker-9] procedure2.ProcedureExecutor(1316): Finished pid=12, state=SUCCESS, hasLock=false; org.apache.hadoop.hbase.client.procedure.           ShellTestProcedure in 336msec


The completion of the ShellTestProcedure was after the assertion was raised.


    def create_procedure_regexp(table_name)
      regexp_string = '[0-9]+ .*ShellTestProcedure SUCCESS.*' \


The regex used by the test isn't found in test output either. 
 list_procedures_test fails due to no matching regex",yes
13177895,"Byteman is an easy to use tool to instrument a java process with agent string.
For example this script defines a rule to print out all the hadoop rpc traffic to the standard output (which is extremely useful for testing development).
This patch adds the byteman.jar to the baseimage and defines a simple logic to add agent instrumentation string to the HADOOP_OPTS (optional it also could download the byteman script from an url)
 
 NPE in DataNode#getDiskBalancerStatus() when the DN is restarted",13170347,"
2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException
 ***** TRACEBACK 4 *****
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
 at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)
 at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)
 at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
 at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
 at org.eclipse.jetty.server.Server.handle(Server.java:534)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
 at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
 at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
 at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
 at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
 at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
 at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
 at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
 at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
2018-06-28 05:12:08,400 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException


We have seen the above exception at datanode startup time. Should improve the NPE. Changing it to an IOE will also allow jmx to return '' correctly for getDiskBalancerStatus
.
 
 NPE in DataNode due to uninitialized DiskBalancer",yes
13294255,"Yetus's SpotBugs module depends on maven_add_install. In Jenkinsfile_GitHub, I'm pretty strict on my module definitions, with

GENERAL_CHECK_PLUGINS = 'all,-compile,-javac,-javadoc,-jira,-shadedjars,-unit'
JDK_SPECIFIC_PLUGINS = 'compile,github,htmlout,javac,javadoc,maven,mvninstall,shadedjars,unit'


The general check requests all but omits compile and maven, which I think means the spotbugs check gets dropped. Before HBASE-23767, we had just a single yetus that did all, so spotbugs would have been run. 
 Enable SpotBugs in PreCommit",13232218,"PRs on github aren't getting findbugs run, which means we don't catch things until nightly.
e.g. https://github.com/apache/hbase/pull/216 
 github PR integration for qabot needs spotbugs/findbugs",yes
13163305,"pom.xml still points to <hive.version.shortname>3.1.0</hive.version.shortname> which causes issues with schemaTool init scripts 
 hive.version.shortname should be 4.0",13165950,"Test failures due to this issue include any table access operations to 
tables that are modified by hive-schema-4.0.0.xxx.sql 
 One item in pom.xml still has 3.1.0 causing Metastore releated tables related test failures. ",yes
13162697,"In the ideal world, it would be possible to have separate white lists for docker registry depending on the security requirement for each type of docker images:
1. Registries from which we can run non-privileged containers without mounts
2. Registries from which we can run non-privileged containers with mounts
3. Registries from which we can run privileged or non-privileged containers with mounts
In the current implementation, there are only type 1 and type 2 or 3.  It would be nice to definite a separate white list to differentiate between 2 and 3. 
 Separate white list for docker.trusted.registries and docker.privileged-container.registries",13161235,"This is a superset of docker.privileged-containers.registries, admin can specify a whitelist and all images from non-privileged-container.registries will be rejected. 
 YARN should have ability to run images only from a whitelist docker registries",yes
13193505,"A real problem in our production cluster. A user point that his table's data can't be replicate to the peer cluster. Then we start to debug the reason. We checked the replication scope, checked the replication wal entry filter, and check the namespace,tablecfs config. But didn't found any problem. We enabled the RS's debug log to find the reason. Finally, we found use use put with skip wal to write data. But it taked a long time... Our replication use wal to replicate data. So the data can't be replicated to peer cluster. I thought throw a exception may be better for user if the table's replication scope is not 0. (as 0 means not replicated). 
 Throw exception when user put data with skip wal to a table which may be replicated",13217415,"The FairScheduler's configuration has the following defaults (from the code: javadoc):

In new style resources, any resource that is not specified will be set to missing or 0%, as appropriate. Also, in the new style resources, units are not allowed. Units are assumed from the resource manager's settings for the resources when the value isn't a percentage. The missing parameter is only used in the case of new style resources without percentages. With new style resources with percentages, any missing resources will be assumed to be 100% because percentages are only used with maximum resource limits.


This is not documented in the hadoop yarn site FairScheduler.html. It is quite intuitive, but still need to be documented though. 
 Fair Scheduler configuration defaults are not documented in case of min and maxResources",No
13155204,"This is a sub-class of LlapBaseRecordReader that wraps the socket inputStream and produces Arrow batches for an external client. 
 Provide an Arrow stream reader for external LLAP clients ",13158584,"""You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."" 
 Arrow SerDe itest failure",yes
13164188,"

Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener; locked 75% of time Bug type IS2_INCONSISTENT_SYNC (click for details) In class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService Field org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener Synchronized 75% of the time Unsynchronized access at AllocationFileLoaderService.java:[line 117] Synchronized access at AllocationFileLoaderService.java:[line 212] Synchronized access at AllocationFileLoaderService.java:[line 228] Synchronized access at AllocationFileLoaderService.java:[line 269]

 
 Findbugs warning IS2_INCONSISTENT_SYNC in AllocationFileLoaderService.reloadListener",13163898,"Per findbugs report in YARN-8390, there is some inconsistent locking of reloadListener

Warnings
Click on a warning row to see full context information.
Multithreaded correctness Warnings



Code
Warning


IS
Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener; locked 75% of time



Bug type IS2_INCONSISTENT_SYNC (click for details) 
In class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService
Field org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener
Synchronized 75% of the time
Unsynchronized access at AllocationFileLoaderService.java:[line 117]
Synchronized access at AllocationFileLoaderService.java:[line 212]
Synchronized access at AllocationFileLoaderService.java:[line 228]
Synchronized access at AllocationFileLoaderService.java:[line 269]



Details
IS2_INCONSISTENT_SYNC: Inconsistent synchronization
The fields of this class appear to be accessed inconsistently with respect to synchronization. This bug report indicates that the bug pattern detector judged that

The class contains a mix of locked and unlocked accesses,
The class is not annotated as javax.annotation.concurrent.NotThreadSafe,
At least one locked access was performed by one of the class's own methods, and
The number of unsynchronized field accesses (reads and writes) was no more than one third of all accesses, with writes being weighed twice as high as reads

A typical bug matching this bug pattern is forgetting to synchronize one of the methods in a class that is intended to be thread-safe.
You can select the nodes labeled ""Unsynchronized access"" to show the code locations where the detector believed that a field was accessed without synchronization.
Note that there are various sources of inaccuracy in this detector; for example, the detector cannot statically detect all situations in which a lock is held. Also, even when the detector is accurate in distinguishing locked vs. unlocked accesses, the code in question may still be correct. 
 Investigate AllocationFileLoaderService.reloadListener locking issue",yes
13260509,"The scheme of links infederationhealth.html are hard coded as 'http'.
It should be set to 'https' when dfs.http.policy is HTTPS_ONLY (HTTP_AND_HTTPS also, maybe)

https://github.com/apache/hadoop/blob/c99a12167ff9566012ef32104a3964887d62c899/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/webapps/router/federationhealth.html#L168-L169
https://github.com/apache/hadoop/blob/c99a12167ff9566012ef32104a3964887d62c899/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/webapps/router/federationhealth.html#L236
 
 RBF: namenode links in NameFederation Health page (federationhealth.html)  cannot use https scheme",13250345,"Currently, explanation aboutCustom WAL Directoryconfiguration is a sub-topic ofBulk Loading,chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of theWrite Ahead Log (WAL)chapter. 
 Move ""Custom WAL Directory"" section from ""Bulk Loading"" to ""Write Ahead Log (WAL)"" chapter",No
13151817,"In the following HA+Federated setup with two nameservices ns1 and ns2:

ns1 -> namenodes nn1, nn2
ns2 -> namenodes nn3, nn4
fs.defaultFS is hdfs://ns1.

A webhdfs request issued to nn3/nn4 will be routed to ns1. This is because setClientNamenodeAddress initializes NameNode#clientNamenodeAddress using fs.defaultFS before the config is overriden. 
 webhdfs requests can be routed incorrectly in federated cluster",13138089,"my cluster has multiple namenodes using HDFS Federation.
webhdfs that is not defaultFS does not work properly.
when I uploaded to non defaultFS namenode using webhdfs.
uploaded file was founded at defaultFS namenode.

I think root cause is that
 clientNamenodeAddress of non defaultFS namenode is always fs.defaultFS.
https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java#L462



/**
   * Set the namenode address that will be used by clients to access this
   * namenode or name service. This needs to be called before the config
   * is overriden.
   */
  public void setClientNamenodeAddress(Configuration conf) {
    String nnAddr = conf.get(FS_DEFAULT_NAME_KEY);
    if (nnAddr == null) {
      // default fs is not set.
      clientNamenodeAddress = null;
      return;
    }

    LOG.info(""{} is {}"", FS_DEFAULT_NAME_KEY, nnAddr);
    URI nnUri = URI.create(nnAddr);

    String nnHost = nnUri.getHost();
    if (nnHost == null) {
      clientNamenodeAddress = null;
      return;
    }

    if (DFSUtilClient.getNameServiceIds(conf).contains(nnHost)) {
      // host name is logical
      clientNamenodeAddress = nnHost;
    } else if (nnUri.getPort() > 0) {
      // physical address with a valid port
      clientNamenodeAddress = nnUri.getAuthority();
    } else {
      // the port is missing or 0. Figure out real bind address later.
      clientNamenodeAddress = null;
      return;
    }
    LOG.info(""Clients are to use {} to access""
        + "" this namenode/service."", clientNamenodeAddress );
  }




so webhdfs is redirected to datanode having wrong namenoderpcaddressparameter
finally file was located namenode of fs,defaultFS

workaround is
 configure fs.defaultFS of each namenode to its own nameservice.
e.g.
 hdfs://ns1 has fs.defaultFS=hdfs://ns1
 hdfs://ns2 has fs.defaultFS=hdfs://ns2
 ....



 
 webhdfs of federated namenode does not  work properly",yes
13193937,"Dummy patch request to test if precommit works after restart 
 Upgrade eclipse jetty version to 9.3.25.v20180904",13189087,"
CVE-2017-7657
CVE-2017-7658
CVE-2017-7656
CVE-2018-12536

We should upgrade the dependency to version 9.3.24 or the latest, if possible. 
 Upgrade Eclipse Jetty version to 9.3.24",yes
13169956,"Run the following queries and you will see the raw data for the table is 4 (that is the number of fields) incorrectly. We need to populate correct data size so data can be split properly.

SET hive.stats.autogather=true;
CREATE TABLE parquet_stats (id int,str string) STORED AS PARQUET;
INSERT INTO parquet_stats values(0, 'this is string 0'), (1, 'string 1');
DESC FORMATTED parquet_stats;



Table Parameters:
	COLUMN_STATS_ACCURATE	true
	numFiles            	1
	numRows             	2
	rawDataSize         	4
	totalSize           	373
	transient_lastDdlTime	1530660523

 
 Populate more accurate rawDataSize for parquet format",13216403,"Now we just throw a RetriesExhaustedException. 
 StatsWork should use footer scan for Parquet",yes
13271064,"Hive currently depends on ORC 1.5.6. We need 1.5.8 upgrade for https://issues.apache.org/jira/browse/HIVE-22499
ORC-1.5.7 includes https://issues.apache.org/jira/browse/ORC-361. It causes some tests overriding MemoryManager to fail. These need to be addressed while upgrading.
 
 Upgrade ORC version to 1.5.8",13270663,"Update orc to 1.5.8 in root pom 
 Update orc version to 1.5.8",yes
13169138,"Druid expressions do not support booleans yet. 
In druid expressions booleans are treated and parsed from longs, however when we store booleans from hive they are serialized as 'true' and 'false' string values. 
Need to make serialization consistent with deserialization and write long values when sending data to druid.  
 write booleans as long when serializing to druid",13203172,"YarnConfiguration.NM_LINUX_CONTAINER_CGROUPS_MOUNT and
YarnConfiguration.NM_LINUX_CONTAINER_CGROUPS_MOUNT_PATH are used in conjunction many places in the code, so for the sake of readabilty and simplicity, it is better to wrap the values of these configs to an object and use it instead of having 2 fields in
CGroupsHandlerImpl and in CgroupsLCEResourcesHandler as well. 
 Create an object for cgroups mount enable and cgroups mount path as they belong together",No
13218114,"TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes fails intermittent - observed in YARN-9311.


[ERROR] testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes[MinSharePreemptionWithDRF](org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption)  Time elapsed: 11.056 s  <<< FAILURE!
java.lang.AssertionError: Incorrect # of containers on the greedy app expected:<6> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyPreemption(TestFairSchedulerPreemption.java:296)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyRelaxLocalityPreemption(TestFairSchedulerPreemption.java:537)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes(TestFairSchedulerPreemption.java:473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

[ERROR] testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes[FairSharePreemptionWithDRF](org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption)  Time elapsed: 10.545 s  <<< FAILURE!
java.lang.AssertionError: Incorrect # of containers on the greedy app expected:<6> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyPreemption(TestFairSchedulerPreemption.java:296)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyRelaxLocalityPreemption(TestFairSchedulerPreemption.java:537)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes(TestFairSchedulerPreemption.java:473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

 
 TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes fails intermittently",13313461,"
The following 2 test cases are failing on unrelated patches very often:

hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler
hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption
Here is an example of both failures


[ERROR] Tests run: 105, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 27.481 s <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler
[ERROR] testNormalizationUsingQueueMaximumAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler)  Time elapsed: 0.178 s  <<< ERROR!
org.apache.hadoop.metrics2.MetricsException: Metrics source PartitionQueueMetrics,partition= already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics.getPartitionMetrics(QueueMetrics.java:360)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics.incrPendingResources(QueueMetrics.java:599)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.updatePendingResources(AppSchedulingInfo.java:399)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.internalAddResourceRequests(AppSchedulingInfo.java:331)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.internalAddResourceRequests(AppSchedulingInfo.java:358)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.updateResourceRequests(AppSchedulingInfo.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.updateResourceRequests(SchedulerApplicationAttempt.java:462)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:931)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler.allocateAppAttempt(TestFairScheduler.java:435)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler.testNormalizationUsingQueueMaximumAllocation(TestFairScheduler.java:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)




[ERROR] Tests run: 40, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 58.843 s <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption
[ERROR] testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes[MinSharePreemption](org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption)  Time elapsed: 10.709 s  <<< FAILURE!
java.lang.AssertionError: Incorrect # of containers on the greedy app expected:<6> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyPreemption(TestFairSchedulerPreemption.java:289)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyRelaxLocalityPreemption(TestFairSchedulerPreemption.java:542)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes(TestFairSchedulerPreemption.java:478)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


 
 Flaky test cases in Fair Scheduler",yes
13279663,"Implementing AWSCredentialsProviders that dynamically retrieve STS tokens based on the URI being accessed fail if Filesystem caching is enabled and the job accesses more than one URI Path within the same bucket. 
 Enable Filesystem caching to optionally include URI Path",13253838,"Provide an option to use Custom Cache Class in FileSystem Class. Currently, the caching is enabled by default and uses the URI schema and authority value to determine whether to create a new FS instance for the given URI or to fetch an already existing one from the cache.
In case of AWS S3 FS Impl, for an S3 path, the authority name will be bucket name, ie Filesystem object will be cached at the bucket level, but providing a custom caching logic can empower the user to cache it at some prefix level and provide more flexibility. 
 Pluggable Filesystem Caching Support in FileSystem Class",yes
13261495,"beeline-site.xml


<configuration xmlns:xi=""http://www.w3.org/2001/XInclude"">
 
 <property>
 <name>beeline.hs2.jdbc.url.container</name>
 <value>jdbc:hive2://c3220-node2.host.com:2181,c3220-node3.host.com:2181,c3220-node4.host.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2</value>
 </property>
 
 <property>
 <name>beeline.hs2.jdbc.url.default</name>
 <value>test</value>
 </property>
 <property>
<name>beeline.hs2.jdbc.url.test</name>
<value>${beeline.hs2.jdbc.url.container}?tez.queue.name=myqueue</value>
</property> 
 <property>
 <name>beeline.hs2.jdbc.url.llap</name>
 <value>jdbc:hive2://c3220-node2.host.com:2181,c3220-node3.host.com:2181,c3220-node4.host.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive</value>
 </property>
 
 </configuration>

beeline fail to connect because it does not parse the substituted value correctly


beeline
Error in parsing jdbc url: ${beeline.hs2.jdbc.url.container}?tez.queue.name=myqueue from beeline-site.xml
beeline>  
 
 Beeline-site parser does not handle the variable substitution correctly",13316175,"
If you want the deletion of a persistent object to cause the deletion of related objects then you need to mark the related fields in the mapping to be ""dependent"".
http://www.datanucleus.org/products/accessplatform/jdo/persistence.html#dependent_fields
http://www.datanucleus.org/products/datanucleus/jdo/persistence.html#_deleting_an_object
The database won't do it:
Derby Schema

ALTER TABLE ""APP"".""COLUMNS_V2"" ADD CONSTRAINT ""COLUMNS_V2_FK1"" FOREIGN KEY (""CD_ID"") REFERENCES ""APP"".""CDS"" (""CD_ID"") ON DELETE NO ACTION ON UPDATE NO ACTION;


https://github.com/apache/hive/blob/65cf6957cf9432277a096f91b40985237274579f/standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql#L452 
 Make ""cols"" dependent so that it cascade deletes",No
13179984,"The following can be observed in master branch:


java.lang.NullPointerException
	at org.apache.hadoop.hbase.rest.TestTableResource.setUpBeforeClass(TestTableResource.java:134)


The NPE comes from the following in TestEndToEndSplitTransaction :


        compactAndBlockUntilDone(TEST_UTIL.getAdmin(),
          TEST_UTIL.getMiniHBaseCluster().getRegionServer(0), daughterA.getRegionName());


Initial check of the code shows that TestEndToEndSplitTransaction uses TEST_UTIL instance which is created within TestEndToEndSplitTransaction. However, TestTableResource creates its own instance of HBaseTestingUtility.
Meaning TEST_UTIL.getMiniHBaseCluster() would return null, since the instance created by TestEndToEndSplitTransaction has hbaseCluster as null. 
 TestTableResource fails with NPE",13180578,"TestTableResource fails consistently with NPE, only in the branch-2s. Both master and branch-1 is fine. 

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 54.397 s <<< FAILURE! - in org.apache.hadoop.hbase.rest.TestTableResource
[ERROR] org.apache.hadoop.hbase.rest.TestTableResource  Time elapsed: 54.395 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.hbase.rest.TestTableResource.setUpBeforeClass(TestTableResource.java:134)

 
 NPE in TestTableResource.setUpBeforeClass (TestTableResource.java:134)",yes
13141979,"yarn service page has stop/delete buttons. These buttons has to be shown/hidden based on ServiceState of each app from ATS. 
 [UI2] yarn-service page need to consider ServiceState to show stop/delete buttons",13140033,"Steps:
1) Launch yarn service
2) Go to service page and click on Setting button->""Stop Service"". The application will be stopped.
3) Refresh page
Here, setting button disappears. Thus, user can not delete service from UI after stopping application
Expected behavior:
Setting button should be present on UI page after application is stopped. If application is stopped, setting button should only have ""Delete Service"" action available. 
 [UI2] YARN service delete option disappears after stopping application",yes
13198046,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13198047,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",yes
13151264,"Looks like HIVE-18979 missed the update 
 TestSparkCliDriver:subquery_scalar - golden file needs to be udpated",13140560,"Not sure what caused this to start failing, but its been failing for a while. 
 TestSparkCliDriver.testCliDriver[subquery_scalar] is consistently failing",yes
13261798,"Capacity Scheduler does not support two percentage values for leaf queue capacity and maximum-capacity settings. So, you can't do something like this:
yarn.scheduler.capacity.root.users.john.leaf-queue-template.capacity=memory-mb=50.0%, vcores=50.0%
Only a single percentage value is accepted.
This makes it nearly impossible to properly convert a similar setting from Fair Scheduler, where such a configuration is valid and accepted (<maxChildResources>).
Note: using absolute resources (yarn.scheduler.capacity.root.users.john.leaf-queue-template.capacity=memory-mb=16384, vcores=8) is addressed in YARN-10154.
 
 Capacity scheduler: enhance leaf-queue-template capacity / maximum-capacity setting",13286520,"In CS, ManagedParent Queue and its template cannot take absolute resource value like 
[memory=8192,vcores=8]
 Thsi Jira is to track and improve the configuration reading module of DynamicQueue to support absolute resource values.
 
 CS Dynamic Queues cannot be configured with absolute resources",yes
13320838,"http://ci.hive.apache.org/job/hive-precommit/job/master/143/testReport/junit/ 
 flaky TestStatsReplicationScenariosMigration.testMetadataOnlyDump",13202274,"TestThriftHttpServer is the first on the flaky list for branch-1 and branch-1.4 with approximately 60% failure rate.
Thrift server is not yet accepting request at the time the test starts.
java.net.ConnectException: Connection refused (Connection refused) at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.checkHttpMethods(TestThriftHttpServer.java:275) at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.testThriftServerHttpOptionsForbiddenWhenOptionsDisabled(TestThriftHttpServer.java:176) 
 ConnectException in TestThriftHttpServer",No
13248118,"1. if you set an s3a encryption key, the Session and Role DelegationToken tests fail...the test setup needs to unset that key for config and bucket
2. SSE-C tests don't seem to be unsetting all their bucket properties 
 S3A delegation token tests fail if fs.s3a.encryption.key set",13233529,"The ITests for Session and Role DTs in S3A set the encryption option (to verify its propagation). But if you have set an encryption key in the config then test setup will fail
Fix: when you set the encryption, clear the options for fs.s3a.encryption.key for the dest bucket 
 S3A delegation tests fail if you set fs.s3a.secret.key",yes
13222568,"Bump jackson version to 2.9.8 
 Bumping jackson version to 2.9.8",13218681,"Currently at:


<jackson.version>2.9.5</jackson.version>


Upgrade to 2.9.8 - contains some improvements for processing Base64 data. 
 Upgrade Jackson to 2.9.8",yes
13168603,"

create or replace view v1 as select 1 as q 


results in an error:


Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Table already exists: default.v1 (state=08S01,code=1)


however the following works (thank you Miklos Gergely)


create or replace view v1 as select 1 as q union all select 1 as qq where false

 
 Fix create view over literals",13196621,"it took quite some time to figure out how to install the ""information_schema"" and ""sys"" schemas (thanks to https://issues.apache.org/jira/browse/HIVE-16941) into a hive 3.1.0/3.1.1 on hdfs/hadoop 2.9.1 and I am still unsure if it is the proper way of doing it.
when I execute:


hive@hive-server ~> schematool -metaDbType derby -dbType hive -initSchema -url jdbc:hive2://localhost:10000/default -driver org.apache.hive.jdbc.HiveDriver""


I receive an error (from --verbose log):


[...]
Error: Error while compiling statement: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table (state=42000,code=40000)
org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!
[...]



It seems to be the last statement during setup of the sys-schema causes the issue. When executing it manually:



0: jdbc:hive2://localhost:10000> CREATE OR REPLACE VIEW `VERSION` AS SELECT 1 AS `VER_ID`, '3.1.0' AS `SCHEMA_VERSION`, 'Hive release version 3.1.0' AS `VERSION_COMMENT`;
Error: Error while compiling statement: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table (state=42000,code=40000)



I have tried to switch the metastore_db from derby embedded to derby server to postgresql and made sure the changed metadatabases each worked, but setting up the information_schema and sys schemas always delivers the same error.
Executing only the select part without the create view works:


0: jdbc:hive2://localhost:10000> SELECT 1 AS `VER_ID`, '3.1.0' AS `SCHEMA_VERSION`, 'Hive release version 3.1.0' AS `VERSION_COMMENT`;
+---------+-----------------+-----------------------------+
| ver_id | schema_version | version_comment |
+---------+-----------------+-----------------------------+
| 1 | 3.1.0 | Hive release version 3.1.0 |
+---------+-----------------+-----------------------------+
1 row selected (0.595 seconds)


It seems to be related to: HIVE-19444
 
 Creating information_schema and sys schema via schematool fails with parser error",yes
13155057,"I have been trying to configure the Hadoop kms to use hdfs as the key provider but it seems that this functionality is failing.
I followed the Hadoop docs for that matter, and I added the following field to my kms-site.xml:


<property> 
   <name>hadoop.kms.key.provider.uri</name>
   <value>jceks://hdfs@nn1.example.com/kms/test.jceks</value> 
   <description> 
      URI of the backing KeyProvider for the KMS. 
   </description> 
</property>

That route exists in hdfs, and I expect the kms to create the file test.jceks for its keystore. However, the kms failed to start due to this error:


ERROR: Hadoop KMS could not be started REASON: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""hdfs"" Stacktrace: --------------------------------------------------- org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""hdfs"" at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3220) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3240) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3291) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3259) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:470) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) at org.apache.hadoop.crypto.key.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:132) at org.apache.hadoop.crypto.key.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88) at org.apache.hadoop.crypto.key.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:660) at org.apache.hadoop.crypto.key.KeyProviderFactory.get(KeyProviderFactory.java:96) at org.apache.hadoop.crypto.key.kms.server.KMSWebApp.contextInitialized(KMSWebApp.java:187) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4276) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4779) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployDirectory(HostConfig.java:1080) at org.apache.catalina.startup.HostConfig.deployDirectories(HostConfig.java:1003) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:507) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:761) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)


For what I could manage to understand, it seems that this error is because there is no FileSystem implemented for HDFS. I have looked up this error but it always refers to a lack of jars for the hdfs-client when upgrading, which I have not done (it is a fresh installation). Ihave tested it using Hadoop 2.7.2 and 2.9.0
Thank you in advance. 
 Hadoop KMS with HDFS keystore: No FileSystem for scheme ""hdfs""",13284027,"I've seen this fail. Here is the latest:


 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 21.044 s <<< FAILURE! - in org.apache.hadoop.hbase.replication.regionserver.TestReplicator
 [ERROR] org.apache.hadoop.hbase.replication.regionserver.TestReplicator.testReplicatorWithErrors  Time elapsed: 7.139 s  <<< FAILURE!
 java.lang.AssertionError: We did not replicate enough rows expected:<10> but was:<7>
   at org.apache.hadoop.hbase.replication.regionserver.TestReplicator.testReplicatorWithErrors(TestReplicator.java:158)


In test logs there are a few (in groups of three) of these:


 4247 2020-02-07 13:50:29,070 WARN  [RS_REFRESH_PEER-regionserver/localhost:0-1.replicationSource,testReplicatorWithErrors.replicationSource.shipperlocalhost%2C49531%2C1581112194639,testReplicatorWithErrors] regionserver.                       ReplicationSourceShipper(223): org.apache.hadoop.hbase.replication.regionserver.TestReplicator$FailureInjectingReplicationEndpointForTest threw unknown exception:
 4248 java.lang.ClassCastException: org.apache.hbase.thirdparty.com.google.protobuf.ServiceException cannot be cast to java.io.IOException
 4249   at org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.parallelReplicate(HBaseInterClusterReplicationEndpoint.java:361)
 4250   at org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.replicate(HBaseInterClusterReplicationEndpoint.java:404)
 4251   at org.apache.hadoop.hbase.replication.regionserver.TestReplicator$ReplicationEndpointForTest.replicate(TestReplicator.java:228)
 4252   at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper.shipEdits(ReplicationSourceShipper.java:188)
 4253   at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper.run(ReplicationSourceShipper.java:118)


Let me fix this and see if it makes TestReplicator stable again. 
 [Flakey Test] TestReplicator#testReplicatorWithErrors: AssertionError: We did not replicate enough rows expected:<10> but was:<7>",No
13278403,"PUT requests in HttpFS with path as ""/"" were not supported .
 
 HttpFS: put requests are not supported for path ""/""",13215373,"


Reason	Tests
FindBugs	module:hbase-server
Boxed value is unboxed and then immediately reboxed in org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.incrementUgiReference(UserGroupInformation) At SecureBulkLoadEndpoint.java:then immediately reboxed in org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.incrementUgiReference(UserGroupInformation) At SecureBulkLoadEndpoint.java:[line 268]


Looking at branch-2 and master I suspect we're doing the same wasteful operation but findbugs can't see it through the lambda definition. 
 Fix box/unbox findbugs warning in secure bulk load",No
13252883,"Current implementation only log process time


if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&
    (processingTime > threeSigma)) {
  LOG.warn(""Slow RPC : {} took {} {} to process from client {}"",
      methodName, processingTime, RpcMetrics.TIMEUNIT, call);
  rpcMetrics.incrSlowRpc();
}


We need to log more details to help us locate the problem (eg. how long it take to request lock, holding lock, or do other things) 
 Log more detail for slow RPC",13232864,"Now, the log ofa slow RPC request just contains themethodName,processingTime and client. Code is here:


if ((rpcMetrics.getProcessingSampleCount() > minSampleSize) &&
    (processingTime > threeSigma)) {
  if(LOG.isWarnEnabled()) {
    String client = CurCall.get().toString();
    LOG.warn(
        ""Slow RPC : "" + methodName + "" took "" + processingTime +
            "" milliseconds to process from client "" + client);
  }
  rpcMetrics.incrSlowRpc();
}


It is not enough to analyze why the RPC request is slow.
The parameter of the request is a very important thing, and need to be logged. 
 Log of a slow RPC request should contain the parameter of the request",yes
13179551,"Backport HBASE-17519 (Rollback the removed cells) to branch-1.3, which handles rollback of append/increment completely in case of failure 
 Backport HBASE-17519 (Rollback the removed cells) to branch-1.3",13173993,"If a custom  uesr is used to run hive server 2, then repl subsystem should use the directories within the user's home directory for various configurations rather than use the default /user/hive/
 
 configure repl configuration directories based on user running hiveserver2",No
13216296,"Using the latest master branch. When creating a table, the following error could be seen in the master's log and it only happens when rsgroup is enabled.


2019-02-17 04:08:41,853 INFO  [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16000] master.HMaster: Client=lixiang//10.23.10.9 create 't1', {NAME => 'cf1', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}
2019-02-17 04:08:41,958 INFO  [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16000] rsgroup.RSGroupAdminServer: Moving table t1 to RSGroup default
2019-02-17 04:08:41,962 INFO  [RegionOpenAndInitThread-t1-1] regionserver.HRegion: creating HRegion t1 HTD == 't1', {NAME => 'cf1', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'} RootDir = file:/home/lixiang/standalonehbase/hbase/.tmp Table name == t1
2019-02-17 04:08:41,964 INFO  [RegionOpenAndInitThread-t1-1] regionserver.HRegion: Closed t1,,1550376521847.1954e4b74647fb1f85342bdff188bdf4.
2019-02-17 04:08:41,967 ERROR [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16000] master.TableStateManager: Unable to get table t1 state
org.apache.hadoop.hbase.master.TableStateManager$TableStateNotFoundException: t1
        at org.apache.hadoop.hbase.master.TableStateManager.getTableState(TableStateManager.java:215)
        at org.apache.hadoop.hbase.master.TableStateManager.isTableState(TableStateManager.java:147)
        at org.apache.hadoop.hbase.master.assignment.AssignmentManager.isTableDisabled(AssignmentManager.java:354)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.moveTables(RSGroupAdminServer.java:411)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.assignTableToGroup(RSGroupAdminEndpoint.java:471)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.postCreateTable(RSGroupAdminEndpoint.java:494)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost$13.call(MasterCoprocessorHost.java:350)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost$13.call(MasterCoprocessorHost.java:347)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult.callObserver(CoprocessorHost.java:551)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.execOperation(CoprocessorHost.java:625)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCreateTable(MasterCoprocessorHost.java:347)
        at org.apache.hadoop.hbase.master.HMaster$4.run(HMaster.java:2083)
        at org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.submitProcedure(MasterProcedureUtil.java:134)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:2066)
        at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:644)
        at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
2019-02-17 04:08:41,969 INFO  [PEWorker-14] hbase.MetaTableAccessor: Added 1 regions to meta.
2019-02-17 04:08:41,972 INFO  [PEWorker-14] hbase.MetaTableAccessor: Updated tableName=t1, state=ENABLING in hbase:meta

 
 RSGroupAdminEndpoint#postCreateTable tries to get table state before it is in meta",13186580,"Similar to HBASE-19509, I found the following logs in master log when creating table


2018-09-21 15:14:47,476 ERROR [RpcServer.default.FPBQ.Fifo.handler=296,queue=26,port=16000] master.TableStateManager: Unable to get table t3 state
org.apache.hadoop.hbase.master.TableStateManager$TableStateNotFoundException: t3
        at org.apache.hadoop.hbase.master.TableStateManager.getTableState(TableStateManager.java:215)
        at org.apache.hadoop.hbase.master.TableStateManager.isTableState(TableStateManager.java:147)
        at org.apache.hadoop.hbase.master.assignment.AssignmentManager.isTableDisabled(AssignmentManager.java:344)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.moveTables(RSGroupAdminServer.java:412)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.assignTableToGroup(RSGroupAdminEndpoint.java:471)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.postCreateTable(RSGroupAdminEndpoint.java:494)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost$12.call(MasterCoprocessorHost.java:335)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost$12.call(MasterCoprocessorHost.java:332)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult.callObserver(CoprocessorHost.java:540)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.execOperation(CoprocessorHost.java:614)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCreateTable(MasterCoprocessorHost.java:332)
        at org.apache.hadoop.hbase.master.HMaster$3.run(HMaster.java:1929)
        at org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.submitProcedure(MasterProcedureUtil.java:131)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1911)
        at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:628)
        at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)


In fact, we only need to change the information of rsgroup without moving region. 
 TableStateNotFoundException thrown from RSGroupAdminEndpoint#postCreateTable when creating table",yes
13322011,"CompactionTxnHandler often doesn't log the preparedStatement parameters, which is really painful when compaction isn't working the way it should. Also expand logging around compaction Cleaner, Initiator, Worker. And some formatting cleanup. 
 Improve logging around CompactionTxnHandler",13270580,"Now, the overview tab load /conf synchronously as follow picture.
  
This issue will change it to an asynchronous method. The effect diagram is as follows.
   
 Reduce NameNode overview tab response time",No
13252427,"CapacitySchedulerQueueManager has incorrect list of queues when there is more than one parent queue (say at middle level) with same name.
For example,

root
	
a
		
b
			
c


d
			
b
				
e









CapacitySchedulerQueueManager#getQueues maintains these list of queues. While parsing ""root.a.d.b"", it overrides ""root.a.b"" with new Queue object in the map because of similar name. After parsing all the queues, map count should be 7, but it is 6. Any reference to queue ""root.a.b"" in code path is nothing but ""root.a.d.b"" object. Since CapacitySchedulerQueueManager#getQueues has been used in multiple places, will need to understand the implications in detail. For example, CapapcityScheduler#getQueue has been used in many places which in turn uses CapacitySchedulerQueueManager#getQueues. cc [~eepayne], Sunil G 
 CapacitySchedulerQueueManager has incorrect list of queues",13263635,"CapacitySchedulerQueueManager allows unsupported Queue hierarchy. When creating a queue with same name as an existing parent queue name - it has to fail with below.


Caused by: java.io.IOException: A is moved from:root.A to:root.B.A after refresh, which is not allowed.Caused by: java.io.IOException: A is moved from:root.A to:root.B.A after refresh, which is not allowed. at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.validateQueueHierarchy(CapacitySchedulerQueueManager.java:335) at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.reinitializeQueues(CapacitySchedulerQueueManager.java:180) at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:762) at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:473) ... 70 more 


In Some cases, the error is not thrown while creating the queue but thrown at submission of job ""Failed to submit application_1571677375269_0002 to YARN : Application application_1571677375269_0002 submitted by user : systest to non-leaf queue : B""
Below scenarios are allowed but it should not


It allows root.A.A1.B when root.B.B1 already exists.
   
1. Add root.A
2. Add root.A.A1
3. Add root.B
4. Add root.B.B1
5. Allows Add of root.A.A1.B 

It allows two root queues:
   
1. Add root.A
2. Add root.B
3. Add root.A.A1
4. Allows Add of root.A.A1.root
	 


Below scenario is handled properly:


It does not allow root.B.A when root.A.A1 already exists.
     
1. Add root.A
2. Add root.B
3. Add root.A.A1
4. Does not Allow Add of root.B.A


This error handling has to be consistent in all scenarios. 
 CapacitySchedulerQueueManager allows unsupported Queue hierarchy",yes
13223557,"Metastore runs background thread out of which one is partition discovery. While removing expired partitions following exception is thrown


2019-03-24 04:24:59.583 WARN [PartitionDiscoveryTask-0] metastore.MetaStoreDirectSql: Failed to execute [select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS"" inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID"" and ""TBLS"".""TBL_NAME"" = ? inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID"" and ""DBS"".""NAME"" = ? inner join ""PARTITION_KEY_VALS"" ""FILTER0"" on ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 inner join ""PARTITION_KEY_VALS"" ""FILTER1"" on ""FILTER1"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER1"".""INTEGER_IDX"" = 1 inner join ""PARTITION_KEY_VALS"" ""FILTER2"" on ""FILTER2"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER2"".""INTEGER_IDX"" = 2 where ""DBS"".""CTLG_NAME"" = ? and ( ( (((case when ""FILTER0"".""PART_KEY_VAL"" <> ? and ""TBLS"".""TBL_NAME"" = ? and ""DBS"".""NAME"" = ? and ""DBS"".""CTLG_NAME"" = ? and ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 then cast(""FILTER0"".""PART_KEY_VAL"" as date) else null end) = ?) and (""FILTER1"".""PART_KEY_VAL"" = ?)) and (""FILTER2"".""PART_KEY_VAL"" = ?)) )] with parameters [logs, sys, hive, __HIVE_DEFAULT_PARTITION__, logs, sys, hive, 2019-03-23, warehouse-1553300821-692w, metastore-db-create-job]
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""PARTITIONS"".""PART_ID"" from ""PARTITIONS"" inner join ""TBLS"" on ""PARTITIONS"".""TBL_ID"" = ""TBLS"".""TBL_ID"" and ""TBLS"".""TBL_NAME"" = ? inner join ""DBS"" on ""TBLS"".""DB_ID"" = ""DBS"".""DB_ID"" and ""DBS"".""NAME"" = ? inner join ""PARTITION_KEY_VALS"" ""FILTER0"" on ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 inner join ""PARTITION_KEY_VALS"" ""FILTER1"" on ""FILTER1"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER1"".""INTEGER_IDX"" = 1 inner join ""PARTITION_KEY_VALS"" ""FILTER2"" on ""FILTER2"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER2"".""INTEGER_IDX"" = 2 where ""DBS"".""CTLG_NAME"" = ? and ( ( (((case when ""FILTER0"".""PART_KEY_VAL"" <> ? and ""TBLS"".""TBL_NAME"" = ? and ""DBS"".""NAME"" = ? and ""DBS"".""CTLG_NAME"" = ? and ""FILTER0"".""PART_ID"" = ""PARTITIONS"".""PART_ID"" and ""FILTER0"".""INTEGER_IDX"" = 0 then cast(""FILTER0"".""PART_KEY_VAL"" as date) else null end) = ?) and (""FILTER1"".""PART_KEY_VAL"" = ?)) and (""FILTER2"".""PART_KEY_VAL"" = ?)) )"".
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)
at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:391)
at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:267)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:2042)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionIdsViaSqlFilter(MetaStoreDirectSql.java:621)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:487)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3426)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3418)
at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:3702)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:3453)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:3406)
at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
at com.sun.proxy.$Proxy33.getPartitionsByExpr(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:4521)
at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
at com.sun.proxy.$Proxy34.drop_partitions_req(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:1288)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:474)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:435)
at org.apache.hadoop.hive.metastore.utils.RetryUtilities$ExponentiallyDecayingBatchWork.run(RetryUtilities.java:91)
at org.apache.hadoop.hive.metastore.Msck.dropPartitionsInBatches(Msck.java:499)
at org.apache.hadoop.hive.metastore.Msck.repair(Msck.java:226)
at org.apache.hadoop.hive.metastore.PartitionManagementTask$MsckThread.run(PartitionManagementTask.java:213)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
NestedThrowablesStackTrace:
org.postgresql.util.PSQLException: ERROR: operator does not exist: date = character varying
Hint: No operator matches the given name and argument type(s). You might need to add explicit type casts.
Position: 886
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200)
at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424)
at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:161)
at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:114)
at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52)
at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java)
at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:375)
at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:552)
at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:645)
at org.datanucleus.store.query.Query.executeQuery(Query.java:1855)
at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807)
at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:368)
at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:267)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:2042)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionIdsViaSqlFilter(MetaStoreDirectSql.java:621)
at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:487)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3426)
at org.apache.hadoop.hive.metastore.ObjectStore$9.getSqlResult(ObjectStore.java:3418)
at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:3702)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:3453)
at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:3406)
at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
at com.sun.proxy.$Proxy33.getPartitionsByExpr(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:4521)
at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
at com.sun.proxy.$Proxy34.drop_partitions_req(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropPartitions(HiveMetaStoreClient.java:1288)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:474)
at org.apache.hadoop.hive.metastore.Msck$2.execute(Msck.java:435)
at org.apache.hadoop.hive.metastore.utils.RetryUtilities$ExponentiallyDecayingBatchWork.run(RetryUtilities.java:91)
at org.apache.hadoop.hive.metastore.Msck.dropPartitionsInBatches(Msck.java:499)
at org.apache.hadoop.hive.metastore.Msck.repair(Msck.java:226)
at org.apache.hadoop.hive.metastore.PartitionManagementTask$MsckThread.run(PartitionManagementTask.java:213)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)

 
 Backport HBASE-21895 ""Error prone upgrade"" to branch-2",13300411,"TestDelegationTokenWithEncryption andTestGenerateDelegationToken always fail.

Incidentally, they don't fail in branch-2.3 and branch-2.2.

I suspect there's a regression with delegation token code, because if I comment out the following code in the test, they pass:



try (Connection conn = ConnectionFactory.createConnection(TEST_UTIL.getConfiguration())) {
 Token<? extends TokenIdentifier> token = TokenUtil.obtainToken(conn);
 UserGroupInformation.getCurrentUser().addToken(token);
}


Effectively, use Kerberos to login instead of delegation token.
The tests fail all the time (100%) in the last 29 runs: 
https://builds.apache.org/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/dashboard.html
Initially I thought this was caused by pluggable authentication (HBASE-23347), but the tests don't fail in branch-2.3 so looks unlikely. 
 Release hbase-thirdparty 3.3.0",No
13307154,"Run the 'catalogjanitor_run' and the 'hbck_chore_run' when we draw the 'HBCK Report' page.
Its PITA running command on the shell every time you need a new report.
Can avoid new behavior by passing '?cache=true' on URL. 
 Missing regionName while logging warning in HBCKServerCrashProcedure",13278403,"PUT requests in HttpFS with path as ""/"" were not supported .
 
 HttpFS: put requests are not supported for path ""/""",No
13139822,"It failed the nightly.
Says this...
Error Message
Waiting timed out after [30,000] msec
Stacktrace
java.lang.AssertionError: Waiting timed out after [30,000] msec
	at org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.testQuotaStatusFromMaster(TestQuotaStatusRPCs.java:267)
... but looking in log I see following:
Odd thing is the test is run three times and it failed all three times for same reason.
[ERROR] Failures: 
[ERROR] org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.testQuotaStatusFromMaster(org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs)
[ERROR]   Run 1: TestQuotaStatusRPCs.testQuotaStatusFromMaster:267 Waiting timed out after [30,000] msec
[ERROR]   Run 2: TestQuotaStatusRPCs.testQuotaStatusFromMaster:267 Waiting timed out after [30,000] msec
[ERROR]   Run 3: TestQuotaStatusRPCs.testQuotaStatusFromMaster:267 Waiting timed out after [30,000] msec
If you go to build artifacts you can download full -output.txt log. I see stuff like this which might be ok....


2018-02-21 01:29:59,546 INFO  [StoreCloserThread-testQuotaStatusFromMaster4,0,1519176558800.1dbd00f38915cd276410065f85140b26.-1] regionserver.HStore(930): Closed f1
2018-02-21 01:29:59,551 ERROR [master/ad51e354307e:0.Chore.2] hbase.ScheduledChore(189): Caught error
java.lang.RuntimeException: java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture@79ec2ef9 rejected from java.util.concurrent.ThreadPoolExecutor@5198a326[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 142]
  at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:200)
  at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)
  at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)
  at org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)
  at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)
  at org.apache.hadoop.hbase.quotas.QuotaRetriever.next(QuotaRetriever.java:106)
  at org.apache.hadoop.hbase.quotas.QuotaRetriever$Iter.<init>(QuotaRetriever.java:125)
  at org.apache.hadoop.hbase.quotas.QuotaRetriever.iterator(QuotaRetriever.java:117)
  at org.apache.hadoop.hbase.quotas.QuotaObserverChore.fetchAllTablesWithQuotasDefined(QuotaObserverChore.java:458)
  at org.apache.hadoop.hbase.quotas.QuotaObserverChore._chore(QuotaObserverChore.java:148)
  at org.apache.hadoop.hbase.quotas.QuotaObserverChore.chore(QuotaObserverChore.java:136)
  at org.apache.hadoop.hbase.ScheduledChore.run(ScheduledChore.java:186)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
  at org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl$JitteredRunnableScheduledFuture.run(JitterScheduledThreadPoolExecutorImpl.java:111)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture@79ec2ef9 rejected from java.util.concurrent.ThreadPoolExecutor@5198a326[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 142]
  at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
  at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
  at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
  at org.apache.hadoop.hbase.client.ResultBoundedCompletionService.submit(ResultBoundedCompletionService.java:171)
  at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.addCallsForCurrentReplica(ScannerCallableWithReplicas.java:320)
  at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:182)
  at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
  at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:192)
  ... 19 more


.. but then this which looks less ok:


2018-02-21 01:30:24,044 DEBUG [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=39067] ipc.CallRunner(141): callId: 16 service: RegionServerStatusService methodName: ReportRegionSpaceUse size: 109 connection: 172.17.0.2:57185 deadline: 1519176634044
org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
  at org.apache.hadoop.hbase.master.HMaster.checkInitialized(HMaster.java:2741)
  at org.apache.hadoop.hbase.master.MasterRpcServices.reportRegionSpaceUse(MasterRpcServices.java:2135)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:11053)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:406)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
2018-02-21 01:30:24,046 ERROR [regionserver/ad51e354307e:0.Chore.1] hbase.ScheduledChore(189): Caught error
java.io.UncheckedIOException: org.apache.hadoop.hbase.TableNotFoundException: hbase:quota
  at org.apache.hadoop.hbase.client.ResultScanner$1.hasNext(ResultScanner.java:55)
  at org.apache.hadoop.hbase.quotas.SpaceQuotaRefresherChore.fetchSnapshotsFromQuotaTable(SpaceQuotaRefresherChore.java:151)
  at org.apache.hadoop.hbase.quotas.SpaceQuotaRefresherChore.chore(SpaceQuotaRefresherChore.java:84)
  at org.apache.hadoop.hbase.ScheduledChore.run(ScheduledChore.java:186)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
  at org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl$JitteredRunnableScheduledFuture.run(JitterScheduledThreadPoolExecutorImpl.java:111)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hbase.TableNotFoundException: hbase:quota
  at org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegionInMeta(ConnectionImplementation.java:842)
  at org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:733)
  at org.apache.hadoop.hbase.client.ConnectionUtils$ShortCircuitingClusterConnection.locateRegion(ConnectionUtils.java:131)
  at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:325)
  at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)
  at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)
  at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:192)
  at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:269)
  at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:437)
  at org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:312)
  at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:597)
  at org.apache.hadoop.hbase.client.ResultScanner$1.hasNext(ResultScanner.java:53)
  ... 11 more
2018-02-21 01:30:24,179 DEBUG [Time-limited test-EventThread] zookeeper.ZKWatcher(443): master:39067-0x161b5fb62d10000, quorum=localhost:49877, baseZNode=/hbase Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/namespace
2018-02-21 01:30:24,604 INFO  [PEWorker-13] procedure2.ProcedureExecutor(1247): Finished pid=6, state=SUCCESS; CreateNamespaceProcedure, namespace=hbase in 1.0980sec
2018-02-21 01:30:24,772 DEBUG [Time-limited test-EventThread] zookeeper.ZKWatcher(443): master:39067-0x161b5fb62d10000, quorum=localhost:49877, baseZNode=/hbase Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/namespace/default
2018-02-21 01:30:24,830 DEBUG [Time-limited test-EventThread] zookeeper.ZKWatcher(443): master:39067-0x161b5fb62d10000, quorum=localhost:49877, baseZNode=/hbase Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/namespace/hbase
2018-02-21 01:30:24,831 INFO  [M:0;ad51e354307e:39067] master.HMaster(927): Master has completed initialization 17.276sec
2018-02-21 01:30:24,832 INFO  [M:0;ad51e354307e:39067] quotas.MasterQuotaManager(87): Quota table not found. Creating...
2018-02-21 01:30:24,833 INFO  [M:0;ad51e354307e:39067] master.HMaster(1783): Client=null/null create 'hbase:quota', {NAME => 'q', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}, {NAME => 'u', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}
2018-02-21 01:30:24,979 ERROR [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=39067] ipc.RpcServer(464): Unexpected throwable object
java.lang.NullPointerException
  at org.apache.hadoop.hbase.master.MasterRpcServices.reportRegionSpaceUse(MasterRpcServices.java:2142)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:11053)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:406)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
2018-02-21 01:30:24,980 DEBUG [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=39067] ipc.CallRunner(141): callId: 18 service: RegionServerStatusService methodName: ReportRegionSpaceUse size: 109 connection: 172.17.0.2:57185 deadline: 1519176634979
java.io.IOException
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:465)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hbase.master.MasterRpcServices.reportRegionSpaceUse(MasterRpcServices.java:2142)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:11053)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:406)
  ... 3 more
2018-02-21 01:30:24,982 DEBUG [regionserver/ad51e354307e:0.Chore.1] regionserver.HRegionServer(1245): Failed to report region sizes to Master. This will be retried.
java.io.IOException: java.io.IOException
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:465)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hbase.master.MasterRpcServices.reportRegionSpaceUse(MasterRpcServices.java:2142)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:11053)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:406)
  ... 3 more

  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
  at org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:100)
  at org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:90)
  at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:358)
  at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:335)
  at org.apache.hadoop.hbase.regionserver.HRegionServer.reportRegionSizesForQuotas(HRegionServer.java:1224)
  at org.apache.hadoop.hbase.quotas.FileSystemUtilizationChore.reportRegionSizesToMaster(FileSystemUtilizationChore.java:187)
  at org.apache.hadoop.hbase.quotas.FileSystemUtilizationChore.chore(FileSystemUtilizationChore.java:136)
  at org.apache.hadoop.hbase.ScheduledChore.run(ScheduledChore.java:186)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
  at org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl$JitteredRunnableScheduledFuture.run(JitterScheduledThreadPoolExecutorImpl.java:111)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(java.io.IOException): java.io.IOException
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:465)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hbase.master.MasterRpcServices.reportRegionSpaceUse(MasterRpcServices.java:2142)
  at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:11053)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:406)
  ... 3 more


You might have some input mighty Josh Elser ? Thanks sir. It failed only this once. I could up the timeouts from 30 seconds because that allowed but my guess it he NPE does us in. Thanks sir. 
 .TestQuotaStatusRPCs.testQuotaStatusFromMaster failed with NPEs and RuntimeExceptions",13158905,"I think we are spending a lot of time copying logs from worker nodes to the server. We should add some logging to print aggregate time spent in rsync to confirm. 
 Add performance metric to find the total time spend in rsync",No
13252783,"we've been regularly getting 4-5 concurrent builds of PRs. 
 fewer concurrent github PR builds",13252353,"IMetaStoreClient exposes the following methods related to altering of partitions:


void alter_partition(String dbName, String tblName, Partition newPart);
void alter_partition(String catName, String dbName, String tblName, Partition newPart);
void alter_partition(String dbName, String tblName, Partition newPart, EnvironmentContext environmentContext);
void alter_partition(String catName, String dbName, String tblName, Partition newPart, EnvironmentContext environmentContext, String writeIdList);
void alter_partition(String catName, String dbName, String tblName, Partition newPart, EnvironmentContext environmentContext);
void alter_partitions(String dbName, String tblName, List<Partition> newParts);
void alter_partitions(String dbName, String tblName, List<Partition> newParts, EnvironmentContext environmentContext);
void alter_partitions(String dbName, String tblName, List<Partition> newParts, EnvironmentContext environmentContext,String writeIdList, long writeId);
void alter_partitions(String catName, String dbName, String tblName, List<Partition> newParts);
void alter_partitions(String catName, String dbName, String tblName, 
List<Partition> newParts, EnvironmentContext environmentContext, String writeIdList, long writeId);
void renamePartition(final String dbname, final String tableName, final List<String> part_vals, final Partition newPart);
void renamePartition(String catName, String dbname, String tableName, List<String> part_vals, Partition newPart, String validWriteIds)

These should be implemented, in order to completely support partition on temporary tables. 
 Disable concurrent execution of the pre commit job for the same PR",yes
13183916,"TestHFileArchiving#testCleaningRace creates HFileCleaner instance within the test.
When SnapshotHFileCleaner.init() is called, there is no master parameter passed in params.
When the chore runs the cleaner during the test, NPE comes out of this line in getDeletableFiles():


      return cache.getUnreferencedFiles(files, master.getSnapshotManager());


since master is null.
We should either check for the null master or, pass master instance properly when constructing the cleaner instance. 
 Partially initialized SnapshotHFileCleaner leads to NPE during TestHFileArchiving",13318439,"HIVE-21784 uses a new WriterOptions instead of the field in OrcRecordUpdater:
https://github.com/apache/hive/commit/f62379ba279f41b843fcd5f3d4a107b6fcd04dec#diff-bb969e858664d98848960a801fd58b5cR580-R583
so in this scenario, the overwrite creates an empty bucket file, which is fine as that was the intention of that patch, but it creates that with invalid schema:


CREATE TABLE test_table (
   cda_id             int,
   cda_run_id         varchar(255),
   cda_load_ts        timestamp,
   global_party_id    string)
PARTITIONED BY (
   cda_date           int,
   cda_job_name       varchar(12))
CLUSTERED BY (cda_id) 
INTO 2 BUCKETS
STORED AS ORC;


INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id
UNION ALL
SELECT 2 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id;

ALTER TABLE test_table ADD COLUMNS (group_id string) CASCADE ;

INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id, 'group_id' as group_id;


because of HIVE-21784, the new empty bucket_00000 shows this schema in orc dump:


Type: struct<_col0:int,_col1:varchar(255),_col2:timestamp,_col3:string,_col4:string>


instead of:


Type: struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<cda_id:int,cda_run_id:varchar(255),cda_load_ts:timestamp,global_party_id:string,group_id:string>>


and this could lead to problems later, when hive tries to look into the file during split generation 
 Empty bucket files are inserted with invalid schema after HIVE-21784",No
13221715,"HBaseTimelineSchemaCreator provides option to configure custom table names for timelineservice tables. The option skipExistingTable ignores the tables already created with custom tablenames and recreates the tables with default table names. The custom table names has to be persisted so that skipExistingTable, HBaseTimelineWriterImpl, HBaseTimelineReaderImpl can use this. It currently expects to pass all custom table names every time TimelineSchemaCreator is used. 
 HBaseTimelineSchemaCreator skipExistingTable should honor custom hbase tablenames",13221706,"HbaseTimelineSchemaCreator provides option to provide custom table name and it creates properly. But The HBaseTimelineWriterImpl / HBaseTimelineReaderImpl does not know the custom name and uses the table with default name leading to data loss.
NM TimelineCollector inserts to default table name '{{prod.timelineservice.entity' }} which won;t be exist.


2019-03-14 15:37:10,739 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
javax.ws.rs.WebApplicationException: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 20 actions: Table 'prod.timelineservice.entity' was not found, got: prod.timelineservice.domain.: 20 times,
        at org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService.putEntities(TimelineCollectorWebService.java:197)
        at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)

 
 Update document for ATS HBase Custom tablenames (-entityTableName)",yes
13273889,"The current permission checker of #MountTableStoreImpl is not very restrict. In some case, any user could add/update/remove MountTableEntry without the expected permission checking.
The following code segment try to check permission when operate MountTableEntry, however mountTable object is from Client/RouterAdmin MountTable mountTable = request.getEntry();, and user could pass any mode which could bypass the permission checker.


  public void checkPermission(MountTable mountTable, FsAction access)
      throws AccessControlException {
    if (isSuperUser()) {
      return;
    }

    FsPermission mode = mountTable.getMode();
    if (getUser().equals(mountTable.getOwnerName())
        && mode.getUserAction().implies(access)) {
      return;
    }

    if (isMemberOfGroup(mountTable.getGroupName())
        && mode.getGroupAction().implies(access)) {
      return;
    }

    if (!getUser().equals(mountTable.getOwnerName())
        && !isMemberOfGroup(mountTable.getGroupName())
        && mode.getOtherAction().implies(access)) {
      return;
    }

    throw new AccessControlException(
        ""Permission denied while accessing mount table ""
            + mountTable.getSourcePath()
            + "": user "" + getUser() + "" does not have "" + access.toString()
            + "" permissions."");
  }


I just propose revoke WRITE MountTableEntry privilege to super user only. 
 RBF: Impose directory level permissions for Mount entries",13286407,"I can find some parity block's content with all 0 when i decommission some DataNode(more than 1) from a cluster. And the probability is very big(parts per thousand).This is a big problem.You can think that if we read data from the zero parity block or use the zero parity block to recover a block which can make us use the error data even we don't know it.
There is some case in the below:
B: Busy DataNode,
D:Decommissioning DataNode,
Others is normal.
1.Group indices is[0, 1, 2, 3, 4, 5, 6(B,D), 7, 8(D)].
2.Group indices is[0(B,D), 1, 2, 3, 4, 5, 6(B,D), 7, 8(D)].
....
In the first case when the block group indices is[0, 1, 2, 3, 4, 5, 6(B,D), 7, 8(D)], the DN may received reconstruct block command and theliveIndices=[0, 1, 2, 3, 4, 5, 7, 8] and the targets's(the field which in the class StripedReconstructionInfo) length is 2. 
The targets's length is 2 which mean that the DataNode need recover 2 internal block in current code.But from the liveIndices we only can find 1 missing block, so the method StripedWriter#initTargetIndices will use 0 as the default recover block and don't care the indices 0 is in the sources indices or not.
When they use sources indices [0, 1, 2, 3, 4, 5]to recover indices [6, 0] use the ec algorithm.We can find that the indices [0]is in the both the sources indices and the targets indices in this case.The returned target buffer in the indices [6]is always 0 from the ecalgorithm.So I think this is theec algorithm's problem. Because it should more fault tolerance.I try to fixed it .But it is too hard. Because the case is too more. The second is another case in the example above(use sources indices [1, 2, 3, 4, 5, 7]to recover indices [0, 6, 0]). So I changed my mind.Invoke theecalgorithm with a correct parameters. Which mean that remove the duplicate target indices 0 in this case.Finally, I fixed it in this way.
 
 Erasure Coding: Decommission may generate the parity block's content with all 0 in some case",No
13168953,"    int deleted = objStore.deleteRuntimeStats(1);
    assertEquals(1, deleted);
The testCleanup could fail if somehow there is GC pause before deleteRuntimeStats happens so actually 2 stats will get deleted rather than one. 
 TestRuntimeStats.testCleanup is flaky",13167734,"This test is timing dependent and sometimes fails.You can see that it sometimes fails in otherwise clean runs.The test inserts a stat, sleeps for 2 seconds, inserts another stat, then deletes stats that are older than 1 second. The test asserts that exactly one stat is deleted. If the deletion is slow for some reason (perhaps a GC?) then 2 stats will be deleted and the test will fail. The trouble is that the 1 second window is too small to workconsistently. 
 TestRuntimeStats.testCleanup() is flaky",yes
13168309,"Intermediately Resource manager going down with following exceptions

2018-06-25 15:24:30,572 FATAL event.EventDispatcher (EventDispatcher.java:run(75)) - Error in handling event type NODE_UPDATE to the Event Dispatcher
java.lang.NullPointerException
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.getLocalityWaitFactor(RegularContainerAllocator.java:268)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.canAssign(RegularContainerAllocator.java:315)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignOffSwitchContainers(RegularContainerAllocator.java:388)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainersOnNode(RegularContainerAllocator.java:469)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.tryAllocateOnNode(RegularContainerAllocator.java:250)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:819)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:857)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocator.assignContainers(ContainerAllocator.java:55)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:868)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1121)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:734)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:558)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:734)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:558)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:734)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:558)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1338)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1333)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1422)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1197)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1059)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1464)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:150)
    at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)
    at java.lang.Thread.run(Thread.java:745)
2018-06-25 15:24:30,573 INFO event.EventDispatcher (EventDispatcher.java:run(79)) - Exiting, bbye..
2018-06-25 15:24:30,579 ERROR delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:run(690)) - ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted

Before the build we applied the patches available for this version where we found the same kind of exception in one of the Jira
https://issues.apache.org/jira/browse/YARN-6629
but on different class file. 
 Resource Manager shutdown with FATAL Exception",13154165,"When running massive queries successively, at some point RM just hangs and stops allocating resources.At the point RM get hangs, YARN throwNullPointerException at RegularContainerAllocator.getLocalityWaitFactor.
There's sufficient space given to yarn.nodemanager.local-dirs (not a node health issue, RM didn't report any node being unhealthy). There is no fixed trigger for this (query or operation).
This problem goes away on restarting ResourceManager. No NM restart is required.

 
 YARN RM hangs abruptly (stops allocating resources) when running successive applications.",yes
13245654,"Only for 3.0.0. Remove the methods which mark deprecated inHBASE-22673. 
 Remove the deprecated methods in Hbck interface",13207436,"Currently HBase depends on version 18 which was released on2016-05-18. Version 21 was released in August 2018.
Relevant dependency upgrades




Name
Currently used version
New version
Notes


surefire.version
2.21.0
2.22.0



maven-compiler-plugin
3.6.1
3.7



maven-dependency-plugin
3.0.1
3.1.1



maven-jar-plugin
3.0.0
3.0.2



maven-javadoc-plugin
3.0.0
3.0.1



maven-resources-plugin
2.7
3.1.0



maven-site-plugin
3.4
3.7.1
Currently not relying on ASF version. See: HBASE-18333


maven-source-plugin
3.0.0
3.0.1



maven-shade-plugin
3.0.0
3.1.1
Newly added to ASF pom


maven-clean-plugin
3.0.0
3.1.0



maven-project-info-reports-plugin
2.9
3.0.0




Version 21 addednet.nicoulaj.maven.plugins:checksum-maven-plugin which introduced SHA512 checksum instead of SHA1. Should verify if we can rely on that for releases or breaks our current processes.
 
 Move to latest ASF Parent POM",No
13311956,"Listing api returns a Remote iterator. Currently it is blocking in the constructor Listing class. Also it is done in batches, so if a batch is exhausted , the next batch will be fetched from s3
thus causing small pauses on the client side every time a batch is fetched. We can add the following optimisations to fix these:

Make the constructor list call to s3 asynchronous.
Fetch the next batch asynchronously while the current batch is getting processed.

 
 Optimise s3a Listing to be fully asynchronous.",13191384,"On loaded jenkins nodes it will be easily timed out, so let's increase the timeout value. 
 Increase the waiting timeout for TestProcedurePriority",No
13180075,"Saw this is a run against tip of branch-2.0. The region had just finished being split when the move goes to run.


2018-08-18 16:55:14,908 INFO  [PEWorker-2] procedure2.ProcedureExecutor: Finished pid=2028, state=SUCCESS, hasLock=false; SplitTableRegionProcedure table=IntegrationTestBigLinkedList, parent=c3f199b5af62ae2ff8f8b6426b21d95d, daughterA=31ccbf098ae615ce30f28ec84c956b8f, daughterB=1890b4c96736f223f31efef11c817c90 in 9.0090sec
2018-08-18 16:55:14,908 INFO  [PEWorker-16] procedure.MasterProcedureScheduler: pid=2038, ppid=2030, state=RUNNABLE:MOVE_REGION_UNASSIGN, hasLock=false; MoveRegionProcedure hri=c3f199b5af62ae2ff8f8b6426b21d95d, source=ve0540.halxg.cloudera.com,16020,1534632630737, destination=ve0540.halxg.cloudera.com,16020,1534632630737 checking lock on c3f199b5af62ae2ff8f8b6426b21d95d
2018-08-18 16:55:14,958 INFO  [PEWorker-16] procedure2.ProcedureExecutor: Initialized subprocedures=[{pid=2095, ppid=2038, state=RUNNABLE:REGION_TRANSITION_DISPATCH, hasLock=false; UnassignProcedure table=IntegrationTestBigLinkedList, region=c3f199b5af62ae2ff8f8b6426b21d95d, server=ve0540.halxg.cloudera.com,16020,1534632630737}]
2018-08-18 16:55:15,008 INFO  [PEWorker-3] procedure.MasterProcedureScheduler: pid=2095, ppid=2038, state=RUNNABLE:REGION_TRANSITION_DISPATCH, hasLock=false; UnassignProcedure table=IntegrationTestBigLinkedList, region=c3f199b5af62ae2ff8f8b6426b21d95d, server=ve0540.halxg.cloudera.com,16020,1534632630737 checking lock on c3f199b5af62ae2ff8f8b6426b21d95d
2018-08-18 16:55:15,085 ERROR [PEWorker-3] procedure2.ProcedureExecutor: CODE-BUG: Uncaught runtime exception: pid=2095, ppid=2038, state=RUNNABLE:REGION_TRANSITION_DISPATCH, hasLock=true; UnassignProcedure table=IntegrationTestBigLinkedList, region=c3f199b5af62ae2ff8f8b6426b21d95d, server=ve0540.halxg.cloudera.com,16020,1534632630737
java.lang.NullPointerException
  at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936)
  at org.apache.hadoop.hbase.master.assignment.RegionStates.getOrCreateServer(RegionStates.java:1097)
  at org.apache.hadoop.hbase.master.assignment.RegionStates.addRegionToServer(RegionStates.java:1125)
  at org.apache.hadoop.hbase.master.assignment.AssignmentManager.markRegionAsClosing(AssignmentManager.java:1477)
  at org.apache.hadoop.hbase.master.assignment.UnassignProcedure.updateTransition(UnassignProcedure.java:204)
  at org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.execute(RegionTransitionProcedure.java:345)
  at org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.execute(RegionTransitionProcedure.java:97)
  at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:873)
  at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1556)
  at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1344)
  at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$900(ProcedureExecutor.java:76)
  at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1854)

 
 [amv2] CODE-BUG NPE in RTP doing Unassign",13178890,"Similar with HBASE-20921, ModifyTable procedure and reopenProcedure won't held the lock, so another procedures like split/merge can execute at the same time.
1. a split happend during ModifyTable, as you can see from the log, the split was nealy complete.


2018-08-05 01:28:31,339 INFO  [PEWorker-8] procedure2.ProcedureExecutor(1659): Finished subprocedure(s) of pid=772, state=RUNNABLE:SPLIT_TABLE_REGION_POST_OPERATION, hasLock=true; SplitTableRegionProce
dure table=IntegrationTestBigLinkedList, parent=357a7a6a62c76bc2d7ab30a6cc812637, daughterA=b13e5d155b65a5f752f3adda78fcfb6a, daughterB=5be3aadcee68d91c3d1e464865550246; resume parent processing.
2018-08-05 01:28:31,345 INFO  [PEWorker-8] procedure2.ProcedureExecutor(1296): Finished pid=795, ppid=772, state=SUCCESS, hasLock=false; AssignProcedure table=IntegrationTestBigLinkedList, region=b13e5
d155b65a5f752f3adda78fcfb6a, target=e010125048016.bja,60020,1533402809226 in 5.0280sec


2. reopenProcedure began to reopen region by moving it


2018-08-05 01:28:31,389 INFO  [PEWorker-11] procedure.MasterProcedureScheduler(631): pid=781, ppid=774, state=RUNNABLE:MOVE_REGION_UNASSIGN, hasLock=false; MoveRegionProcedure hri=357a7a6a62c76bc2d7ab3
0a6cc812637, source=e010125048016.bja,60020,1533402809226, destination=e010125048016.bja,60020,1533402809226 checking lock on 357a7a6a62c76bc2d7ab30a6cc812637
2018-08-05 01:28:31,390 INFO  [PEWorker-3] procedure2.ProcedureExecutor(1296): Finished pid=772, state=SUCCESS, hasLock=false; SplitTableRegionProcedure table=IntegrationTestBigLinkedList, parent=357a7
a6a62c76bc2d7ab30a6cc812637, daughterA=b13e5d155b65a5f752f3adda78fcfb6a, daughterB=5be3aadcee68d91c3d1e464865550246 in 21.9050sec
2018-08-05 01:28:31,518 INFO  [PEWorker-11] procedure2.ProcedureExecutor(1533): Initialized subprocedures=[{pid=797, ppid=781, state=RUNNABLE:REGION_TRANSITION_DISPATCH, hasLock=false; UnassignProcedur
e table=IntegrationTestBigLinkedList, region=357a7a6a62c76bc2d7ab30a6cc812637, server=e010125048016.bja,60020,1533402809226}]
2018-08-05 01:28:31,530 INFO  [PEWorker-15] procedure.MasterProcedureScheduler(631): pid=797, ppid=781, state=RUNNABLE:REGION_TRANSITION_DISPATCH, hasLock=false; UnassignProcedure table=IntegrationTest
BigLinkedList, region=357a7a6a62c76bc2d7ab30a6cc812637, server=e010125048016.bja,60020,1533402809226 checking lock on 357a7a6a62c76bc2d7ab30a6cc812637


3. MoveRegionProcdure fails since the region did not exis any more (due to split)


2018-08-05 01:28:31,543 ERROR [PEWorker-15] procedure2.ProcedureExecutor(1517): CODE-BUG: Uncaught runtime exception: pid=797, ppid=781, state=RUNNABLE:REGION_TRANSITION_DISPATCH, hasLock=true; Unassig
nProcedure table=IntegrationTestBigLinkedList, region=357a7a6a62c76bc2d7ab30a6cc812637, server=e010125048016.bja,60020,1533402809226
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936)
        at org.apache.hadoop.hbase.master.assignment.RegionStates.getOrCreateServer(RegionStates.java:1097)
        at org.apache.hadoop.hbase.master.assignment.RegionStates.addRegionToServer(RegionStates.java:1125)
        at org.apache.hadoop.hbase.master.assignment.AssignmentManager.markRegionAsClosing(AssignmentManager.java:1455)
        at org.apache.hadoop.hbase.master.assignment.UnassignProcedure.updateTransition(UnassignProcedure.java:204)
        at org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.execute(RegionTransitionProcedure.java:349)
        at org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.execute(RegionTransitionProcedure.java:101)
        at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:873)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1498)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1278)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$900(ProcedureExecutor.java:76)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1785)


We need to think about the case, and find a ultimately solution for it, otherwise, issues like this one and HBASE-20921 will keep comming. 
 Possible NPE if ModifyTable and region split happen at the same time",yes
13166312,"these are just hanging around in QTestUtil...they don't even in use...since they are looking for hadoop versions like  ""0.23"" and ""0.20"" ... 
 qtests: retire hadoop_major version specific tests; and logics",13191384,"On loaded jenkins nodes it will be easily timed out, so let's increase the timeout value. 
 Increase the waiting timeout for TestProcedurePriority",No
13160221,"Some test cases ofTestFsDatasetImpl failed on Windows due to:

using File#setWritable interface;
test directory conflict between test cases (details in HDFS-13408);

 
 Fix TestFsDatasetImpl test failures on Windows",13260189,"I am trying to bring commits from trunk/branch-3.2 to branch-3.1, but some of them do not compile because of the commons-logging to slf4j migration. 
One of the issue is GenericTestUtils.DelayAnswer do not accept slf4j logger API.
Backport HADOOP-14624 to branch-3.1 to make backport easier. It updates the DelayAnswer signature, but it's in the test scope, so we're not really breaking backward compat. 
 Backport HADOOP-14624 to branch-3.1",No
13337489,"HADOOP-17227 manages to get -min and -max mixed up through the call chain,. 


hadoop s3guard markers -audit -max 2000  s3a://stevel-london/


leads to


2020-10-27 18:11:44,434 [main] DEBUG s3guard.S3GuardTool (S3GuardTool.java:main(2154)) - Exception raised
46: Marker count 0 out of range [2000 - 0]
	at org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanResult.finish(MarkerTool.java:489)
	at org.apache.hadoop.fs.s3a.tools.MarkerTool.run(MarkerTool.java:318)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:505)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:2134)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.main(S3GuardTool.java:2146)
2020-10-27 18:11:44,436 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 46: 46: Marker count 0 out of range [2000 - 0]


Trivial fix. 
 S3A marker tool mixes up -min and -max",13151199,"Description:
Goals:

Allow infra engineer / data scientist to run unmodified Tensorflow jobs on YARN.
Allow jobs easy access data/models in HDFS and other storages.
Can launch services to serve Tensorflow/MXNet models.
Support run distributed Tensorflow jobs with simple configs.
Support run user-specified Docker images.
Support specify GPU and other resources.
Support launch tensorboard if user specified.
Support customized DNS name for roles (like tensorboard.$user.$domain:6006)

Why this name?

Because Submarine is the only vehicle canlet human to explore deep places. B-)

Please refer to on-going design doc, and add your thoughts:https://docs.google.com/document/d/199J4pB3blqgV9SCNvBbTqkEoQdjoyGMjESV4MktCo0k/edit#

Submarine all document list in here:https://issues.apache.org/jira/browse/SUBMARINE-113

See Also:

Zeppelin integration with Submarine design: https://docs.google.com/document/d/16YN8Kjmxt1Ym3clx5pDnGNXGajUT36hzQxjaik1cP4A/edit#heading=h.4jov859x47qe

 
 Hadoop {Submarine} Project: Simple and scalable deployment of deep learning training / serving jobs on Hadoop",No
13221280,"The workQueue length wasn't specified, so the thread number never be increase.
The thread number increase tomaximumPoolSize only when the workQueue is full.
file location: org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java
private void initializeStripedBlkReconstructionThreadPool(int numThreads) {
  LOG.debug(""Using striped block reconstruction; pool threads={}"",
    numThreads);
  stripedReconstructionPool = DFSUtilClient.getThreadPoolExecutor(2,
    numThreads, 60, new LinkedBlockingQueue<>(),
    ""StripedBlockReconstruction-"", false);
  stripedReconstructionPool.allowCoreThreadTimeOut(true);
 } 
 EC: Parameter maxPoolSize in striped reconstruct thread pool isn't affecting number of threads",13220588,"InErasureCodingWorker,stripedReconstructionPool is create by


initializeStripedBlkReconstructionThreadPool(conf.getInt(
    DFSConfigKeys.DFS_DN_EC_RECONSTRUCTION_THREADS_KEY,
    DFSConfigKeys.DFS_DN_EC_RECONSTRUCTION_THREADS_DEFAULT));

private void initializeStripedBlkReconstructionThreadPool(int numThreads) {
  LOG.debug(""Using striped block reconstruction; pool threads={}"",
      numThreads);
  stripedReconstructionPool = DFSUtilClient.getThreadPoolExecutor(2,
      numThreads, 60, new LinkedBlockingQueue<>(),
      ""StripedBlockReconstruction-"", false);
  stripedReconstructionPool.allowCoreThreadTimeOut(true);
}

so stripedReconstructionPool is a ThreadPoolExecutor, and the queue is aLinkedBlockingQueue, then the active thread is awalys 2, thedfs.datanode.ec.reconstruction.threads not take effect.
 
 dfs.datanode.ec.reconstruction.threads not take effect",yes
13291666,"I observed a test failure in TestZooKeeper#testMasterSessionExpired on my local rig. On a casual read of the logs from testMasterSessionExpired, it appears we have a faulty assumption related to master MTTR; the master abort is logged ~1250ms after ZK session close, which seems entirely too fast to me. Once the master aborts, the damage is done and the test cannot recover.
The first re-run passes. Surefire does not keep logs of successful tests, so I don't know the timing between events in the successful run. 
 [flakey test] TestZooKeeper",13291665,"Its failing with decent frequency of late in shutdown of cluster. Seems basic. There is an unassign/move going on. Test just checks Master can come back up after being killed. Does not check move is done. If on subsequent cluster shutdown, if the move can't report the Master because its shutting down, then the move fails, we abort the server, and then we get a wonky loop where we can't close because server is aborting.
At the root, there is a misaccounting when the unassign close fails where we don't cleanup references in the regionserver local RIT accounting. Deeper than this, close code is duplicated in three places that I can see; in RegionServer, in CloseRegionHandler, and in UnassignRegionHandler.
Let me fix this issue and the code dupe.
Details:
From https://builds.apache.org/job/HBase-Flaky-Tests/job/branch-2/5733/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.TestMasterAbortAndRSGotKilled-output.txt
Here is the unassign handler failing because master went down earlier (Its probably trying to talk to the old Master location)


***** ABORTING region server asf905.gq1.ygridcore.net,32989,1584000644108: Failed to close region ede67f9f661acc1241faf468b081d548 and can not recover *****
Cause:
java.io.IOException: Failed to report close to master: ede67f9f661acc1241faf468b081d548
	at org.apache.hadoop.hbase.regionserver.handler.UnassignRegionHandler.process(UnassignRegionHandler.java:125)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


... then the cluster shutdown tries to close the same Region... but fails because we are aborting because of above.... 


2020-03-12 08:11:16,600 ERROR [RS_CLOSE_REGION-regionserver/asf905:0-0] helpers.MarkerIgnoringBase(159): ***** ABORTING region server asf905.gq1.ygridcore.net,32989,1584000644108: Unrecoverable exception while closing region hbase:namespace,,1584000652744.78f4ae5beda711a9bebad0b6b8376cc9., still finishing close *****
java.io.IOException: Aborting flush because server is aborted...
	at org.apache.hadoop.hbase.regionserver.HRegion.internalPrepareFlushCache(HRegion.java:2545)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2530)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2504)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2495)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1650)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1552)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:110)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


....
And the RS keeps looping trying to close the Region even though we're aborted and there is handling in RS close Regions to deal with abort.
Trouble seems to be because when UnassignRegionHandler fails its region close, it does not unregister the Region with rs.getRegionsInTransitionInRS().remove(encodedNameBytes, Boolean.FALSE);
 
 [Flakey Tests] TestMasterAbortAndRSGotKilled fails in teardown",yes
13165172,"FSPermissionChecker may throw ArrayIndexOutOfBoundsException:0 when check if has permission, since it only check inode's aclFeature if null or not but not check it's entry size. When it meets aclFeature not null but it's entry size equal to 0, it will throw AIOOE.


private boolean hasPermission(INodeAttributes inode, FsAction access) {
  ......
  final AclFeature aclFeature = inode.getAclFeature();
  if (aclFeature != null) {
    // It's possible that the inode has a default ACL but no access ACL.
    int firstEntry = aclFeature.getEntryAt(0);
    if (AclEntryStatusFormat.getScope(firstEntry) == AclEntryScope.ACCESS) {
      return hasAclPermission(inode, access, mode, aclFeature);
    }
  }
  ......
}


Actually if use default INodeAttributeProvider, it can ensure that when inode's aclFeature is not null and it's entry size also will be greater than 0, but INodeAttributeProvider is a public interface, we could not ensure external implement (e.g. Apache Sentry, Apache Ranger) also has the similar constraint.  
 FSPermissionChecker may throws AIOOE when check inode permission",13202274,"TestThriftHttpServer is the first on the flaky list for branch-1 and branch-1.4 with approximately 60% failure rate.
Thrift server is not yet accepting request at the time the test starts.
java.net.ConnectException: Connection refused (Connection refused) at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.checkHttpMethods(TestThriftHttpServer.java:275) at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.testThriftServerHttpOptionsForbiddenWhenOptionsDisabled(TestThriftHttpServer.java:176) 
 ConnectException in TestThriftHttpServer",No
13283135,"I see this test fail a lot in my environments. It also uses such a large array that it seems particularly memory wasteful and difficult to get good contention in the test as well. 
 TestSyncTimeRangeTracker fails quite easily and allocates a very expensive array.",13156715,"As mentioned in this comment, the MRAppMaster fails for docker containers if there is no additional user lookup strategy (e.g. bind-mounting /var/run/nscd or /etc/passwd). We need a better solution so that users can still run even if they are not known inside of the container by name 
 MRAppMaster fails when using UID:GID pair within docker container",No
13223590,"Upgrade to consider security fixes.
Especiallyhttps://issues.apache.org/jira/browse/THRIFT-4506 
 Upgrade Thrift to 0.13.0",13308116,"Hive has been using thrift 0.9.3 for a long time. We might be able to avail new features like deprecation support etc in the newer releases of thrift. But this impacts interoperability between older clients and newer servers. We need to assess what can break atleast for the purposes of documenting before we make this change. 
 Upgrade thrift version in hive",yes
13345117,"Remove HTrace dependency as it is depending on old jackson jars. Use a no-op tracer for now to eliminate potential security issues.
The plan is to move part of the code in PR#1846 out here for faster review. 
 Replace HTrace with No-Op tracer",13341443,"HADOOP-17171 raised the concern that the deprecated htrace binaries has a few CVEs in its dependency jackson-databind. Not that HADOOP-15566 may not be merged any time soon. We can replace the existing htrace impl with a noop tracer (dummy).
This could be realized by reusing some code in HADOOP-15566's PR. 
 Replace HTrace with NoOp tracer",yes
13151214,"Now that branch for hive 3.0.0 is cut and we have started preparing for hive 3.1.0 development we need to add metastore upgrade scripts to upgrade from 3.0.0 to 3.1.0 
 Update metastore upgrade scripts to prepare for 3.1.0 development",13161817,"metastore schema init scripts are missing for the new release after branch was cut-out. 
 Create schema scripts for Hive 3.1.0 and Hive 4.0.0",yes
13301011,"After reverting HBASE-23881, it is fine. 
 TestDelegationToken is broken",13300407,"TestDelegationTokenWithEncryption andTestGenerateDelegationToken always fail.

Incidentally, they don't fail in branch-2.3 and branch-2.2.

I suspect there's a regression with delegation token code, because if I comment out the following code in the test, they pass:



try (Connection conn = ConnectionFactory.createConnection(TEST_UTIL.getConfiguration())) {
 Token<? extends TokenIdentifier> token = TokenUtil.obtainToken(conn);
 UserGroupInformation.getCurrentUser().addToken(token);
}


Effectively, use Kerberos to login instead of delegation token.
The tests fail all the time (100%) in the last 29 runs: 
https://builds.apache.org/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/dashboard.html
Initially I thought this was caused by pluggable authentication (HBASE-23347), but the tests don't fail in branch-2.3 so looks unlikely. 
 TestDelegationTokenWithEncryption always fails",yes
13315643,"Double locking blocks another threads in ResourceManager waiting for the lock.
I found the issue on testing hadoop-3.1.4-RC2 with RM-HA enabled deployment. ResourceManager blocks on submitApplication waiting for the lock when I run example MR applications.

""IPC Server handler 45 on default port 8032"" #211 daemon prio=5 os_prio=0 tid=0x00007f0e45a40200 nid=0x418 waiting on condition [0x00007f0e14abe000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000085d56510> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)
        at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2521)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:417)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:342)
        at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:678)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:277)
        at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:563)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1015)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:943)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2943)


 
 Fix double locking in CapacityScheduler#reinitialize in branch-3.1",13219206,"As a best practice - Reentrant lock has to be acquired before try clause. 
https://stackoverflow.com/questions/31058681/java-locking-structure-best-pattern
There are many places where lock is obtained inside try.


    try {
       this.writeLock.lock();
      ....
    } finally {
      this.writeLock.unlock();
    }


 
 Reentrant lock() before try",yes
13244400,"When I tried to read a Parquet file from a Hive (with Tez execution engine) table with a small decimal column, I got the following exception:


Caused by: java.lang.UnsupportedOperationException: org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$8$1
	at org.apache.parquet.io.api.PrimitiveConverter.addInt(PrimitiveConverter.java:98)
	at org.apache.parquet.column.impl.ColumnReaderImpl$2$3.writeValue(ColumnReaderImpl.java:248)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:367)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:226)
	... 28 more


Steps to reproduce:

Create a Hive table with a single decimal(4, 2) column
Create a Parquet file with int32 column annotated with decimal(4, 2) logical type, put it into the previously created table location (or use the attached parquet file, in this case the column should be named as 'd', to match the Hive schema with the Parquet schema in the file)
Execute a select * on this table

Also, I'm afraid that similar problems can happen with int64 decimals too. Parquet specification  allows both of these cases. 
 Hive is unable to read Parquet int32 annotated with decimal",13148585,"Parquet supports several minor types for Decimal ligical data type:
https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#decimal
But Hive supports only ""fixed_len_byte_array"":
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java#L335
After creating parquet external table and quering it via Hive:


hive> select * from decimal_parquet;
OK
Failed with exception java.io.IOException:org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file maprfs:///tmp/decimal_parquet/0_0_0.parquet


The sample of parquet file with decimal int32 values is added to the jira:


vitalii@vitalii-pc:~$ java -jar parquet-tools/parquet-mr/parquet-tools/target/parquet-tools-1.6.0rc3-SNAPSHOT.jar schema /tmp/decimal_parquet/0_0_0.parquet 
message root {
  optional binary a (UTF8);
  optional int32 b (DECIMAL(7,2));
}

vitalii@vitalii-pc:~$ java -jar parquet-tools/parquet-mr/parquet-tools/target/parquet-tools-1.6.0rc3-SNAPSHOT.jar cat /tmp/md4107_par/0_0_0.parquet 
a = a
b = 100

 
 Hive can't read int32 and int64 Parquet decimal values",yes
13337397,"If the Cleaner didn't remove any files, don't mark the compaction queue entry as ""succeeded"" but instead leave it in ""ready for cleaning"" state for later cleaning. If it removed at least one file, then the compaction queue entry as ""succeeded"". This is a partial fix, HIVE-24291 is the complete fix. 
 compactor.Cleaner should not set state ""mark cleaned"" if it didn't remove any files",13302959,"When doing ls on a mount point parent, the returned user/group ACL is incorrect. It always showing the user and group being current user, with some arbitrary ACL. Which could misleading any application depending on this API.
cc Chen Liang Virajith Jalaparti 
 ViewFS does not return correct user/group and ACL",No
13155200,"Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches. 
 Arrow batch serializer",13155202,"Support pushing arrow batches through org.apache.arrow.vector.ipc.ArrowOutputStream in LllapOutputFormatService. 
 Support ArrowOutputStream in LlapOutputFormatService",yes
13270626,"We need to manage different config groups in Ambari to run multiple thrift servers. This is because hbase thrift server will not able to resolve _HOST pattern fromhbase.thrift.spnego.principal. so we explicitly specify the hostname for each group now.
 
 Thrift Resolve FQDN from SPNEGO principal",13269098,"HBASE-19852 is not backwards compatible since it now requires the SPNEGO thrift configs. I haven't seen anything in Apache HBase about changing this so that the older configs still work with a merged keytab. (fall back to the non SPNEGO specific principal/keytab configs)
I wrote the original patch in HBASE-19852 and with hindsight being 20/20, I think this section of could be extended to fall back to not requiring the additional configs.
https://github.com/apache/hbase/blame/master/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift/ThriftHttpServlet.java#L78
Supporting the older configs allows upgrade from HBase 1.x to 2.x without needing to change the configs ahead of time. I'll make sure to log a deprecation warning if the older configs are used. 
 HBase Thrift SPNEGO configs (HBASE-19852) should be backwards compatible",yes
13220403,"YARN-6712 removes the usage of commons-logging dependency. The dependency can be removed. 
 Remove commons-logging dependency from remaining hadoop-yarn",13140688,"The assignment of version,suite and keyName should happen lazily, right before it's used in case the fileXAttr is null 
 Change the code order in getFileEncryptionInfo to avoid unnecessary call of assignment",No
13169640,"Hello!
distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. https://issues.apache.org/jira/browse/HADOOP-13786added support for more efficient S3 committers that do not use renames.
Please update distcp to use these efficient committers and no renames.
Thanks! 
 Update distcp to use zero-rename s3 committers",13209442,"When writing to an S3-based target, the temp file and rename logic in RetriableFileCopyCommand adds some unnecessary cost to the job, as the rename operation does a server-side copy + delete in S3 [1]. The renames are parallelized across all of the DistCp map tasks, so the severity is mitigated to some extent. However a configuration property to conditionally allow distributed copies to avoid that expense and write directly to the target path would improve performance considerably.
[1] https://github.com/apache/hadoop/blob/release-3.2.0-RC1/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md#object-stores-vs-filesystems 
 Avoid expensive rename when DistCp is writing to S3",yes
13140188,"After upgrading to the latest IDEA the IDE throws error messages in every few minutes like


The Checkstyle rules file could not be parsed.
SuppressionCommentFilter is not allowed as a child in Checker
The file has been blacklisted for 60s.

This is caused by some backward incompatible changes in checkstyle source code:
http://checkstyle.sourceforge.net/releasenotes.html

8.1: Make SuppressionCommentFilter and SuppressWithNearbyCommentFilter children of TreeWalker.
8.2: remove FileContentsHolder module as FileContents object is available for filters on TreeWalker in TreeWalkerAudit Event.

IDEA uses checkstyle 8.8
We should upgrade our checkstyle version to be compatible with IDEA's checkstyle plugin.
 Also it's a good time to upgrade maven-checkstyle-plugin as well to brand new 3.0. 
 Checkstyle version is not compatible with IDEA's checkstyle plugin",13191840,"Once weget pluggable device framework mature, we can port existingGPU related code into this new framework. 
 Documentation of the pluggable device framework",No
13191384,"On loaded jenkins nodes it will be easily timed out, so let's increase the timeout value. 
 Increase the waiting timeout for TestProcedurePriority",13329416,"As now we support altering meta table, it will be better to also deal with changing meta replica number using altering meta, i.e, we could unify the logic in MasterMetaBootstrap to ModifyTableProcedure, and another benefit is that we do not need to restart master when changing the replica number for meta. 
 Change meta replica count by altering meta table descriptor",No
13155969,"All these tests fail the same way. 
 TestNegativeMinimrCliDriver (cluster_tasklog_retrieval, mapreduce_stack_trace, mapreduce_stack_trace_turnoff, and minimr_broken_pipe) are failing",13155504,"this is apparently caused by HIVE-18739, specifically changing
private static ThreadLocal<SessionStates> tss in SessionState to private static InheritableThreadLocal<SessionStates> tss
need to figure out why this is.  
Looks like
TestNegativeMinimrCliDriver -Dqfile=mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace.q,cluster_tasklog_retrieval.q
are also broken by this 
 broken test: TestNegativeMinimrCliDriver#testCliDriver[minimr_broken_pipe]",yes
13174559,"Two things:

We truncate RpcServer output to 1000 characters for trace logging. Would be better if that value was configurable.
There is the chance for an ArrayIndexOutOfBounds when truncating the TRACE log message.

Esteban mentioned this to me earlier, so I'm crediting him as the reporter.
cc: Josh Elser 
 Improve RpcServer TRACE logging",13181697,"Two things:

We truncate RpcServer output to 1000 characters for trace logging. Would be better if that value was configurable.
There is the chance for an ArrayIndexOutOfBounds when truncating the TRACE log message.

Esteban mentioned this to me earlier, so I'm crediting him as the reporter.
cc: Josh Elser 
 Backport 'HBASE-20942 Improve RpcServer TRACE logging' to branch-2.1",yes
13205433,"I have seen a couple of precommit builds across JIRAs fail in TestSSLFactory#testServerWeakCiphers with the error:

[ERROR]   TestSSLFactory.testServerWeakCiphers:240 Expected to find 'no cipher suites in common' but got unexpected exception:javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)

 
 TestSSLFactory#testServerWeakCiphers sporadically fails in precommit builds",13171244,"After Kerberos ticket expires, RegistryDNS throws NPE error:


2018-07-06 01:26:25,025 ERROR yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(68)) - Thread Thread[TGT Renewer for rm/host1.example.com@EXAMPLE.COM,5,main] threw an Exception.

java.lang.NullPointerException

    at javax.security.auth.kerberos.KerberosTicket.getEndTime(KerberosTicket.java:482)

    at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:894)

    at java.lang.Thread.run(Thread.java:745)
 
 YARN RegistryDNS throws NPE when Kerberos tgt expires",No
13298387,"The following highlighted line seems to be incorrect in the test suite:
https://github.com/apache/hive/blob/master/ql/src/test/results/clientpositive/perf/tez/cbo_query95.q.out#L89
Note that the project takes all the columns from the table scan, yet it only needs a couple of them.
I did some very small debugging on this. When I removed the applyJoinOrderingTransform here:https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L1897
... the problem goes away. So presumably one of the rules in there is causing the problem.
Here is a slightly simplified version of the query which has the same problem (using the same tpc-ds database):
explain cbo with ws_wh as
(select ws1.ws_order_number
from web_sales ws1,web_returns wr2 
where ws1.ws_order_number = wr2.wr_order_number)
select
 ws_order_number
from
 web_sales ws1 
where
ws1.ws_order_number in (select wr_order_number
              from web_returns,ws_wh
              where wr_order_number = ws_wh.ws_order_number)
 
 Project not defined correctly after reordering a join",13149416,"Running the example commands specified in shell docs for ""append"" and ""incr"" throw following error:


ERROR: Failed to provide both column family and column qualifier for append



ERROR: Failed to provide both column family and column qualifier for incr

While running the same command via java does not require the user to provide both column and qualifier and works smoothly.

Steps to reproduce:
1) APPEND


hbase(main):002:0> create 't1', 'c1', 'c2'
Created table t1
Took 0.8151 seconds 
hbase(main):003:0> append 't1', 'r1', 'c1', 'value'

ERROR: Failed to provide both column family and column qualifier for append

Appends a cell 'value' at specified table/row/column coordinates.

 hbase> append 't1', 'r1', 'c1', 'value', ATTRIBUTES=>{'mykey'=>'myvalue'}
 hbase> append 't1', 'r1', 'c1', 'value', {VISIBILITY=>'PRIVATE|SECRET'}

The same commands also can be run on a table reference. Suppose you had a reference
t to table 't1', the corresponding command would be:

 hbase> t.append 'r1', 'c1', 'value', ATTRIBUTES=>{'mykey'=>'myvalue'}
 hbase> t.append 'r1', 'c1', 'value', {VISIBILITY=>'PRIVATE|SECRET'}

Took 0.0326 seconds

hbase(main):004:0> scan 't1'
ROW COLUMN+CELL 
0 row(s)
Took 0.1273 seconds 

While the same command would run if we run the following java code:


 try (Connection connection = ConnectionFactory.createConnection(config);
 Admin admin = connection.getAdmin();) {
 Table table = connection.getTable(TableName.valueOf(""t1""));
 Append a = new Append(Bytes.toBytes(""r1""));
 a.addColumn(Bytes.toBytes(""c1""), null, Bytes.toBytes(""value""));
 table.append(a);
 }

Scan result after executing java code:


hbase(main):005:0> scan 't1'
ROW COLUMN+CELL 
r1 column=c1:, timestamp=1522649623090, value=value 
1 row(s)
Took 0.0188 seconds



2) INCREMENT:
Similarly in case of increment, we get the following error (shell):


hbase(main):006:0> incr 't1', 'r2', 'c1', 111

ERROR: Failed to provide both column family and column qualifier for incr

Increments a cell 'value' at specified table/row/column coordinates.
To increment a cell value in table 'ns1:t1' or 't1' at row 'r1' under column
'c1' by 1 (can be omitted) or 10 do:

 hbase> incr 'ns1:t1', 'r1', 'c1'
 hbase> incr 't1', 'r1', 'c1'
 hbase> incr 't1', 'r1', 'c1', 1
 hbase> incr 't1', 'r1', 'c1', 10
 hbase> incr 't1', 'r1', 'c1', 10, {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> incr 't1', 'r1', 'c1', {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> incr 't1', 'r1', 'c1', 10, {VISIBILITY=>'PRIVATE|SECRET'}

The same commands also can be run on a table reference. Suppose you had a reference
t to table 't1', the corresponding command would be:

 hbase> t.incr 'r1', 'c1'
 hbase> t.incr 'r1', 'c1', 1
 hbase> t.incr 'r1', 'c1', 10, {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> t.incr 'r1', 'c1', 10, {VISIBILITY=>'PRIVATE|SECRET'}

Took 0.0103 seconds 
hbase(main):007:0> scan 't1'
ROW COLUMN+CELL 
r1 column=c1:, timestamp=1522649623090, value=value 
1 row(s)
Took 0.0062 seconds


While the same command would run, if we run the following java code:


 try (Connection connection = ConnectionFactory.createConnection(config);
 Admin admin = connection.getAdmin();) {
 Table table = connection.getTable(TableName.valueOf(""t1""));
 Increment incr = new Increment(Bytes.toBytes(""r2""));
 incr.addColumn(Bytes.toBytes(""c1""), null, 111);
 table.increment(incr);
 scan(table);
 }


Scan result after executing java code:


hbase(main):008:0> scan 't1'
ROW COLUMN+CELL 
r1 column=c1:, timestamp=1522649623090, value=value 
r2 column=c1:, timestamp=1522649933949, value=\x00\x00\x00\x00\x00\x00\x00o 
2 row(s)
Took 0.0133 seconds 

 
 When qualifier is not specified, append and incr operation do not work (shell)",No
13155178,"There is way too much logging when a user submits a query against a table which does not exist. In an ad-hoc setting, it is quite normal that a user fat-fingers a table name. Yet, from the perspective of the Hive administrator, there was perhaps a major issue based on the volume and severity of logging. Please change the logging to INFO level, and do not present a stack trace, for such a trivial error.

See the attached file for a sample of what logging a single ""table not found"" query generates. 
 Logging Too Verbose For TableNotFound",13151985,"Please improve the logging for queries that fail with SemanticException.  For example, when performing an action on atablethat doesnotexist. The most common reason why this happens is that someone fat fingers the table/column name. It is not a system error pe se, but a user validation error.  This is not something the cluster administrator should have to worry about.  Yet, Hive performs some pretty extreme logging on the matter.  I have attached to this JIRA the logging produced by a single submission of the following query:


select * from madeup;



For SemanticException exceptions, please print the Exception getMessage() to the server INFO logging so that the query's life-cycle can be traced, but do not blast ERRORs and stack traces to the log file. 
 Improve Logging for SemanticException Handling",yes
13155987,"This could be limited to assert exceptions; but might interfere with other exceptions...discovered while ""fixing"" testreopt after HIVE-19269


create table tu(id_uv int,id_uw int,u int);
create table tv(id_uv int,v int);
create table tw(id_uw int,w int);

insert into tu values (10,10,10),(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5),(6,6,6);
insert into tv values (10,10),(1,1),(2,2),(3,3);
insert into tw values (10,10),(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9);

set zzz=0;
set hive.vectorized.execution.enabled=false;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- fails as expected

set hive.vectorized.execution.enabled=true;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- there is a result set

 
 Vectorization: assert_true HiveException erroneously gets suppressed to NULL",13294547,"Added inHBASE-23946, we can remove this filter onceHBASE-24007 lands. 
 [JDK11] Remove `unit` filter from nightly and precommit jobs",No
13149508,"start-ozone.sh calls start-dfs.sh to start the NN and DN in a ozone cluster. Starting of datanode fails because of incomplete classpaths as datanode is unable to load all the plugins.
Setting the class path to the following values does resolve the issue:


export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/ozone/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/hdsl/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/ozone/lib/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/hdsl/lib/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/cblock/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/cblock/lib/*

 
 Ozone: start-ozone.sh fail to start datanode because of incomplete classpaths",13155987,"This could be limited to assert exceptions; but might interfere with other exceptions...discovered while ""fixing"" testreopt after HIVE-19269


create table tu(id_uv int,id_uw int,u int);
create table tv(id_uv int,v int);
create table tw(id_uw int,w int);

insert into tu values (10,10,10),(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5),(6,6,6);
insert into tv values (10,10),(1,1),(2,2),(3,3);
insert into tw values (10,10),(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9);

set zzz=0;
set hive.vectorized.execution.enabled=false;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- fails as expected

set hive.vectorized.execution.enabled=true;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- there is a result set

 
 Vectorization: assert_true HiveException erroneously gets suppressed to NULL",No
13163305,"pom.xml still points to <hive.version.shortname>3.1.0</hive.version.shortname> which causes issues with schemaTool init scripts 
 hive.version.shortname should be 4.0",13159234,"TestResolveHdfsSymlink#testFcResolveAfs fails on Windowswith error message:
[INFO] Running org.apache.hadoop.fs.TestResolveHdfsSymlink
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 28.574 s <<< FAILURE! - in org.apache.hadoop.fs.TestResolveHdfsSymlink
[ERROR] testFcResolveAfs(org.apache.hadoop.fs.TestResolveHdfsSymlink) Time elapsed: 0.039 s <<< ERROR!
java.io.IOException: Mkdirs failed to create file:/E:/OSS/hadoop/hadoop-hdfs-project/hadoop-hdfs/file:/E:/OSS/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/n014HnmeeA
 at org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:360)
 at org.apache.hadoop.fs.TestResolveHdfsSymlink.testFcResolveAfs(TestResolveHdfsSymlink.java:88)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)
[INFO]
[INFO] Results:
[INFO]
[ERROR] Errors:
[ERROR] TestResolveHdfsSymlink.testFcResolveAfs:88 ╗ IO Mkdirs failed to create file:/...
[INFO]
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0 
 merge master-txnstats branch",yes
13204592,"If a configuration file contains a setting for a property then later includes another file that also sets that property to a different value then the property will be parsed incorrectly. For example, consider the following configuration file:

<configuration xmlns:xi=""http://www.w3.org/2001/XInclude"">
     <property>
         <name>myprop</name>
         <value>val1</value>
     </property>
<xi:include href=""/some/other/file.xml""/>
</configuration>


with the contents of /some/other/file.xml as:

     <property>
       <name>myprop</name>
       <value>val2</value>
     </property>


Parsing this configuration should result in myprop=val2, but it actually results in myprop=val1. 
 Order of property settings is incorrect when includes are processed",13202372,"If a configuration resource is a bufferedinputstream and the resource has an included xml file, the properties from the included file are read and stored in the properties of the configuration, but they are not stored in the resource cache. So, if a later resource is added to the config and the properties are recalculated from the first resource, the included properties are lost. 
 Configuration: Included properties are not cached if resource is a stream",yes
13147955,"Add support for Alter table add columns for Druid. 
Currently it is not supported and throws exception.  
 Add support for Alter table add columns for Druid",13185726,"./dev-support/bin/hadoop.sh
PATCH_NAMING_RULE=""https://wiki.apache.org/hadoop/HowToContribute""


https://wiki.apache.org/hadoop/HowToContribute was moved to https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute. Let's update the personality. 
 Update PATCH_NAMING_RULE in the personality file",No
13285370,"Right now if we have columnstat on a column ; we use that to estimate things about the column; - however if an UDF is executed on a column ; the resulting column is treated as unknown thing and defaults are assumed.
An improvement could be to give wide estimation(s) in case of frequently used udf.
For example; consider substr(c,1,1) ; no matter what the input; the output is at most a 1 long string 
 Enhance data size estimation for fields computed by UDFs",13199795,"Many java doc errors are in deviceplugin

[ERROR] /Users/rsharmaks/Repos/Apache/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DeviceRuntimeSpec.java:29: error: bad HTML entity
[ERROR]  * This is a spec used to prepare & run container.
[ERROR]                                   ^
[ERROR] /Users/rsharmaks/Repos/Apache/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/DeviceRuntimeSpec.java:35: error: bad HTML entity
[ERROR]  * The volume & device mounts describes key isolation requirements
[ERROR] /Users/rsharmaks/Repos/Apache/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/Device.java:56: error: unknown tag: domain
[ERROR]    * PCI Bus ID in format [[[[<domain>]:]<bus>]:][<slot>][.[<func>]].
[ERROR]                               ^
[ERROR] /Users/rsharmaks/Repos/Apache/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/Device.java:56: error: unknown tag: bus
[ERROR]    * PCI Bus ID in format [[[[<domain>]:]<bus>]:][<slot>][.[<func>]].
[ERROR]                                          ^
[ERROR] /Users/rsharmaks/Repos/Apache/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/Device.java:56: error: unknown tag: slot
[ERROR]    * PCI Bus ID in format [[[[<domain>]:]<bus>]:][<slot>][.[<func>]].
[ERROR]                                                   ^
[ERROR] /Users/rsharmaks/Repos/Apache/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/deviceplugin/Device.java:56: error: unknown tag: func
[ERROR]    * PCI Bus ID in format [[[[<domain>]:]<bus>]:][<slot>][.[<func>]].

 
 Javadoc error in deviceplugin package",No
13212082,"The project currently depends on libthrift-0.9.3, however thrift released 0.12.0 on 2019-JAN-04. This release includes a security fix for THRIFT-4506 (CVE-2018-1320). Updating thrift to the latest version will remove that vulnerability.
Also note the Apache Thrift project does not publish ""libfb303"" any longer. fb303 is contributed code (in '/contrib') and it has not been maintained.

Ps.: 0.9.3.1 also addresses the CVE, see THRIFT-4506 
 Upgrade Apache Thrift to 0.9.3-1",13202193,"I was looking into some compile profiles for tables with lots of columns; and it turned out that thrift 0.9.3 is allocating a List during every hashcode calculation; but luckily THRIFT-2877 is improving on that - so I propose to upgrade to at least 0.10.0  
 Upgrade thrift to at least 0.10.0",yes
13197879,"With automatic failover enabled, I am unable to make use of the new transitionToObserver HAAdmin command. This JIRA is to remove the limitation and allow manual transition between Standby and Observer. 
 Allow manual transition from Standby to Observer",13197877,"Currently if automatic failover is enabled in a HA environment, transition from standby to observer would be blocked:


[hdfs@*** hadoop-3.3.0-SNAPSHOT]$ bin/hdfs haadmin -transitionToObserver ha2
Automatic failover is enabled for NameNode at ****
Refusing to manually manage HA state, since it may cause
a split-brain scenario or other incorrect state.
If you are very sure you know what you are doing, please
specify the --forcemanual flag.


We should allow manual transition between standby and observer in this case. 
 Allow manual failover between standby and observer",yes
13318623,"Currently, the way to list snapshots is do a ls on <snapshotttableRootPath>/.snapshot directory. Since creation time is not recorded , there is no way to actually figure out the chronological order of snapshots. The idea here is to add a command to list snapshots for a snapshottable directory along with snapshot Ids which grow monotonically as snapshots are created in the system. With snapID, it will be helpful to figure out the chronology of snapshots in the system. 
 Add a command to list all snapshots for a snaphottable root with snapshot Ids",13322011,"CompactionTxnHandler often doesn't log the preparedStatement parameters, which is really painful when compaction isn't working the way it should. Also expand logging around compaction Cleaner, Initiator, Worker. And some formatting cleanup. 
 Improve logging around CompactionTxnHandler",No
13327655,"There is a constructor of PathLocation as follows, it's for creating a new PathLocation with a prioritised nsId.



public PathLocation(PathLocation other, String firstNsId) {
  this.sourcePath = other.sourcePath;
  this.destOrder = other.destOrder;
  this.destinations = orderedNamespaces(other.destinations, firstNsId);
}


When I was reading the code ofMultipleDestinationMountTableResolver, I thought this constructor was to create a PathLocation with an override destination. It took me a while before I realize this is a constructor to sort the destinations inside.
Maybe I think this constructor can be more clear about its usage?
 
 [Flaky Test] TestReplicationDisableInactivePeer#testDisableInactivePeer",13222779,"S3AFileSystem#innerMkdirscurrently does not create parent directories. The code checks for the existence of the parent directories and if they do not exist, it adds them tometadataStoreDirs list. But this listis not used and therefore the parent directories are never created. Only the given directory path is created. 
 S3AFileSystem#innerMkdirs builds needless lists",No
13244945,"Set up a standalone hbase cluster.
Start hbase shell.
 create a table and put some data.
 Try to take a snapshot with the command : snapshot 'default:table1', 'snap1'
 The command fails with an NPE error. 
 snapshot fails with NPE in standalone mode.",13223508,"It looks like afterHBASE-21098, taking a snapshot fails in local mode. 
 Taking a snapshot fails in local mode",yes
13298235,"For the owner of snapshots(not global admin user), currently list_snapshots returns empty if i just use simple acls for authorization but not use authentication.
The code in AccessController.preListSnapshot:


if (SnapshotDescriptionUtils.isSnapshotOwner(snapshot, user)) {
// list it, if user is the owner of snapshot
AuthResult result = AuthResult.allow(""listSnapshot "" + snapshot.getName(),
""Snapshot owner check allowed"", user, null, null, null);
accessChecker.logResult(result);
}

And SnapshotManager.takeSnapshotInternal:


if (User.isHBaseSecurityEnabled(master.getConfiguration()) && user != null) {
  builder.setOwner(user.getShortName());
}


User.isHBaseSecurityEnabled：


public static boolean isHBaseSecurityEnabled(Configuration conf) {
  return ""kerberos"".equalsIgnoreCase(conf.get(HBASE_SECURITY_CONF_KEY));
}


So i think the logic of setOwner is used for authorization, not authentication, SnapshotManager should not only setOwner when hbase.security.authentication = kerberos, which cause listSnapshots returns empty when i just use simple acls. 
 Backport HBASE-23896 to branch-1: Snapshot owner cannot delete snapshot when ACL is enabled and Kerberos is not enabled",13287724,"When ACL is enabled and Kerberos is not enabled, the snapshot owner cannot delete the snapshot. This is because the owner of the snapshot cannot be taken during permission verification. By investigation, found that only after HBase has enabled security authentication, the owner will be set when doing snapshot. 
SnapshotManager#takeSnapshotInternal
SnapshotManager.java

RpcServer.getRequestUser().ifPresent(user -> {
  if (User.isHBaseSecurityEnabled(master.getConfiguration())) {
    builder.setOwner(user.getShortName());
  }
});


 
 Snapshot owner cannot delete snapshot when ACL is enabled and Kerberos is not enabled",yes
13203077,"Before https://issues.apache.org/jira/browse/HIVE-20915 , the optimizer sort dynamic partitions is turn off for these tests. So these test can have group by in the query plan which can trigger compute statistics. After the jira, the optimizer is enabled, because of a a known bug with dynamic sorted partition optimization (HIVE-16100), the computer statistics does not happen. Fix the tests to disable the optimizer to test the same code path as before. 
 Fix autoColumnStats tests to make auto stats gather possible.",13192012,"Reproducer


set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.stats.autogather=true;

create table t11(i int, j int) partitioned by (s string);
insert into t11 partition(s) values(3,4, 'p1'),(4,5, 'p2'),(6,9,'p3');

hive> desc formatted t11 j;
OK
col_name            	j
data_type           	int
min
max
num_nulls
distinct_count
avg_col_len
max_col_len
num_trues
num_falses
bitVector
comment             	from deserializer
COLUMN_STATS_ACCURATE	{}




hive> explain insert into t11 partition(s) values(3,4, 'p1'),(4,5, 'p2'),(6,9,'p3');

STAGE PLANS:
  Stage: Stage-1
    Tez
      DagId: vgarg_20181016113701_f3aa9f8f-b38b-47a8-8149-b5521bf072f6:13
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
      DagName: vgarg_20181016113701_f3aa9f8f-b38b-47a8-8149-b5521bf072f6:13
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: _dummy_table
                  Row Limit Per Split: 1
                  Statistics: Num rows: 1 Data size: 10 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: array(const struct(3,4,'p1'),const struct(4,5,'p2'),const struct(6,9,'p3')) (type: array<struct<col1:int,col2:int,col3:string>>)
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 64 Basic stats: COMPLETE Column stats: COMPLETE
                    UDTF Operator
                      Statistics: Num rows: 1 Data size: 64 Basic stats: COMPLETE Column stats: COMPLETE
                      function name: inline
                      Select Operator
                        expressions: col1 (type: int), col2 (type: int), col3 (type: string)
                        outputColumnNames: _col0, _col1, _col2
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col2 (type: string)
                          sort order: +
                          Map-reduce partition columns: _col2 (type: string)
                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                          value expressions: _col0 (type: int), _col1 (type: int)
        Reducer 2
            Execution mode: vectorized
            Reduce Operator Tree:
              Select Operator
                expressions: VALUE._col0 (type: int), VALUE._col1 (type: int), KEY._col2 (type: string)
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                File Output Operator
                  compressed: false
                  Dp Sort State: PARTITION_SORTED
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  table:
                      input format: org.apache.hadoop.mapred.TextInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                      name: default.t11

  Stage: Stage-2
    Dependency Collection

  Stage: Stage-0
    Move Operator
      tables:
          partition:
            s
          replace: false
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.t11

  Stage: Stage-3
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: i, j
          Column Types: int, int
          Table: default.t11


Notice that explain plan has autogather stats branch missing 
 Autogather stats doesn't work when SDPO (sort dynamic partition optimization) is ON",yes
13345236,"In branch-3.3, javadoc is failing in hadoop-aws module.
https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2522/3/artifact/out/branch-javadoc-hadoop-tools_hadoop-aws.txt

[ERROR] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2522/src/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ListingOperationCallbacks.java:81: error: unexpected text
[ERROR] * {@link this.getMaxKeys()}.
[ERROR] ^


Found in https://github.com/apache/hadoop/pull/2522#issuecomment-741996431 
 Javadoc failure in hadoop-aws (branch-3.3)",13321043,"mvn javadoc:javadoc -pl hadoop-tools/hadoop-aws is failing:

[ERROR] /Users/aajisaka/git/hadoop/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ListingOperationCallbacks.java:80: error: unexpected text
[ERROR]    * {@link this.getMaxKeys()}.
[ERROR]      ^

 
 Fixing javadoc in ListingOperationCallbacks",yes
13206354,"


// If delta amount to apply is 0, don't write WAL or MemStore.
long deltaAmount = getLongValue(delta);
// TODO: Does zero value mean reset Cell? For example, the ttl.
apply = deltaAmount != 0;


This is anoptimization when increment 0. But it introduced some new problems.
1.As the TODO said,Does zero value mean reset ttl?
2.HBASE-17318 have to introduce a newvariable ""firstWrite"" because it don't apply 0.
3. There is a coprocessor methodpostMutationBeforeWAL to return a new cell. But it may be not applied.



// Give coprocessors a chance to update the new cell
if (coprocessorHost != null) {
  newCell =
      coprocessorHost.postMutationBeforeWAL(mutationType, mutation, currentValue, newCell);
}
// If apply, we need to update memstore/WAL with new value; add it toApply.
if (apply || firstWrite) {
  toApply.add(newCell);
}



So myproposal is remove thisoptimization.Any suggestions are welcomed.




 
 Remove the TODO when increment zero",13337489,"HADOOP-17227 manages to get -min and -max mixed up through the call chain,. 


hadoop s3guard markers -audit -max 2000  s3a://stevel-london/


leads to


2020-10-27 18:11:44,434 [main] DEBUG s3guard.S3GuardTool (S3GuardTool.java:main(2154)) - Exception raised
46: Marker count 0 out of range [2000 - 0]
	at org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanResult.finish(MarkerTool.java:489)
	at org.apache.hadoop.fs.s3a.tools.MarkerTool.run(MarkerTool.java:318)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:505)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.run(S3GuardTool.java:2134)
	at org.apache.hadoop.fs.s3a.s3guard.S3GuardTool.main(S3GuardTool.java:2146)
2020-10-27 18:11:44,436 [main] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 46: 46: Marker count 0 out of range [2000 - 0]


Trivial fix. 
 S3A marker tool mixes up -min and -max",No
13137405,"There are edge cases when the application recovery fails with an exception.
Example failure scenario:

setup: a queue is a leaf queue in the primary RM's config and the same queue is a parent queue in the secondary RM's config.
When failover happens with this setup, the recovery will fail for applications on this queue, and an APP_REJECTED event will be dispatched to the async dispatcher. On the same thread (that handles the recovery), a NullPointerException is thrown when the applicationAttempt is tried to be recovered (https://github.com/apache/hadoop/blob/55066cc53dc22b68f9ca55a0029741d6c846be0a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#L494). I don't see a good way to avoid the NPE in this scenario, because when the NPE occurs the APP_REJECTED has not been processed yet, and we don't know that the application recovery failed.

Currently the first exception will abort the recovery, and if there are X applications, there will be ~X passive -> active RM transition attempts - the passive -> active RM transition will only succeed when the last APP_REJECTED event is processed on the async dispatcher thread. 
 Improve error handling when application recovery fails with exception",13307183,"Resourcemanager recover failed when fair scheduler queue acl changed. Because of queue acl changed, when recover the application (addApplication() in fairscheduler) is rejected. Then recover the applicationAttempt (addApplicationAttempt() in fairscheduler) get Application is null. This will lead to two RM is at standby. Repeat as follows.

user run a long running application.
change queue acl (aclSubmitApps) so that the user does not have permission.
restart the RM.



2020-05-25 16:04:06,191 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1590393162216_0005 with final state: FAILED
2020-05-25 16:04:06,192 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplicationAttempt(FairScheduler.java:663)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1246)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:116)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1072)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1036)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:789)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:845)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:897)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:850)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:723)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:322)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:427)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1173)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:584)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:980)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1021)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1017)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1659)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1017)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:301)
        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:813)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)


 
 Resourcemanager recover failed when fair scheduler queue acl changed",yes
13247983,"Quote [~jojochuang]'s comment:

ContentSummary has a field erasureCodingPolicy which was added in HDFS-11647, but webhdfs GETCONTENTSUMMARY doesn't include that.
Examples:
Directory, Before

GET /webhdfs/v1/tmp/?op=GETCONTENTSUMMARY HTTP/1.1

{
  ""ContentSummary"": {
    ""directoryCount"": 15,
    ""fileCount"": 1,
    ""length"": 180838,
    ""quota"": -1,
    ""spaceConsumed"": 542514,
    ""spaceQuota"": -1,
    ""typeQuota"": {}
  }
}


Directory, After, With EC policy RS-6-3-1024k set

GET /webhdfs/v1/tmp/?op=GETCONTENTSUMMARY HTTP/1.1

{
  ""ContentSummary"": {
    ""directoryCount"": 15,
    ""ecPolicy"": ""RS-6-3-1024k"",
    ""fileCount"": 1,
    ""length"": 180838,
    ""quota"": -1,
    ""spaceConsumed"": 542514,
    ""spaceQuota"": -1,
    ""typeQuota"": {}
  }
}


Directory, After, No EC policy set

GET /webhdfs/v1/tmp/?op=GETCONTENTSUMMARY HTTP/1.1

{
  ""ContentSummary"": {
    ""directoryCount"": 15,
    ""ecPolicy"": """",
    ""fileCount"": 1,
    ""length"": 180838,
    ""quota"": -1,
    ""spaceConsumed"": 542514,
    ""spaceQuota"": -1,
    ""typeQuota"": {}
  }
}


File, After, No EC policy set

GET /webhdfs/v1/tmp/file?op=GETCONTENTSUMMARY HTTP/1.1

{
  ""ContentSummary"": {
    ""directoryCount"": 0,
    ""ecPolicy"": ""Replicated"",
    ""fileCount"": 1,
    ""length"": 29,
    ""quota"": -1,
    ""spaceConsumed"": 29,
    ""spaceQuota"": -1,
    ""typeQuota"": {}
  }
}

 
 WebHDFS: Add erasureCodingPolicy field to GETCONTENTSUMMARY response",13247166,"HDFS-11647 added erasureCodingPolicy to ContentSummary. We should add this info to the result from WebHDFS getContentSummary call as well. 
 WebHDFS: Add erasureCodingPolicy to ContentSummary",yes
13259874,"It's more than flaky. Recently, in most of the cases it's the only failing test in otherwise green runs.
full hive logs:  hive.log.tar.gz 


Error Message
java.util.concurrent.ExecutionException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
Stacktrace
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.testMTT(TestBuddyAllocator.java:149)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.testMTT(TestBuddyAllocator.java:145)
	... 33 more
Caused by: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
	at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:454)
	at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:299)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.allocateAndUseBuffer(TestBuddyAllocator.java:254)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.allocateUp(TestBuddyAllocator.java:231)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.access$000(TestBuddyAllocator.java:43)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator$1.call(TestBuddyAllocator.java:119)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator$1.call(TestBuddyAllocator.java:116)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Standard Error
2019-09-30T03:04:43,170  INFO [main] LlapIoImpl: Buddy allocator with direct buffers; memory mapped off /home/hiveptest/104.197.225.41-hiveptest-1/apache-github-source-source/llap-server/target/tmp/llap-5698481178354861395; allocation sizes 8 - 256, arena size 256, total size 6144
2019-09-30T03:04:43,174  WARN [pool-6-thread-3] LlapIoImpl: Failed to allocate [3]X[256] bytes after [15] attempt, evicted [0] bytes and partially allocated [256] bytes
2019-09-30T03:04:43,174  WARN [pool-6-thread-2] LlapIoImpl: Failed to allocate [3]X[256] bytes after [15] attempt, evicted [0] bytes and partially allocated [0] bytes
2019-09-30T03:04:43,175 ERROR [pool-6-thread-2] LlapIoImpl: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
2019-09-30T03:04:43,175 ERROR [pool-6-thread-3] LlapIoImpl: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 256, allocated
Arena: 
  free list for size 8: 5, 
  free list for size 16: 0, 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 16, free
  block 2 at 16: size 8, allocated
  block 3 at 24: size 8, allocated
  block 4 at 32: size 8, allocated
  block 5 at 40: size 8, free
  block 6 at 48: size 16, allocated
  block 8 at 64: size 64, allocated
  block 16 at 128: size 128, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 6, 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 16, free
  block 2 at 16: size 16, allocated
  block 4 at 32: size 16, allocated
  block 6 at 48: size 16, free
  block 8 at 64: size 64, allocated
  block 16 at 128: size 128, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 0, 
  free list for size 64: 
  free list for size 128: 16, 
  free list for size 256: 
  block 0 at 0: size 32, free
  block 4 at 32: size 32, allocated
  block 8 at 64: size 32, allocated
  block 12 at 96: size 32, allocated
  block 16 at 128: size 128, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 64, allocated
  block 8 at 64: size 64, allocated
  block 16 at 128: size 128, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 64, allocated
  block 8 at 64: size 64, allocated
  block 16 at 128: size 64, allocated
  block 24 at 192: size 64, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 8, 
  free list for size 128: 16, 
  free list for size 256: 
  block 0 at 0: size 64, free
  block 8 at 64: size 64, free
  block 16 at 128: size 128, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
2019-09-30T03:04:43,176 ERROR [pool-6-thread-2] cache.TestBuddyAllocator: Failed to allocate 3 of 255; {[256.@0]}, {[16.@0][8*@2][8*@3][8*@4][8.@5][16*@6][64*@8][128*@16]}, {[16.@0][16*@2][16*@4][16.@6][64*@8][128*@16]}, {[32.@0][32*@4][32*@8][32*@12][128.@16]}, {[64.@0][64*@8][128*@16]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, 
2019-09-30T03:04:43,177 ERROR [pool-6-thread-3] cache.TestBuddyAllocator: Failed to allocate 3 of 255; {[256.@0]}, {[16.@0][8*@2][8*@3][8*@4][8.@5][16*@6][64*@8][128*@16]}, {[16.@0][16*@2][16*@4][16.@6][64*@8][128*@16]}, {[32.@0][32*@4][32*@8][32*@12][128.@16]}, {[64.@0][64*@8][128*@16]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, 

 
 In NameNode Web UI's Startup Progress page, Loading edits always shows 0 sec",13278086,"When DFSRouter fails to fetch or parse JMX output from NameNode, it prints only the error message. Therefore we had to modify the source code to print the stacktrace of the exception to find the root cause. 
 RBF: Print stacktrace when DFSRouter fails to fetch/parse JMX output from NameNode",No
13326378,"MetaFixer fails to fix overlaps when multiple tables have overlaps
Steps to reproduce from UT.

Create table t1 and t2 with split keys, [""bbb"", ""ccc"", ""ddd"", ""eee""]
Create extra region in both t1 and t2 with start key ""bbb"" and end key ""ddd""
Run catalog janitor, It will report total 4 overlaps, 2 from each table.
Run MetaFixer, wait for merges to finish.
Run the catalog janitor again and verify report, there should not be any overlap
Overlap still exists. Reproduced!!!

Analysis.

When I run the same scenario for just one table t1, overlaps are fixed successfully.
Seems problem with MetaFixer#calculateMerges.
I think merges should be calculated within a table. Across the table merge does not have significance.

 
 MetaFixer fails to fix overlaps when multiple tables have overlaps",13159983,"On clicking DN UI logs link, the HTTPS uri gets redirected to http Uri and fails. 
 DN UI logs link is broken when https is enabled",No
13327655,"There is a constructor of PathLocation as follows, it's for creating a new PathLocation with a prioritised nsId.



public PathLocation(PathLocation other, String firstNsId) {
  this.sourcePath = other.sourcePath;
  this.destOrder = other.destOrder;
  this.destinations = orderedNamespaces(other.destinations, firstNsId);
}


When I was reading the code ofMultipleDestinationMountTableResolver, I thought this constructor was to create a PathLocation with an override destination. It took me a while before I realize this is a constructor to sort the destinations inside.
Maybe I think this constructor can be more clear about its usage?
 
 [Flaky Test] TestReplicationDisableInactivePeer#testDisableInactivePeer",13184659,"In service-rpc/src/gen/thrift/gen-javabean/org/apache/hive/service/rpc/thrift/TOpenSessionReq, if client protocol is unset, validate() and toString() prints both username and password to logs.
Logging apassword is a security risk. We should hide the *******.
=====Edit===== (no longer relevant, see comments)
This issue is tricky since it is caused in a fully generated class. I've been playing around and have found one working solution, butI'd truly appreciate ideas for a more elegant solution or input.
The problem:
 TCLIService.thrift is the template for generating all classes in service-rpc. Struct TOpenSessionReq is OpenSession()'s one parameter and is defined thus:

struct TOpenSessionReq {
  1: required TProtocolVersion client_protocol = TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V10
  2: optional string username
  3: optional string password
  4: optional map<string, string> configuration
}


In the generated class TOpenSessionReq.java, client_protocol is checked by a validate() method, which is called quite a few times; if client_protocol is not set, it throws a TProtocolException, passing along a toString(). This toString() gets the names and values of all fields, including username and password.
Working solution:

Create a separate struct containing only the username and password, and pass it to OpenSession() as a second parameter. Since all fieldsin the new structare ""optional"", the generated validate() is empty – toString() is never used. This involves changing core classes and breaks the ""Each function should take exactly one parameter"" coding convention (detailed at service-rpc/if/TCLIService.thrift:27).
 See working-solution.patch.

What doesn't work:

Making client_protocol optional instead of required. Apparently this will break everything.
Overwriting toString() �– TOpenSessionReq is a struct.
Creating two Thrift structs, one struct for required (TRequiredReq) and one for optional (TOptionalReq) fields, and nesting them in struct TOpenSessionReq. This doesn't work because validate() in TOpenSessionReq can call TOptionalReq.toString(), which prints the password to logs. This will happen if TRequiredReq.client_protocol isn't set.
 See non-solution.patch
Asking Thrift devs to change their code. I wrote them an email but have no expectations.

 
 TOpenSessionReq logs password and username",No
13155701,"The HashSet comparison is not working for some reason:

java.lang.AssertionError: expected: java.util.HashSet<[hb]> but was: java.util.HashSet<[hb]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint(TestPlacementConstraintTransformations.java:116)

 
 YARN precommit build failing in TestPlacementConstraintTransformations",13215437,"Missing quotes in sql string is causing sql execution error for postgres.



metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(201)) - MetaException(message:Unable to update transaction database org.postgresql.util.PSQLException: ERROR: relat
ion ""database_params"" does not exist
Position: 25
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200)
at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424)
at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:321)
at org.postgresql.jdbc.PgStatement.executeQuery(PgStatement.java:284)
at com.zaxxer.hikari.pool.ProxyStatement.executeQuery(ProxyStatement.java:108)
at com.zaxxer.hikari.pool.HikariProxyStatement.executeQuery(HikariProxyStatement.java)
at org.apache.hadoop.hive.metastore.txn.TxnHandler.updateReplId(TxnHandler.java:907)
at org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn(TxnHandler.java:1023)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.commit_txn(HiveMetaStore.java:7703)
at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
at com.sun.proxy.$Proxy39.commit_txn(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$commit_txn.getResult(ThriftHiveMetastore.java:18730)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$commit_txn.getResult(ThriftHiveMetastore.java:18714)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:636)
at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:631)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:631)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
)

 
 Hive replication to a target with hive.strict.managed.tables enabled is failing when used HMS on postgres.",No
13169927,"Currently the hcat command adds HBase jars to the classpath by using find to walk the directories under $HBASE_HOME/lib.


# Look for HBase in a BigTop-compatible way. Avoid thrift version
# conflict with modern versions of HBase.
HBASE_HOME=${HBASE_HOME:-""/usr/lib/hbase""}
HBASE_CONF_DIR=${HBASE_CONF_DIR:-""${HBASE_HOME}/conf""}
if [ -d ${HBASE_HOME} ] ; then
   for jar in $(find $HBASE_HOME -name '*.jar' -not -name '*thrift*'); do
      HBASE_CLASSPATH=$HBASE_CLASSPATH:${jar}
   done
   export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CLASSPATH}""
fi
if [ -d $HBASE_CONF_DIR ] ; then
    HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CONF_DIR}""
fi


This is incorrect as that path contains jars for a mixture of purposes; hbase client jars, hbase server jars, and hbase shell specific jars. The inclusion of unneeded jars is mostly innocuous until the upcoming HBase 2.1.0 release. That release will have HBASE-20615 and HBASE-19735, which will mean most client facing installations will have a number of shaded client artifacts present.
With those changes in place, the current implementation will include in the hcat runtime a mix of shaded and non-shaded hbase artifacts that include some Hadoop classes rewritten to use a shaded version of protobuf. When these mix with other Hadoop classes in the classpath that have not been rewritten hcat will fail with errors that look like this:


Exception in thread ""main"" java.lang.ClassCastException: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to org.apache.hadoop.hbase.shaded.com.google.protobuf.Message
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:225)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
        at com.sun.proxy.$Proxy28.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:875)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
        at com.sun.proxy.$Proxy29.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1643)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1495)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1492)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1507)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1668)
        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:686)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:625)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:557)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:524)
        at org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:149)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:313)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:227)


To fix this issue in a way that transparently handles HBase versions the hcat command should take the same approach as the hive cli and ask HBase for the appropriate set of jars:


  # HBase detection. Need bin/hbase and a conf dir for building classpath entries.
  # Start with BigTop defaults for HBASE_HOME and HBASE_CONF_DIR.
  HBASE_HOME=${HBASE_HOME:-""/usr/lib/hbase""}
  HBASE_CONF_DIR=${HBASE_CONF_DIR:-""/etc/hbase/conf""}
  if [[ ! -d $HBASE_CONF_DIR ]] ; then
    # not explicitly set, nor in BigTop location. Try looking in HBASE_HOME.
    HBASE_CONF_DIR=""$HBASE_HOME/conf""
  fi

  # perhaps we've located the HBase config. if so, include it on classpath.
  if [[ -d $HBASE_CONF_DIR ]] ; then
    export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CONF_DIR}""
  fi

  # look for the hbase script. First check HBASE_HOME and then ask PATH.
  if [[ -e $HBASE_HOME/bin/hbase ]] ; then
    HBASE_BIN=""$HBASE_HOME/bin/hbase""
  fi
  HBASE_BIN=${HBASE_BIN:-""$(which hbase)""}

  # perhaps we've located HBase. If so, include its details on the classpath
  if [[ -n $HBASE_BIN ]] ; then
    # exclude ZK, PB, and Guava (See HIVE-2055)
    # depends on HBASE-8438 (hbase-0.94.14+, hbase-0.96.1+) for `hbase mapredcp` command
    for x in $($HBASE_BIN mapredcp 2>&2 | tr ':' '\n') ; do
      if [[ $x == *zookeeper* || $x == *protobuf-java* || $x == *guava* ]] ; then
        continue
      fi
      # TODO: should these should be added to AUX_PARAM as well?
      export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${x}""
    done
  fi

 
 Remove ATSHook",13138393,"Most common way to implement this is via ordered aggregate which allows users to specify sort specification with group by clause. Some implementations also allow to use these with window functions.
Since Hive doesn't have concept of ordered aggregates yet, one possibility is to support these only for window functions where sort specification is also taken from window clause. 
 Add percentile_cont and percentile_disc udaf",No
13199836,"submit patches of HADOOP-15920 to the HDFS yetus runs, see what they say, tune tests/HDFS as a appropriate 
 HDFS to pass new available() tests",13197743,"During internal testing, we found a nasty race condition which occurs during decommissioning.
Node manager, incorrect behaviour:

2018-06-18 21:00:17,634 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Received SHUTDOWN signal from Resourcemanager as part of heartbeat, hence shutting down.
2018-06-18 21:00:17,634 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Message from ResourceManager: Disallowed NodeManager nodeId: node-6.hostname.com:8041 hostname:node-6.hostname.com


Node manager, expected behaviour:

2018-06-18 21:07:37,377 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Received SHUTDOWN signal from Resourcemanager as part of heartbeat, hence shutting down.
2018-06-18 21:07:37,377 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Message from ResourceManager: DECOMMISSIONING  node-6.hostname.com:8041 is ready to be decommissioned


Note the two different messages from the RM (""Disallowed NodeManager"" vs ""DECOMMISSIONING""). The problem is that ResourceTrackerService can see an inconsistent state of nodes while they're being updated:

2018-06-18 21:00:17,575 INFO org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: hostsReader include:{172.26.12.198,node-7.hostname.com,node-2.hostname.com,node-5.hostname.com,172.26.8.205,node-8.hostname.com,172.26.23.76,172.26.22.223,node-6.hostname.com,172.26.9.218,node-4.hostname.com,node-3.hostname.com,172.26.13.167,node-9.hostname.com,172.26.21.221,172.26.10.219} exclude:{node-6.hostname.com}
2018-06-18 21:00:17,575 INFO org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: Gracefully decommission node node-6.hostname.com:8041 with state RUNNING
2018-06-18 21:00:17,575 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: Disallowed NodeManager nodeId: node-6.hostname.com:8041 node: node-6.hostname.com
2018-06-18 21:00:17,576 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Put Node node-6.hostname.com:8041 in DECOMMISSIONING.
2018-06-18 21:00:17,575 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn     IP=172.26.22.115        OPERATION=refreshNodes  TARGET=AdminService     RESULT=SUCCESS
2018-06-18 21:00:17,577 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Preserve original total capability: <memory:8192, vCores:8>
2018-06-18 21:00:17,577 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: node-6.hostname.com:8041 Node Transitioned from RUNNING to DECOMMISSIONING


When the decommissioning succeeds, there is no output logged from ResourceTrackerService. 
 get patch for S3a nextReadPos(), through Yetus",yes
13285482,"Add sample delegation SAS token generation code in test framework for reference for any authorizer adopters of SAS authentication. 
 ABFS: Test code with Delegation SAS generation logic",13270805,"ABFS supports OAuth and Shared Key but currently lacks support for [Shared Access Signatures (SAS)|https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview]. SAS is a great way to constrain access to a low-privilege ABFS client. The ABFS client does not need to possess persistent credentials for accessing storage but instead can request temporary, constrained access tokens from a trusted endpoint. This endpoint can authenticate the caller, make an authorization decision and return a constrained SAS token. The token may have an expiration, it may be scoped to a specific file or directory, and it may grant an action or set of actions such as read, write, list, or delete.
Azure Storage also has a new identity based SAS scheme in preview named Delegation SAS. These new Delegation SAS have these advantages over Service SAS:
1) Delegation SAS provide authentication as well as authorization. The user identity associated with each request will appear in the logs when logging is enabled for the account.
2) Instead of using storage account keys to sign tokens, Delegation SAS relies on keys assigned to each user. These keys are called user delegation keys. If a storage account key is leaked, an attacker would have full access to the storage account. If a user delegation key is leaked, an attacker would only have access to resources that user has access to within the Blob service–for example, the user might only have read access to a specific container.
Thisfeature will add support for the ABFS driver to authenticate against a trusted endpoint. The endpoint will return a SAS which the ABFS driver will use to access Azure storage. The SAS may be a container or directory SAS to be used for all subsequent operations, and thus cached for the lifetime of the filesystem. Or it may be a SAS to be used for the current filesystem operation, in this case, the ABFS driver will request a SAS for each operation.
 
 ABFS: Support for Shared Access Signatures (SAS)",yes
13154137,"Was digging into a problem with Ankit Singhal where setting RPC quotas on number of requests wasn't having any effect on a multi-Get
Ankit did enough digging to find that this was because each RPC was being treated as one request instead of the number of requests contained within the RPC itself.
Thinking as an operator, this is a pretty ineffective control because a user could just craft their API usage easily to work around any kind of limits I want to set to control their impact on the system. TimeBasedLimiter is assuming that one call to the quota code can only count as one request which I think is just wrong. 
 RPC quota requests ineffective due to not counting multi-actions",13136010,"Basically, rpc throttling does not work for request count based rater for multi. for the following code, when it calls limiter's checkQuota(), numWrites/numReads is lost.



@Override
public void checkQuota(int numWrites, int numReads, int numScans) throws ThrottlingException {
  writeConsumed = estimateConsume(OperationType.MUTATE, numWrites, 100);
  readConsumed = estimateConsume(OperationType.GET, numReads, 100);
  readConsumed += estimateConsume(OperationType.SCAN, numScans, 1000);

  writeAvailable = Long.MAX_VALUE;
  readAvailable = Long.MAX_VALUE;
  for (final QuotaLimiter limiter : limiters) {
    if (limiter.isBypass()) continue;

    limiter.checkQuota(writeConsumed, readConsumed);
    readAvailable = Math.min(readAvailable, limiter.getReadAvailable());
    writeAvailable = Math.min(writeAvailable, limiter.getWriteAvailable());
  }

  for (final QuotaLimiter limiter : limiters) {
    limiter.grabQuota(writeConsumed, readConsumed);
  }
}
 
 hbase rpc throttling does not work for multi() with request count rater.",yes
13170985,"VectorizedOrcInputFormat creates Orc reader options without passing in the configuration object.Without it setting orc configurations will not have any impact.
Example:
set orc.force.positional.evolution=true;
does not work for positional schema evolution (will attach test case). 
 OrcInputFormat does not pass conf to orc reader options",13160809,"Configuration is not passed to ORC's Reader.Option in OrcFileInputFormat which causes someORC configurations to not be able to be picked up.
Related issues:
For example, the ORC upgrade in Hive 2.3.x changed schema evolution from positional to column name matching. A backwards compatibility configuration ""orc.force.positional.evolution"" could be set in ORC Reader.Options by ORC-120 however it could not be picked up resulting in null values when querying ORC tables where the column names do not match. 
 Configuration not passed to ORC Reader.Options",yes
13310234,"In LocalFs and any other FileContext based on ChecksumFs, the renameInternal(src, dest, overwrite) method is broken since it is not implemented. The method in FilterFs will be invoked, which is checksum unaware. This can result in file leaks. 
 LocalFs rename is broken.",13228211,"ChecksumFS doesn't override FilterFS rename/3, so doesn't rename the checksum with the file.
As a result, if a file is renamed over an existing file using rename(src, dest, OVERWRITE) the renamed file will be considered to have an invalid checksum -the old one is picked up instead. 
 ChecksumFS.Make FileSystem.rename(path, path, options) doesn't rename checksum",yes
13215159,"When eventPerFile is enabled, writer is set to null after the first event, it causes an NPE in the next path until handleTick comes back. 
 NPE in HiveProtoLoggingHook for eventPerFile mode.",13215090,"https://github.com/apache/hive/blob/4ddc9de90b6de032d77709c9631ab787cef225d5/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java#L308can cause NPE. There is no uncaught exception handler for this thread. This NPE can silently fail and drop the event. 
 NPE in Hive Proto Logger",yes
13261621,"I have a default jdbc connection string and I want to build on top on it to have customized connections like setting custom queue names.  


$ cat .beeline/beeline-site.xml
  <configuration  xmlns:xi=""http://www.w3.org/2001/XInclude"">

    <property>
      <name>beeline.hs2.jdbc.url.base</name>
      <value>jdbc:hive2://localhost/</value>
    </property>

    <property>
      <name>beeline.hs2.jdbc.url.myqueue</name>
      <value>${beeline.hs2.jdbc.url.base}?tez.queue.name=myqueue</value>
    </property>
  </configuration>
$ beeline -c myqueue
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Error in parsing jdbc url: ${beeline.hs2.jdbc.url.base}?tez.queue.name=myqueue from beeline-site.xml
Beeline version 3.1.0.3.1.0.0-78 by Apache Hive
beeline>


Relevant code is found in https://github.com/apache/hive/blob/master/beeline/src/java/org/apache/hive/beeline/hs2connection/BeelineSiteParser.java#L94
Entry<T,T>#getValue() skips the variable expansion .  Using Configuration#get(key) would make this work. 
 variable expansion doesn't work in beeline-site.xml",13261495,"beeline-site.xml


<configuration xmlns:xi=""http://www.w3.org/2001/XInclude"">
 
 <property>
 <name>beeline.hs2.jdbc.url.container</name>
 <value>jdbc:hive2://c3220-node2.host.com:2181,c3220-node3.host.com:2181,c3220-node4.host.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2</value>
 </property>
 
 <property>
 <name>beeline.hs2.jdbc.url.default</name>
 <value>test</value>
 </property>
 <property>
<name>beeline.hs2.jdbc.url.test</name>
<value>${beeline.hs2.jdbc.url.container}?tez.queue.name=myqueue</value>
</property> 
 <property>
 <name>beeline.hs2.jdbc.url.llap</name>
 <value>jdbc:hive2://c3220-node2.host.com:2181,c3220-node3.host.com:2181,c3220-node4.host.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive</value>
 </property>
 
 </configuration>

beeline fail to connect because it does not parse the substituted value correctly


beeline
Error in parsing jdbc url: ${beeline.hs2.jdbc.url.container}?tez.queue.name=myqueue from beeline-site.xml
beeline>  
 
 Beeline-site parser does not handle the variable substitution correctly",yes
13150211,"

create table target stored as orc as select ss_ticket_number, ss_item_sk, current_timestamp as `ts` from tpcds_bin_partitioned_orc_1000.store_sales;

create table source stored as orc as select sr_ticket_number, sr_item_sk, d_date from tpcds_bin_partitioned_orc_1000.store_returns join tpcds_bin_partitioned_orc_1000.date_dim where d_date_sk = sr_returned_date_sk;


merge /* +semi(T, sr_ticket_number, S, 10000) */ into target T using (select * from source where year(d_date) = 1998) S ON T.ss_ticket_number = S.sr_ticket_number and sr_item_sk = ss_item_sk 
when matched THEN UPDATE SET ts = current_timestamp
when not matched and sr_item_sk is not null and sr_ticket_number is not null THEN INSERT VALUES(S.sr_ticket_number, S.sr_item_sk, current_timestamp);


The semijoin hints are ignored and the code says 


     todo: do we care to preserve comments in original SQL?


https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java#L624
in this case we do. 
 Merge: Semijoin hints are dropped by the merge",13324409,"If there are large number of events that haven't been cleaned up for some reason, then ObjectStore.cleanWriteNotificationEvents() can run out of memory while it loads all the events to be deleted.
 It should fetch events in batches.
Similar tohttps://issues.apache.org/jira/browse/HIVE-19430 
 ObjectStore.cleanWriteNotificationEvents OutOfMemory on large number of pending events",No
13264090,"hive-druid-handler jar has shaded version of druid classes, druid-hdfs-storage also has non-shaded classes. 

 
[hive@hiveserver2-1 lib]$ ls |grep druid
calcite-druid-1.19.0.7.0.2.0-163.jar
druid-bloom-filter-0.15.1.7.0.2.0-163.jar
druid-hdfs-storage-0.15.1.7.0.2.0-163.jar
hive-druid-handler-3.1.2000.7.0.2.0-163.jar
hive-druid-handler.jar


Exception below - 


Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:177)
  ... 22 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:765)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:630)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)
  ... 3 more
Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:96)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:104)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:743)
  ... 6 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.copyFilesWithChecks(HdfsDataSegmentPusher.java:163)
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.push(HdfsDataSegmentPusher.java:145)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$mergeAndPush$4(AppenderatorImpl.java:747)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86)

 
 Add ability to read Druid metastore password from jceks",13290058,"EC Blocks :blk_-9223372036854291632_10668910,blk_-9223372036854291631_10668910,blk_-9223372036854291630_10668910,blk_-9223372036854291629_10668910,blk_-9223372036854291628_10668910

Two block DN restarted :blk_-9223372036854291630_10668910 &blk_-9223372036854291632_10668910


2020-03-03 18:12:17,074 DEBUG hdfs.DataStreamer: DFSClient seqno: -2 reply: OOB_RESTART downstreamAckTimeNanos: 0 flag: 8
2020-03-03 18:13:39,469 DEBUG hdfs.DataStreamer: DFSClient seqno: -2 reply: OOB_RESTART downstreamAckTimeNanos: 0 flag: 8 


Restarted streams are stuck in below stacktrace :


java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.hadoop.hdfs.DFSStripedOutputStream$MultipleBlockingQueue.take(DFSStripedOutputStream.java:110) at org.apache.hadoop.hdfs.StripedDataStreamer.setupPipelineInternal(StripedDataStreamer.java:140) at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1540) at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1276) at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:669) at org.apache.hadoop.hdfs.StripedDataStreamer.run(StripedDataStreamer.java:46)

 
 EC : File write hanged when DN is shutdown by admin command.",No
13213619,"This is a missing part in our ITBLL. We can config a dummy replication endpoint which replicates nothing but only reads and verifies that all the wal files are readable and the entries are all fine.
With this I think it will be much easier to catch the problem in the parent issue. 
 Create a special ReplicationEndpoint just for verifying the WAL entries are fine",13151249,"Missed in HIVE-18859 
 Update golden files for negative tests",No
13301827,"Find out why the result set is different forcorrelationoptimizer14.q after moving to TestMiniLlapLocalCliDriver. Checkhttps://reviews.apache.org/r/72421/#comment308835for details. 
 Investigate why the results have changed for correlationoptimizer14.q",13272180,"There is a scenario when different SplitGenerator instances try to cover the delta-only buckets (having no base file) more than once, so there could be multiple OrcSplit instances generated for the same delta file, causing more tasks to read the same delta file more than once, causing duplicate records in a simple select star query.
File structure for a 256 bucket table


drwxrwxrwx   - hive hadoop          0 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/base_0000013
-rw-r--r--   3 hive hadoop        353 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/base_0000013/bucket_00012
-rw-r--r--   3 hive hadoop       1642 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/base_0000013/bucket_00140
drwxrwxrwx   - hive hadoop          0 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/delta_0000014_0000014_0000
-rwxrwxrwx   3 hive hadoop        348 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/delta_0000014_0000014_0000/bucket_00012
-rwxrwxrwx   3 hive hadoop       1635 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/delta_0000014_0000014_0000/bucket_00140
drwxrwxrwx   - hive hadoop          0 2019-11-29 16:04 /apps/hive/warehouse/naresh.db/test1/delta_0000015_0000015_0000
-rwxrwxrwx   3 hive hadoop        348 2019-11-29 16:04 /apps/hive/warehouse/naresh.db/test1/delta_0000015_0000015_0000/bucket_00012
-rwxrwxrwx   3 hive hadoop       1808 2019-11-29 16:04 /apps/hive/warehouse/naresh.db/test1/delta_0000015_0000015_0000/bucket_00140
drwxrwxrwx   - hive hadoop          0 2019-11-29 16:06 /apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000
-rwxrwxrwx   3 hive hadoop        348 2019-11-29 16:06 /apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00043
-rwxrwxrwx   3 hive hadoop       1633 2019-11-29 16:06 /apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00171


in this case, when bucket_00171 file has a record, and there is no base file for that, a select  with ETL split strategy can generate 2 splits for the same delta bucket...
the scenario of the issue:
1. ETLSplitStrategy contains a covered[] array which is shared between the SplitInfo instances to be created
2. a SplitInfo instance is created for every base file (2 in this case)
3. for every SplitInfo, a SplitGenerator is created, and in the constructor, parent's getSplit is called, which tries to take care of the deltas
I'm not sure at the moment what's the intention of this, but this way, duplicated delta split can be generated, which can cause duplicated read later (note that both tasks read the same delta file: bucket_00171)


2019-12-01T16:24:53,669  INFO [TezTR-127843_16_30_0_171_0 (1575040127843_0016_30_00_000171_0)] orc.ReaderImpl: Reading ORC rows from hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00171 with {include: [true, true, true, true, true, true, true, true, true, true, true, true], offset: 0, length: 9223372036854775807, schema: struct<idp_warehouse_id:bigint,idp_audit_id:bigint,batch_id:decimal(9,0),source_system_cd:varchar(500),insert_time:timestamp,process_status_cd:varchar(20),business_date:date,last_update_time:timestamp,report_date:date,etl_run_time:timestamp,etl_run_nbr:bigint>}
2019-12-01T16:24:53,672  INFO [TezTR-127843_16_30_0_171_0 (1575040127843_0016_30_00_000171_0)] lib.MRReaderMapred: Processing split: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat:OrcSplit [hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1, start=171, length=0, isOriginal=false, fileLength=9223372036854775807, hasFooter=false, hasBase=false, deltas=[{ minTxnId: 14 maxTxnId: 14 stmtIds: [0] }, { minTxnId: 15 maxTxnId: 15 stmtIds: [0] }, { minTxnId: 16 maxTxnId: 16 stmtIds: [0] }]]
2019-12-01T16:24:55,807  INFO [TezTR-127843_16_30_0_425_0 (1575040127843_0016_30_00_000425_0)] orc.ReaderImpl: Reading ORC rows from hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00171 with {include: [true, true, true, true, true, true, true, true, true, true, true, true], offset: 0, length: 9223372036854775807, schema: struct<idp_warehouse_id:bigint,idp_audit_id:bigint,batch_id:decimal(9,0),source_system_cd:varchar(500),insert_time:timestamp,process_status_cd:varchar(20),business_date:date,last_update_time:timestamp,report_date:date,etl_run_time:timestamp,etl_run_nbr:bigint>}
2019-12-01T16:24:55,813  INFO [TezTR-127843_16_30_0_425_0 (1575040127843_0016_30_00_000425_0)] lib.MRReaderMapred: Processing split: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat:OrcSplit [hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1, start=171, length=0, isOriginal=false, fileLength=9223372036854775807, hasFooter=false, hasBase=false, deltas=[{ minTxnId: 14 maxTxnId: 14 stmtIds: [0] }, { minTxnId: 15 maxTxnId: 15 stmtIds: [0] }, { minTxnId: 16 maxTxnId: 16 stmtIds: [0] }]]


seems like this issue doesn't affect AcidV2, as getSplits() returns an empty collection or throws an exception in case of unexpected deltas (which was the case here, where deltas was not unexpected):
https://github.com/apache/hive/blob/8ee3497f87f81fa84ee1023e891dc54087c2cd5e/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java#L1178-L1197 
 ACID v1: covered delta-only splits (without base) should be marked as covered (branch-2)",No
13210478,"Add S3A tests to run terasort for the magic and directory committers.
MAPREDUCE-7091 is a requirement for this
Bonus feature: print the results to see which committers are faster in the specific test setup. As that's a function of latency to the store, bandwidth and size of jobs, it's not at all meaningful, just interesting. 
 S3A tests to include Terasort",13218533,"TeraGen is hard-coded to use FileOutputCommitter to commit the job. This patch is to allow TeraGen to use schema-specific committer for optimization. For example, using S3A committers for s3a:// object storage. 
 Allow TeraGen to use schema-specific output committer",yes
13318439,"HIVE-21784 uses a new WriterOptions instead of the field in OrcRecordUpdater:
https://github.com/apache/hive/commit/f62379ba279f41b843fcd5f3d4a107b6fcd04dec#diff-bb969e858664d98848960a801fd58b5cR580-R583
so in this scenario, the overwrite creates an empty bucket file, which is fine as that was the intention of that patch, but it creates that with invalid schema:


CREATE TABLE test_table (
   cda_id             int,
   cda_run_id         varchar(255),
   cda_load_ts        timestamp,
   global_party_id    string)
PARTITIONED BY (
   cda_date           int,
   cda_job_name       varchar(12))
CLUSTERED BY (cda_id) 
INTO 2 BUCKETS
STORED AS ORC;


INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id
UNION ALL
SELECT 2 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id;

ALTER TABLE test_table ADD COLUMNS (group_id string) CASCADE ;

INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id, 'group_id' as group_id;


because of HIVE-21784, the new empty bucket_00000 shows this schema in orc dump:


Type: struct<_col0:int,_col1:varchar(255),_col2:timestamp,_col3:string,_col4:string>


instead of:


Type: struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<cda_id:int,cda_run_id:varchar(255),cda_load_ts:timestamp,global_party_id:string,group_id:string>>


and this could lead to problems later, when hive tries to look into the file during split generation 
 Empty bucket files are inserted with invalid schema after HIVE-21784",13310962,"The YARN CLI logs command line option -logFiles was changed to -log_files  in 2.9 and later releases.   This change was made as part of YARN-5363.
Verizon Media is in the process of moving from Hadoop-2.8 to Hadoop-2.10, and while testing integration with Spark, we ran into this issue.   We are concerned that we will run into more cases of this as we roll out to production, and rather than break user scripts, we'd prefer to add -logFiles as an alias of -log_files.  If both are provided, -logFiles will be ignored. 
 Add support for yarn logs -logFile to retain backward compatibility",No
13256025,"HBASE-22728 moved out jackson transitive dependencies. mostly good, but moving jackson2 to provided in hbase-server broke few things
testing-util needs a transitive jackson 2 in order to start the minicluster, currently fails with CNFE for com.fasterxml.jackson.databind.ObjectMapper when trying to initialize the master.
shaded-testing-util needs a relocated jackson 2 for the same reason
it's not used for any of the mapreduce stuff in hbase-server, so hbase-shaded-server for that purpose should be fine. But it is used by WALPrettyPrinter and some folks might expect that to work from that artifact since it is present. 
 Replace Jackson with relocated gson everywhere but hbase-rest",13169927,"Currently the hcat command adds HBase jars to the classpath by using find to walk the directories under $HBASE_HOME/lib.


# Look for HBase in a BigTop-compatible way. Avoid thrift version
# conflict with modern versions of HBase.
HBASE_HOME=${HBASE_HOME:-""/usr/lib/hbase""}
HBASE_CONF_DIR=${HBASE_CONF_DIR:-""${HBASE_HOME}/conf""}
if [ -d ${HBASE_HOME} ] ; then
   for jar in $(find $HBASE_HOME -name '*.jar' -not -name '*thrift*'); do
      HBASE_CLASSPATH=$HBASE_CLASSPATH:${jar}
   done
   export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CLASSPATH}""
fi
if [ -d $HBASE_CONF_DIR ] ; then
    HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CONF_DIR}""
fi


This is incorrect as that path contains jars for a mixture of purposes; hbase client jars, hbase server jars, and hbase shell specific jars. The inclusion of unneeded jars is mostly innocuous until the upcoming HBase 2.1.0 release. That release will have HBASE-20615 and HBASE-19735, which will mean most client facing installations will have a number of shaded client artifacts present.
With those changes in place, the current implementation will include in the hcat runtime a mix of shaded and non-shaded hbase artifacts that include some Hadoop classes rewritten to use a shaded version of protobuf. When these mix with other Hadoop classes in the classpath that have not been rewritten hcat will fail with errors that look like this:


Exception in thread ""main"" java.lang.ClassCastException: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to org.apache.hadoop.hbase.shaded.com.google.protobuf.Message
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:225)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
        at com.sun.proxy.$Proxy28.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:875)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
        at com.sun.proxy.$Proxy29.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1643)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1495)
        at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1492)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1507)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1668)
        at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:686)
        at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:625)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:557)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:524)
        at org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:149)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:313)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:227)


To fix this issue in a way that transparently handles HBase versions the hcat command should take the same approach as the hive cli and ask HBase for the appropriate set of jars:


  # HBase detection. Need bin/hbase and a conf dir for building classpath entries.
  # Start with BigTop defaults for HBASE_HOME and HBASE_CONF_DIR.
  HBASE_HOME=${HBASE_HOME:-""/usr/lib/hbase""}
  HBASE_CONF_DIR=${HBASE_CONF_DIR:-""/etc/hbase/conf""}
  if [[ ! -d $HBASE_CONF_DIR ]] ; then
    # not explicitly set, nor in BigTop location. Try looking in HBASE_HOME.
    HBASE_CONF_DIR=""$HBASE_HOME/conf""
  fi

  # perhaps we've located the HBase config. if so, include it on classpath.
  if [[ -d $HBASE_CONF_DIR ]] ; then
    export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${HBASE_CONF_DIR}""
  fi

  # look for the hbase script. First check HBASE_HOME and then ask PATH.
  if [[ -e $HBASE_HOME/bin/hbase ]] ; then
    HBASE_BIN=""$HBASE_HOME/bin/hbase""
  fi
  HBASE_BIN=${HBASE_BIN:-""$(which hbase)""}

  # perhaps we've located HBase. If so, include its details on the classpath
  if [[ -n $HBASE_BIN ]] ; then
    # exclude ZK, PB, and Guava (See HIVE-2055)
    # depends on HBASE-8438 (hbase-0.94.14+, hbase-0.96.1+) for `hbase mapredcp` command
    for x in $($HBASE_BIN mapredcp 2>&2 | tr ':' '\n') ; do
      if [[ $x == *zookeeper* || $x == *protobuf-java* || $x == *guava* ]] ; then
        continue
      fi
      # TODO: should these should be added to AUX_PARAM as well?
      export HADOOP_CLASSPATH=""${HADOOP_CLASSPATH}:${x}""
    done
  fi

 
 Remove ATSHook",No
13210478,"Add S3A tests to run terasort for the magic and directory committers.
MAPREDUCE-7091 is a requirement for this
Bonus feature: print the results to see which committers are faster in the specific test setup. As that's a function of latency to the store, bandwidth and size of jobs, it's not at all meaningful, just interesting. 
 S3A tests to include Terasort",13156416,"Terasort is very slow on S3, because it still uses the classic rename-to-commit algorithm on the sort, even while teragen and the reporting can use the new committer
Reason: org.apache.hadoop.examples.terasort.TeraOutputFormathas overriden getOutputCommitter even though it doesn't need to. 
 Terasort on S3A to switch to new committers",yes
13253234,"When we do source package of hadoop, it should not contain submarine project/code. 
 Exclude submarine from hadoop source build",13167227,"When a precommit run fails due to license issues, we get pointed to a file in our maven logs:

/testptch/hbase/hbase-assembly/target/maven-shared-archive-resources/META-INF/LICENSE


But we don't have that file saved, so we don't know what the actual failure was. So we should save that in our build artifacts. Or maybe we can print a snippet from that file directly into the maven log. Both would be acceptable. 
 precommit should archive generated LICENSE file",No
13142663,"In HBASE-20123, I logged the following stack trace:


2018-03-03 14:46:10,858 ERROR [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(237): java.io.IOException: Path hdfs://localhost:40578/backupUT/.tmp/backup_1520088356047 is not a symbolic link
java.io.IOException: Path hdfs://localhost:40578/backupUT/.tmp/backup_1520088356047 is not a symbolic link
  at org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:338)
  at org.apache.hadoop.fs.FileStatus.readFields(FileStatus.java:461)
  at org.apache.hadoop.tools.CopyListingFileStatus.readFields(CopyListingFileStatus.java:155)
  at org.apache.hadoop.io.SequenceFile$Reader.getCurrentValue(SequenceFile.java:2308)
  at org.apache.hadoop.tools.CopyListing.validateFinalListing(CopyListing.java:163)
  at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:91)
  at org.apache.hadoop.tools.GlobbedCopyListing.doBuildListing(GlobbedCopyListing.java:90)
  at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:84)
  at org.apache.hadoop.tools.DistCp.createInputFileListing(DistCp.java:382)
  at org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupCopyJob$BackupDistCp.createInputFileListing(MapReduceBackupCopyJob.java:297)


Steve Loughran pointed out that the assertion in FileStatus.java is not accurate:


    assert (isDirectory() && getSymlink() == null) || !isDirectory();



It's assuming that getSymlink() returns null if there is no symlink, but instead it raises and exception.
Steve proposed the following replacement:


    assert (!(isDirectory() && isSymlink())

 
 Imprecise assertion in FileStatus w.r.t. symlink",13249255,"There are some scraping error when using '/prom' endpoint:

invalid metric type ""_young _generation counter""
invalid metric type ""_old _generation counter""
invalid metric type ""apache.hadoop.hdfs.server.datanode.fsdataset.impl._fs_dataset_impl_dfs_used gauge""

 
 Fix invalid metric types in PrometheusMetricsSink",No
13184678,"fix java doc error from node attributes in yarn-api module. 
 Javadoc error in node attributes",13155536,"Hive can use statistics to answer queries like count. This can be problematic on external tables where another tool might add files that Hive doesn’t know about. In that case Hive will return incorrect results. 
 Disable compute.query.using.stats for external table",No
13135006,"This Jira will track toModify existing placement constraints to support node attributes. 
 Modify placement constraints to support node attributes",13159147,"Modifications required in Distributed shell to support NodeAttributes 
 Modify distributedshell to support Node Attributes",yes
13287276,"S3A innerMkdirs() calls getFileStatus() to probe dest path for being a file or dir, then goes to reject/no-op.
The HEAD path + / in that code may add a 404 to the S3 load balancers, so subsequent probes for the path fail. 
Proposed: only look for file then LIST underneath
if no entry found: probe for parent being a dir (LIST; HEAD + /), if true create the marker entry. If not, start the walk (or should we then check?)
This increases the cost of mkdir on an existing empty dir marker; reduces it on a non-empty dir. Creates dir markers above dir markers to avoid those cached 404s. 
 s3a mkdirs() to not check dest for a dir marker",13279610,"Not seen in the wild, but inferred from a code review
mkdirs creates an empty dir marker -but it looks for one first
proposed

only look for file marker and LIST; don't worry about an empty dir marker
always PUT one there

Saves a HEAD on every mkdir too 
 s3a mkdir path/ can add 404 to S3 load balancers",yes
13204972,"Currently the user guide created in HDFS-14131 does not make any mention of the recommendation for dfs.ha.tail-edits.period, but the default works very poorly in combination with this feature. We should update the documentation to reflect this. 
 Update ""Consistent Read from Observer"" User Guide with Edit Tailing Frequency",13204964,"We should document dfs.ha.tail-edits.period in the user guide. The default value is too large for ObserverReadProxyProvider and we should recommend a value.
We can also address some remaining issues in HDFS-14131. 
 Document dfs.ha.tail-edits.period in user guide.",yes
13176303,"There is a typo in the existing YarnConfiguration which uses theDEFAULT_NM_LOCALIZER_PORT as the default for NM Collector Service port. This Jira aims to fix the typo. 
 Fix NM Collector Service Port issue in YarnConfiguration",13131670,"Starting Beeline gives the following warnings:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in[jar:file:/opt/cloudera/parcels/CDH-6.x-1.cdh6.x.p0.215261/jars/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in[jar:file:/opt/cloudera/parcels/CDH-6.x-1.cdh6.x.p0.215261/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Seehttp://www.slf4j.org/codes.html#multiple_bindingsfor an explanation.
SLF4J: Actual binding is of type[org.apache.logging.slf4j.Log4jLoggerFactory]
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property 'org.apache.logging.log4j.simplelog.StatusLogger.level' to TRACE to show Log4j2 internal initialization logging.
 
 Beeline gives log4j warnings",No
13250682,"Sample text which has a map as its field:


E1719F13B8213BA7EE1694B0D2DC838B,1563490705041,detailtype:wt|topansweruuid:|questionuuid:57291db409ee405cb572d347babd8416
E8EB4433DB8F4CAB84FCDA769E2AE7BA,1563490701704,detailtype:wt|topansweruuid:|questionuuid:b26abb023eae4efc982924d35aad1f57

Create table,


CREATE TABLE temp_table(
user_id string,
ct STRING,
ep MAP<STRING,STRING>
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '|'
MAP KEYS TERMINATED BY "":""
STORED AS TEXTFILE
LOCATION '/user/test/temp_table';

Execute query,


SELECT lag(ep, 1) over (partition by user_id order by ct) as lag_ep
from temp_table;

It thows,


2019-08-14T10:46:01,626 INFO [pool-11-thread-1] exec.FileSinkOperator: Using serializer : class org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe[[[B@d44c263]:[_col0]:[map<string,string>]] and formatter : org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat@751a939a
2019-08-14T10:46:01,663 ERROR [pool-11-thread-1] ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":""04E55B1D60F5D70807BF224997623188"",""reducesinkkey1"":""1563490210162""},""value"":{""_col0"":{""detailtype"":""wt"",""topansweruuid"":"""",""questionuuid"":""3efc91897aec41249d39e2d2a4a6c0e5""}}}
at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:243)
at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryMap cannot be cast to java.util.Map
at org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.getMap(StandardMapObjectInspector.java:85)
at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:451)
at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:372)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag$GenericUDAFLeadLagEvaluator.iterate(GenericUDAFLeadLag.java:156)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:213)
at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.processRow(WindowingTableFunction.java:407)
at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.processRow(PTFOperator.java:325)
at org.apache.hadoop.hive.ql.exec.PTFOperator.process(PTFOperator.java:139)
at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:995)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:941)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:928)
at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:234)

However, simple select is OK:


select ep['detailtype'] from temp_table;

 
 Cannot use structured field in a window function for a text table",13208431,"
DROP TABLE IF EXISTS dummy;
CREATE TABLE dummy (i int);
INSERT INTO TABLE dummy VALUES (1);

DROP TABLE IF EXISTS struct_table_example;
CREATE TABLE struct_table_example (a int, s1 struct<f1: boolean, f2: string, f3: int, f4: int> ) STORED AS ORC;

INSERT INTO TABLE struct_table_example SELECT 1, named_struct('f1', false, 'f2', 'test', 'f3', 3, 'f4', 4)  FROM dummy;

select s1.f1, s1.f2, rank() over (partition by s1.f2 order by s1.f4) from struct_table_example;


This would throw the following error

Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":""test"",""reducesinkkey1"":4},""value"":{""_col1"":{""f1"":false,""f2"":""test"",""f3"":3,""f4"":4}}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:297)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:317)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":""test"",""reducesinkkey1"":4},""value"":{""_col1"":{""f1"":false,""f2"":""test"",""f3"":3,""f4"":4}}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:365)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:287)
	... 16 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct cannot be cast to org.apache.hadoop.io.IntWritable
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.getPrimitiveJavaObject(WritableIntObjectInspector.java:46)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:412)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.copyToStandardObject(GenericUDAFRank.java:219)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank$GenericUDAFAbstractRankEvaluator.iterate(GenericUDAFRank.java:154)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:192)
	at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.processRow(WindowingTableFunction.java:407)
	at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.processRow(PTFOperator.java:325)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.process(PTFOperator.java:139)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:356)
	... 17 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1546783872011_263870_1_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:196)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:79) (state=08S01,code=2)

 
 PTF with nested structure throws ClassCastException",yes
13256713,"colored textWhen the datanode keeping in DECOMMISSION_INPROGRESS status, the EC internal block in that datanode will be replicated many times.
// added 2019/09/19
I reproduced this scenario in a 163 nodes cluster with decommission 100 nodes simultaneously. 
 
   
 Erasure Coding: the internal block is replicated many times when datanode is decommissioning",13229532,"

// [WARN] [RedundancyMonitor] : Failed to place enough replicas, still in need of 2 to reach 167 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable: unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}


In a large-scale cluster, decommissioning large-scale datanodes cause EC block groups to replicate a large number of duplicate internal blocks. 
 Erasure Coding: decommissioning datanodes cause replicate a large number of duplicate EC internal blocks",yes
13161096,"S3Guard updates its table on a getFileStatus call, but not on a directory listing.
While this makes directory listings faster (no need to push out an update), it slows down subsequent queries of the files, such as a sequence of:


statuses = s3a.listFiles(dir)
for (status: statuses) {
  if (status.isFile) {
      try(is = s3a.open(status.getPath())) {
        ... do something
      }
}


this is because the open() is doing the getFileStatus check, even after the listing.
Updating the DDB tables after a listing would give those reads a speedup, albeit at the expense of initiating a (bulk) update in the list call. Of course, we could consider making that async, though that design (essentially a write-buffer) would require the buffer to be checked in the reads too.  
 S3Guard to self update on directory listings of S3",13311584,"The release tool doesn't url encode all characters provided for ASF_USERNAME, ASF_PASSWORD. 
 Create release should url-encode all characters when building git uri",No
13135006,"This Jira will track toModify existing placement constraints to support node attributes. 
 Modify placement constraints to support node attributes",13134919,"Currently, we have only two scopes defined: NODE and RACK against which we check the cardinality of the placement.
This idea should be extended to support node-attribute scopes. For eg: Placement of containers across upgrade domains and failure domains.  
 Support special Node Attribute scopes in addition to NODE and RACK",yes
13231303,"Using theIndexedFileFormat yarn.nodemanager.remote-app-log-dir configured to an s3a URI throws the following exception during log aggregation:

Cannot create writer for app application_1556199768861_0001. Skip log upload this time. 
java.io.IOException: java.io.FileNotFoundException: No such file or directory: s3a://adamantal-log-test/logs/systest/ifile/application_1556199768861_0001/adamantal-3.gce.cloudera.com_8041
	at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:247)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:306)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:464)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:420)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: No such file or directory: s3a://adamantal-log-test/logs/systest/ifile/application_1556199768861_0001/adamantal-3.gce.cloudera.com_8041
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2488)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2382)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2321)
	at org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)
	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1244)
	at org.apache.hadoop.fs.FileContext$15.next(FileContext.java:1240)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1246)
	at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:228)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:195)
	... 7 more


This stack trace point to LogAggregationIndexedFileController$initializeWriter where we do the following steps (in a non-rolling log aggregation setup):

create FSDataOutputStream
writing out a UUID
flushing
immediately after that we call a GetFileStatus to get the length of the log file (the bytes we just wrote out), and that's where the failures happens: the file is not there yet due to eventual consistency.

Maybe we can get rid of that, so we can use IFile format against a s3a target. 
 IFile format is not working against s3a remote folder",13143517,"YARN-1461 adds the concept ofApplication Tagsto Yarn applications. In particular, auser is able to annotate application with multiple tags to classify apps. We can leverage this todefineapplication namespaces based on application tags forplacement constraints.
Below is a typical use case.
There are a lot of TF jobs running on Yarn, and some of them are consuming resources heavily. So we want to limit number of PS on each node for such BIG players but ignore those SMALL ones. To achieve this, we can do following steps:

Addapplication tag ""big-tf"" tothese big TF jobs
For each PS request, we add ""ps"" source tag and map it to constraint ""notin, node, tensorflow/ps"" or ""cardinality, node, tensorflow/ps, 0, 2"" for finer grained controls.

 
 Support application tags when defining application namespaces for placement constraints",No
13222639,"We definitely need to capture any exception during CacheUpdateMasterWork.update(), otherwise, Java would refuse to schedule future update(). 
 Metastore cache update shall capture exception",13284036,"I've been studying unit tests of late. A few are failing but then the output is missing a detail or shutdown complains of NPE because startup didn't succeed.
Here are super minor items I've been carrying around that I'd like to land. They do  not change the function of tests (there is an attempt at a fix of TestLogsCleaner).

TestFullLogReconstruction log the server we've chosen to expire and then note where we starting counting rows
TestAsyncTableScanException use a define for row counts; count 100 instead of 1000 and see if helps
TestRawAsyncTableLimitedScanWithFilter check connection was made before closing it in tearDown
TestLogsCleaner use single mod time. Make it for sure less than now in case test runs all in the same millisecond (would cause test fail)
TestReplicationBase test table is non-null before closing in tearDown

 
 Add null checks and logging to misc set of tests",No
13328117,"In Java 11, there are a lot of false-positive findbugs warnings in try-with-resources.
Ref: https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java11-linux-x86_64/1/artifact/out/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html
This issue has been fixed by https://github.com/spotbugs/spotbugs/pull/1248 but now there are no releases that include this fix. 
 [JDK 11] Upgrade SpotBugs to 4.1.3 to fix false-positive warnings",13208469,"The latest version of Spotbugs is 3.1.10 and it can be available via spotbugs-maven-plugin. 
 Upgrade Spotbugs to the latest version",yes
13169640,"Hello!
distcp through 3.1.0 appears to copy files and then rename them into their final/destination filename. https://issues.apache.org/jira/browse/HADOOP-13786added support for more efficient S3 committers that do not use renames.
Please update distcp to use these efficient committers and no renames.
Thanks! 
 Update distcp to use zero-rename s3 committers",13142151,"Currently Distcp uploads a file by two strategies

append parts
copy to temp then rename

option 2 executes the following sequence in promoteTmpToTarget


    if ((fs.exists(target) && !fs.delete(target, false))
        || (!fs.exists(target.getParent()) && !fs.mkdirs(target.getParent()))
        || !fs.rename(tmpTarget, target)) {
      throw new IOException(""Failed to promote tmp-file:"" + tmpTarget
                              + "" to: "" + target);
    }


For any object store, that's a lot of HTTP requests; for S3A you are looking at 12+ requests and an O(data) copy call. 
This is not a good upload strategy for any store which manifests its output atomically at the end of the write().
Proposed: add a switch to write directly to the dest path, which can be supplied as either a conf option (distcp.direct.write = true) or a CLI option (-direct).
 
 Distcp to add no-rename copy option",yes
13198047,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13140158,"It takes too much time to start a MetaStore for every test instance.
To reduce the running time, start only 1 MetaStore instance, and run every test against this instance 
 Refactor tests, so only 1 MetaStore instance will be started per test class and test configuration",No
13254641,"Currently, 'hadoop checknative' command supports checking native libs, such as zlib, snappy, openssl and ISA-L etc. It's necessary to include pmdk lib in the checking. 
 Check native pmdk lib by 'hadoop checknative' command",13284779,"hdfs-2.10.0-webapps-secondary-status.html miss moment.js
 
 hdfs-2.10.0-webapps-secondary-status.html miss moment.js",No
13303572,"testSnappyCompressDecompress testSnappyCompressDecompressInMultiThreads reproducibly fails on CentOS 8. These tests has no issue on CentOS 7. 
 Fix failure of TestSnappyCompressorDecompressor on CentOS 8",13274977,"
org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor
org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit
org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads
org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress

These test will fail on X86 and ARM platform.
Trace back

org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor
org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit
12:00:33 [ERROR] TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit:92 Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got un
expected exception: java.lang.NullPointerException
 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:877)
 at com.google.common.base.Joiner.toString(Joiner.java:452)
 at com.google.common.base.Joiner.appendTo(Joiner.java:109)
 at com.google.common.base.Joiner.appendTo(Joiner.java:152)
 at com.google.common.base.Joiner.join(Joiner.java:195)
 at com.google.common.base.Joiner.join(Joiner.java:185)
 at com.google.common.base.Joiner.join(Joiner.java:211)
 at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$2.assertCompression(CompressDecompressTester.java:329)
 at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:135)
 at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit(TestCompressorDecompressor.java:89)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads
org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress
[ERROR] testSnappyCompressDecompress(org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor) Time elapsed: 0.003 s <<< ERROR!
java.lang.InternalError: Could not decompress data. Input is invalid.
 at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompressBytesDirect(Native Method)
 at org.apache.hadoop.io.compress.snappy.SnappyDecompressor.decompress(SnappyDecompressor.java:235)
 at org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor.testSnappyCompressDecompress(TestSnappyCompressorDecompressor.java:192)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
 at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
 at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
 at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
 at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
 at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
 at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
 at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
 at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
 at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
 at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
 at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
 at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
 at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
 at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

 
 SnappyCompressor test cases wrongly assume that the compressed data is always smaller than the input data",yes
13288866,"Currently when a table is dropped we are deriving if it was used by any materialized views from the error message got from the Metastore. Instead of this we should check it in advance, from theHiveMaterializedViewsRegistry. 
 Fix checking if a table is used by a materialized view before dropping",13271775,"If you try dropping a table which is part of the definition of a created materialized view, the table is not dropped, which is the desired state as it is part of the materialized view.
However, there was a ""drop"" call to the table, so it tried to drop it but did not succeed, leaving it in an inconsistent state.

Repro:
-------
1) Create tables:



CREATE TABLE emps (empid INT,deptno INT,name VARCHAR(256),salary FLOAT,hire_date TIMESTAMP)STORED AS ORC TBLPROPERTIES ('transactional'='true');

CREATE TABLE depts (deptno INT,deptname VARCHAR(256),locationid INT)STORED AS ORC TBLPROPERTIES ('transactional'='true');



2) Create the VM:



CREATE MATERIALIZED VIEW mv1 AS SELECT empid, deptname, hire_date FROM emps JOIN deptsON (emps.deptno = depts.deptno) WHERE hire_date >='2016-01-01';



3) Following is in backend database at this point:



mysql> select TBL_ID, DB_ID, SD_ID, TBL_NAME, TBL_TYPE from TBLS where DB_ID=16;
+--------+-------+-------+----------+-------------------+
| TBL_ID | DB_ID | SD_ID | TBL_NAME | TBL_TYPE     |
+--------+-------+-------+----------+-------------------+
|   81 |  16 |  81 | emps   | MANAGED_TABLE   |
|   83 |  16 |  83 | depts  | MANAGED_TABLE   |
|   84 |  16 |  84 | mv1   | MATERIALIZED_VIEW |
+--------+-------+-------+----------+-------------------+
3 rows in set (0.00 sec)



4) Let's drop the 'emps' table:



0: jdbc:hive2://c1122-node2.squadron.support.> drop table emps;
INFO : Compiling command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b): drop table emps
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO : Completed compiling command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b); Time taken: 0.05 seconds
INFO : Executing command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b): drop table emps
INFO : Starting task [Stage-0:DDL] in serial mode
INFO : Completed executing command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b); Time taken: 10.281 seconds
INFO : OK
No rows affected (16.949 seconds)


No issue displayed

5) List tables:



0: jdbc:hive2://c1122-node2.squadron.support.> show tables;
INFO  : Compiling command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413): show tables
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
INFO  : Completed compiling command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413); Time taken: 0.041 seconds
INFO  : Executing command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413): show tables
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413); Time taken: 0.016 seconds
INFO  : OK
+-----------+
| tab_name  |
+-----------+
| depts     |
| emps      |
+-----------+
2 rows selected (0.08 seconds)



6) Now, from the backend-db point of view:



mysql> select TBL_ID, DB_ID, SD_ID, TBL_NAME, TBL_TYPE from TBLS where DB_ID=16;
+--------+-------+-------+----------+-------------------+
| TBL_ID | DB_ID | SD_ID | TBL_NAME | TBL_TYPE     |
+--------+-------+-------+----------+-------------------+
|   81 |  16 | NULL | emps   | MANAGED_TABLE   |
|   83 |  16 |  83 | depts  | MANAGED_TABLE   |
|   84 |  16 |  84 | mv1   | MATERIALIZED_VIEW |
+--------+-------+-------+----------+-------------------+
3 rows in set (0.00 sec)


The table is left with NULL in SD_ID, making it not available.

7) From Metastore.log



2019-12-02T20:00:25,545 INFO [pool-6-thread-195]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(907)) - 196: source:172.25.34.150 drop_table : tbl=hive.mvs.emps
2019-12-02T20:00:25,545 INFO [pool-6-thread-195]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(349)) - ugi=hive	ip=172.25.34.150	cmd=source:172.25.34.150 drop_table : tbl=hive.mvs.emps	
2019-12-02T20:00:25,580 INFO [pool-6-thread-195]: metastore.ObjectStore$RetryingExecutor (ObjectStore.java:run(9966)) - Attempting to acquire the DB log notification lock: 0 out of 10 retries
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""NEXT_EVENT_ID"" from ""NOTIFICATION_SEQUENCE"" for update"".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:391) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:216) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.lambda$lockForUpdate$0(ObjectStore.java:9936) ~[hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:9963) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:9938) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:10002) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at com.sun.proxy.$Proxy28.addNotificationEvent(Unknown Source) [?:?]
	at org.apache.hive.hcatalog.listener.DbNotificationListener.process(DbNotificationListener.java:968) [hive-hcatalog-server-extensions-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropTable(DbNotificationListener.java:198) [hive-hcatalog-server-extensions-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier$19.notify(MetaStoreListenerNotifier.java:99) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:273) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:335) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:2670) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:2842) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at com.sun.proxy.$Proxy30.drop_table_with_environment_context(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:15533) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:15517) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_112]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_112]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) [hadoop-common-3.1.1.3.1.0.0-78.jar:?]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.sql.BatchUpdateException: Cannot delete or update a parent row: a foreign key constraint fails (""hive"".""MV_TABLES_USED"", CONSTRAINT ""MV_TABLES_USED_FK2"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS"" (""TBL_ID""))
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2058) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1471) ~[mysql-connector-java.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:125) ~[HikariCP-2.6.1.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-2.6.1.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:319) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:211) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:633) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 37 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Cannot delete or update a parent row: a foreign key constraint fails (""hive"".""MV_TABLES_USED"", CONSTRAINT ""MV_TABLES_USED_FK2"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS"" (""TBL_ID""))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_112]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_112]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_112]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_112]
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.Util.getInstance(Util.java:386) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1041) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4187) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4119) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2570) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2731) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2820) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2159) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2462) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2010) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1471) ~[mysql-connector-java.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:125) ~[HikariCP-2.6.1.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-2.6.1.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:319) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:211) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:633) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 37 more





8) If you try to query the table:



0: jdbc:hive2://c1122-node2.squadron.support.> select * from emps;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table emps. null (state=42000,code=40000)



It fails as expected.
9) If you try to query the MV:


0: jdbc:hive2://c1122-node2.squadron.support.> select * from mv1; INFO : Compiling command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2): select * from mv1 INFO : Semantic Analysis Completed (retrial = false) INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:mv1.empid, type:int, comment:null), FieldSchema(name:mv1.deptname, type:varchar(256), comment:null), FieldSchema(name:mv1.hire_date, type:timestamp, comment:null)], properties:null) INFO : Completed compiling command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2); Time taken: 0.229 seconds INFO : Executing command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2): select * from mv1 INFO : Completed executing command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2); Time taken: 0.01 seconds INFO : OK +------------+---------------+----------------+ | mv1.empid | mv1.deptname | mv1.hire_date | +------------+---------------+----------------+ +------------+---------------+----------------+ No rows selected (0.276 seconds)


It does not fail, as the underlying data has not changed, and the table is still being shown as valid.

10) Insert data into ""depts"" table and rebuild the mv.


$ INSERT INTO TABLE depts VALUES (101,'IT',25);
$ INSERT INTO TABLE depts VALUES (102,'Eng',11);

0: jdbc:hive2://c1122-node2.squadron.support.> ALTER MATERIALIZED VIEW mvs.mv1 REBUILD;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table emps. null (state=42000,code=40000)


This fails as expected.
 
 Drop table involved in materialized view leaves the table in inconsistent state",yes
13159403,"If the user goes to Applications -> app -> Resource Usage for a finished application, they get this message: ""No resource usage data is available for this application!"". 
I think it would be better to hide this tab for finished applications, or at least add something like ""this application is not using any resources because it is finished"" to the message. 
 [UI2] Improve ""Resource Usage"" tab error message when there are no data available.",13242132,"Loading an fsimage is basically a single threaded process. The current fsimage is written out in sections, eg iNode, iNode_Directory, Snapshots, Snapshot_Diff etc. Then at the end of the file, an index is written that contains the offset and length of each section. The image loader code uses this index to initialize an input stream to read and process each section. It is important that one section is fully loaded before another is started, as the next section depends on the results of the previous one.
What I would like to propose is the following:
1. When writing the image, we can optionally output sub_sections to the index. That way, a given section would effectively be split into several sections, eg:


   inode_section offset 10 length 1000
     inode_sub_section offset 10 length 500
     inode_sub_section offset 510 length 500
     
   inode_dir_section offset 1010 length 1000
     inode_dir_sub_section offset 1010 length 500
     inode_dir_sub_section offset 1010 length 500


Here you can see we still have the original section index, but then we also have sub-section entries that cover the entire section. Then a processor can either read the full section in serial, or read each sub-section in parallel.
2. In the Image Writer code, we should set a target number of sub-sections, and then based on the total inodes in memory, it will create that many sub-sections per major image section. I think the only sections worth doing this for are inode, inode_reference, inode_dir and snapshot_diff. All others tend to be fairly small in practice.
3. If there are under some threshold of inodes (eg 10M) then don't bother with the sub-sections as a serial load only takes a few seconds at that scale.
4. The image loading code can then have a switch to enable 'parallel loading' and a 'number of threads' where it uses the sub-sections, or if not enabled falls back to the existing logic to read the entire section in serial.
Working with a large image of 316M inodes and 35GB on disk, I have a proof of concept of this change working, allowing just inode and inode_dir to be loaded in parallel, but I believe inode_reference and snapshot_diff can be make parallel with the same technique.
Some benchmarks I have are as follows:


Threads   1     2     3     4 
--------------------------------
inodes    448   290   226   189 
inode_dir 326   211   170   161 
Total     927   651   535   488 (MD5 calculation about 100 seconds)


The above table shows the time in seconds to load the inode section and the inode_directory section, and then the total load time of the image.
With 4 threads using the above technique, we are able to better than half the load time of the two sections. With the patch in HDFS-13694 it would take a further 100 seconds off the run time, going from 927 seconds to 388, which is a significant improvement. Adding more threads beyond 4 has diminishing returns as there are some synchronized points in the loading code to protect the in memory structures. 
 Improve fsimage load time by writing sub-sections to the fsimage index",No
13201476,"In this case the delay between the ack from RS and the region report was 1.5s, so I'm not sure what caused the race (network hiccup? unreported retry by protobuf transport?) but in any case I don't see anything that prevents this from happening in a normal case, with a narrowed time window. Any delay (e.g. a GC pause on RS right after the report is built, and ack is sent for the close) or retries expands the window.
Master starts moving the region and the source RS acks by 21:51,206

Master:
2018-11-21 21:21:49,024 INFO  [master/6:17000.Chore.1] master.HMaster: balance hri=<region hash>, source=<RS 1>,17020,1542754626176, destination=<RS 2>,17020,1542863268158
...
Server:
2018-11-21 21:21:49,394 INFO  [RS_CLOSE_REGION-regionserver/<RS 1>:17020-1] handler.UnassignRegionHandler: Close <region hash>
...
2018-11-21 21:21:51,095 INFO  [RS_CLOSE_REGION-regionserver/<RS 1>:17020-1] handler.UnassignRegionHandler: Closed <region hash>


By then the region is removed from onlineRegions, so the master proceeds.

2018-11-21 21:21:51,206 INFO  [PEWorker-4] procedure2.ProcedureExecutor: Finished subprocedure(s) of pid=667, state=RUNNABLE:REGION_STATE_TRANSITION_CONFIRM_CLOSED, hasLock=true; TransitRegionStateProcedure table=<table>, region=<region hash>, REOPEN/MOVE; resume parent processing.
2018-11-21 21:21:51,386 INFO  [PEWorker-13] assignment.RegionStateStore: pid=667 updating hbase:meta row=<region hash>, regionState=OPENING, regionLocation=<RS 2>,17020,1542863268158


There are no obvious errors/delays that I see in RS log, and it doesn't log starting to send the report.
However, at 21:52.853 the report is processed that still contains this region.

2018-11-21 21:21:52,853 WARN  [RpcServer.default.FPBQ.Fifo.handler=48,queue=3,port=17000] assignment.AssignmentManager: Killing <RS 1>,17020,1542754626176: rit=OPENING, location=<RS 2>,17020,1542863268158, table=<table>, region=<region hash> reported OPEN on server=<RS 1>,17020,1542754626176 but state has otherwise.
***** ABORTING region server <RS 1>,17020,1542754626176: org.apache.hadoop.hbase.YouAreDeadException: rit=OPENING, location=<RS 2>,17020,1542863268158, table=<table>, region=<region hash> reported OPEN on server=<RS 1>,17020,1542754626176 but state has otherwise.


RS shuts down in an orderly manner and it can be seen from the log that this region is actually not present (there's no line indicating it's being closed, unlike for other regions).
I think there needs to be some sort of versioning for region operations and/or in RS reports to allow master to account for concurrent operations and avoid races. Probably per region with either a grace period or an additional global version, so that master could avoid killing RS based on stale reports, but still kill RS if it did retain an old version of the region state due to some bug after acking a new version. 
 race between region report and region move causes master to kill RS",13195639,"In the periodic regionServerReport from RS to master, we will call master.getAssignmentManager().reportOnlineRegions() to make sure the RS has a same state with Master. If RS holds a region which master think should be on another RS, the Master will kill the RS.
But, the regionServerReport could be lagging(due to network or something), which can't represent the current state of RegionServer. Besides, we will call reportRegionStateTransition and try forever until it successfully reported to master  when online a region. We can count on reportRegionStateTransition calls.
I have encountered cases that the regions are closed on the RS and  reportRegionStateTransition to master successfully. But later, a lagging regionServerReport tells the master the region is online on the RS(Which is not at the moment, this call may generated some time ago and delayed by network somehow), the the master think the region should be on another RS, and kill the RS, which should not be. 
 Do not kill RS if reportOnlineRegions fails",yes
13159537,"
Client Execution succeeded but contained differences (error code = 1) after executing tez_vector_dynpart_hashjoin_1.q 
407c407
< -13036 1
---
> -8915 1
410c410
< -8915 1
---
> -13036 1

 
 Flaky test: TestMiniLlapLocalCliDriver.tez_vector_dynpart_hashjoin_1",13159320,"When a split task is finished, master will delete the wal first, then remove the task's zk node. So if master crashed after delelte the wal, the zk task node may be leaved on zk. When master resubmit this task, the task will failed by FileNotFoundException.
We also handle FileNotFoundException inWALSplitter. But not handle this inSplitLogWorker.



  try {
    in = getReader(path, reporter);
  } catch (EOFException e) {
    if (length <= 0) {
      // TODO should we ignore an empty, not-last log file if skip.errors
      // is false? Either way, the caller should decide what to do. E.g.
      // ignore if this is the last log in sequence.
      // TODO is this scenario still possible if the log has been
      // recovered (i.e. closed)
      LOG.warn(""Could not open {} for reading. File is empty"", path, e);
    }
    // EOFException being ignored
    return null;
  }
} catch (IOException e) {
  if (e instanceof FileNotFoundException) {
    // A wal file may not exist anymore. Nothing can be recovered so move on
    LOG.warn(""File {} does not exist anymore"", path, e);
    return null;
  }
}



// Here fs.getFileStatus may throw FileNotFoundException, too. We should handle this exception as the WALSplitter.getReader.
try {
  if (!WALSplitter.splitLogFile(walDir, fs.getFileStatus(new Path(walDir, filename)),
    fs, conf, p, sequenceIdChecker,
      server.getCoordinatedStateManager().getSplitLogWorkerCoordination(), factory)) {
    return Status.PREEMPTED;
  }
} 



 
 Enable TestMiniLlapLocalCliDriver#tez_dynpart_hashjoin_1.q and TestMiniLlapLocalCliDriver#tez_vector_dynpart_hashjoin_1.q",yes
13281448,"TestSplitTransactionOnCluster fails 80% of time in the GCE run up on https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2/lastSuccessfulBuild/artifact/dashboard.html
We want to run a compaction to clear up references but logic doesn't handle case where compaction already running. 
 [Flakey Tests] TestSplitTransactionOnCluster",13278441,"testShutDownFixupWhenDaughterHasSplit is flaky.
I had a look at it, it fails because the compaction isn't ready when the assertion is called. The function HRegion.compact() has a comment that it can block for a long time, do not use it for a time-sensitive thread.

edit: also found a weird debug message, it can also be a problem:
compactions.SortedCompactionPolicy(231): Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
 
 fix flaky TestSplitTransactionOnCluster",yes
13323878,"Hi,
When using spark streaming with deltalake, I got the following exception occasionally, something like 1 out of 100. Thanks.


Caused by: java.io.FileNotFoundException: No such file or directory: s3a://[pathToFolder]/date=2020-07-29/part-00005-046af631-7198-422c-8cc8-8d3adfb4413e.c000.snappy.parquet
 at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2255)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2149)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2088)
 at org.apache.spark.sql.delta.files.DelayedCommitProtocol$$anonfun$8.apply(DelayedCommitProtocol.scala:141)
 at org.apache.spark.sql.delta.files.DelayedCommitProtocol$$anonfun$8.apply(DelayedCommitProtocol.scala:139)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
 at scala.collection.AbstractTraversable.map(Traversable.scala:104)
 at org.apache.spark.sql.delta.files.DelayedCommitProtocol.commitTask(DelayedCommitProtocol.scala:139)
 at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)
 at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)


----Environment---
hadoop = ""3.1.2""
 hadoop-aws = ""3.1.2""
spark = ""2.4.5""
spark-on-k8s-operator = ""v1beta2-1.1.2-2.4.5""

deployed into AWS EKS kubernates. Version information below:
Server Version: version.Info{Major:""1"", Minor:""16+"", GitVersion:""v1.16.8-eks-e16311"", GitCommit:""e163110a04dcb2f39c3325af96d019b4925419eb"", GitTreeState:""clean"", BuildDate:""2020-03-27T22:37:12Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""} 
 delta.io spark task commit encountering S3 cached 404/FileNotFoundException",13248907,"If S3Guard is encountering delayed consistency (FNFE from tombstone; failure to open file) then 

it only retries with the same times as everything else. We should make it differently configurable
when an FNFE is finally thrown, rename() treats it as being caused by the original source path missing, when in fact its something else. Proposed: somehow propagate the failure up differently, probably in the S3AFileSystem.copyFile() code
don't do HEAD checks when creating files
shell commands to avoid deleteOnExit calls as these also generate HEAD calls by way of exists() checks

eliminating the HEAD checks will stop 404s getting into the S3 load balancer/cache during file creation 
 Avoid/handle cached 404s during S3A file creation",yes
13261783,"Capacity Scheduler does not support two percentage values for capacity and maximum-capacity settings. So, you can't do something like this:
yarn.scheduler.capacity.root.users.john.maximum-capacity=memory-mb=50.0%, vcores=50.0%
It's possible to use absolute resources, but not two separate percentages (which expresses capacity as a percentage of the overall cluster resource). Such a configuration is accepted in Fair Scheduler. 
 Capacity scheduler: enhance capacity / maximum-capacity setting",13286520,"In CS, ManagedParent Queue and its template cannot take absolute resource value like 
[memory=8192,vcores=8]
 Thsi Jira is to track and improve the configuration reading module of DynamicQueue to support absolute resource values.
 
 CS Dynamic Queues cannot be configured with absolute resources",yes
13176303,"There is a typo in the existing YarnConfiguration which uses theDEFAULT_NM_LOCALIZER_PORT as the default for NM Collector Service port. This Jira aims to fix the typo. 
 Fix NM Collector Service Port issue in YarnConfiguration",13283135,"I see this test fail a lot in my environments. It also uses such a large array that it seems particularly memory wasteful and difficult to get good contention in the test as well. 
 TestSyncTimeRangeTracker fails quite easily and allocates a very expensive array.",No
13246979,"RITs will lead toCatalogJanitor not running, and then parent region will not be cleaned.
As a result, the last assertion of checking if parent region is null will fail.

Error Message
expected:<null> but was:<maven-jdk8-551kz,45127,1563877982046>
Stacktrace
java.lang.AssertionError: expected:<null> but was:<maven-jdk8-551kz,45127,1563877982046> at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.testMasterRestartAtRegionSplitPendingCatalogJanitor(TestSplitTransactionOnCluster.java:577)
Currently, the caseis only waiting for master be active while not care about RITs completion when master starts up.
Failed test results is attached. We can seeCatalogJanitor chore run failed log in it,
2019-07-23 18:34:03,241 INFO [Time-limited test] regionserver.TestSplitTransactionOnCluster(572): Starting run of CatalogJanitor 2019-07-23 18:34:03,241 WARN [Time-limited test] master.CatalogJanitor(119): CatalogJanitor is disabled! Enabled=true, maintenanceMode=false, am=org.apache.hadoop.hbase.master.assignment.AssignmentManager@1ce25bee, metaLoaded=true, hasRIT=true clusterShutDown=false

 
 TestSplitTransactionOnCluster.testMasterRestartAtRegionSplitPendingCatalogJanitor is flakey",13235639,"It is time consuming. 
 Split TestSCP",No
13261032,"Tried to use the hbase-thirdparty lib. Ran into this issue https://github.com/eclipse/jetty.project/issues/3244 when I tried to update core to use the new hbase-thirdparty (A single failed unit test, TestProtobufUtil, complained of missing ByteBuffer position method when buffer was offheap). Pushed a recompiled artifact, one that uses the 3.1.0 tag., built with jdk8 and that works if local repo but can't override published artifaict.  Let me make a new one.
Need an issue else the RC scripts fail. Let this be it. Resovling against hbase-thirdparty-3.1.1. 
 Squash hbase-thirdparty-3.1.0; was compiled w/ jdk10 so ""NoSuchMethodError: java.nio.ByteBuffer.*""",13315510,"RetryInvocationHandler logs an INFO level message on every failover except the first. This used to be ideal before when there were only 2 proxies in the FailoverProxyProvider. But if there are more than 2 proxies (as is possible with 3 or more NNs in HA), then there could be more than one failover to find the currently active proxy.
To avoid creating noise in clients logs/ console, RetryInvocationHandler should skip logging once for each proxy. 
 Skip Retry INFO logging on first failover from a proxy",No
13215034,"If you run WASB in secure mode, it doesn't set connectingUsingSAS to true, which can break things 
 WASB in secure mode does not set connectingUsingSAS",13289370,"When load FSImage, it will sort sections in FileSummary and load Section's in SectionName enum sequence. But the sort method is wrong , when I use branch-2.6.0 to load fsimage write by branch-2 with patchhttps://issues.apache.org/jira/browse/HDFS-14771, it will throw NPE because it load INODE first 


2020-03-03 14:33:26,618 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.lang.NullPointerException
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadPermission(FSImageFormatPBINode.java:101)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectory(FSImageFormatPBINode.java:148)
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadRootINode(FSImageFormatPBINode.java:332)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeSection(FSImageFormatPBINode.java:218)
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:254)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:180)
    at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:226)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1036)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1020)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:741)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:677)
    at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:290)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1092)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:780)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:609)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:666)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:838)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:817)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1538)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1606)



I print the load  order:


2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE,  offset = 37, length = 11790829 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 37, length = 826591 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 826628, length = 828192 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 1654820, length = 835240 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 2490060, length = 833630 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 3323690, length = 909445 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 4233135, length = 866147 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 5099282, length = 1272751 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 6372033, length = 1311876 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 7683909, length = 1251510 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 8935419, length = 1296120 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 10231539, length = 770082 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_SUB,  offset = 11001621, length = 789245 ]
2020-03-03 15:49:36,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 11790866, length = 67038 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 11857904, length = 84692 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 11942596, length = 71759 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = NS_INFO,  offset = 8, length = 29 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = STRING_TABLE,  offset = 12567596, length = 440 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_REFERENCE,  offset = 12566380, length = 0 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SNAPSHOT,  offset = 12566191, length = 83 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR,  offset = 11790866, length = 774068 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = FILES_UNDERCONSTRUCTION,  offset = 12564934, length = 1257 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SNAPSHOT_DIFF,  offset = 12566274, length = 106 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SECRET_MANAGER,  offset = 12566380, length = 1209 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = CACHE_MANAGER,  offset = 12567589, length = 7 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12014355, length = 84629 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12098984, length = 65215 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12164199, length = 64496 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12228695, length = 68122 ]
2020-03-03 15:49:36,425 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12296817, length = 53417 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12350234, length = 51455 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12401689, length = 80305 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = INODE_DIR_SUB,  offset = 12481994, length = 82940 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: [name = SNAPSHOT_DIFF_SUB,  offset = 12566274, length = 106 ]
2020-03-03 15:49:36,426 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Current loadin


The order is wrong 
 Release 2.2.4",No
13184516,"The jenkins build was failing on hadoop branch-2.8 for quite a while, see an example result in YARN-8664. The error

 Step 24/31 : RUN pip install pylint
 ---> Running in b1f0e6c1d209
 Downloading/unpacking pylint
 ...
 setuptools_scm.version.SetuptoolsOutdatedWarning: your setuptools is too old (<12)
 Complete output from command python setup.py egg_info:
 /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'



 detail output can be seenhere. 
 Hadoop: branch-2.8 docker image failed to build",13175918,"HADOOP-15610 fixes the pip install failures on branch 3.x releases, but it is still failing on branch-2, though with a slightly different error message


Downloading/unpacking pylint
 Running setup.py (path:/tmp/pip_build_root/pylint/setup.py) egg_info for package pylint
 /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'
 warnings.warn(msg)
 Traceback (most recent call last):
 File ""<string>"", line 17, in <module>
 File ""/tmp/pip_build_root/pylint/setup.py"", line 177, in <module>
 install()
 File ""/tmp/pip_build_root/pylint/setup.py"", line 174, in install
 **kwargs)
 File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup
 _setup_distribution = dist = klass(attrs)
 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 239, in __init__
 self.fetch_build_eggs(attrs.pop('setup_requires'))
 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 264, in fetch_build_eggs
 replace_conflicting=True
 File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 620, in resolve
 dist = best[req.key] = env.best_match(req, ws, installer)
 File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 858, in best_match
 return self.obtain(req, installer) # try and download/install
 File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 870, in obtain
 return installer(requirement)
 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 314, in fetch_build_egg
 return cmd.easy_install(req)
 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 616, in easy_install
 return self.install_item(spec, dist.location, tmpdir, deps)
 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 646, in install_item
 dists = self.install_eggs(spec, download, tmpdir)
 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 834, in install_eggs
 return self.build_and_install(setup_script, setup_base)
 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1040, in build_and_install
 self.run_setup(setup_script, setup_base, args)
 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1025, in run_setup
 run_setup(setup_script, args)
 File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 50, in run_setup
 lambda: execfile(
 File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 100, in run
 return func()
 File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 52, in <lambda>
 {'__file__':setup_script, '__name__':'__main__'}
 File ""setup.py"", line 76, in <module>

 File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup
 _setup_distribution = dist = klass(attrs)
 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 243, in __init__
 _Distribution.__init__(self,attrs)
 File ""/usr/lib/python2.7/distutils/dist.py"", line 287, in __init__
 self.finalize_options()
 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 277, in finalize_options
 ep.load()(self, ep.name, value)
 File ""build/bdist.linux-x86_64/egg/setuptools_scm/integration.py"", line 10, in version_keyword
 File ""build/bdist.linux-x86_64/egg/setuptools_scm/version.py"", line 66, in _warn_if_setuptools_outdated
 setuptools_scm.version.SetuptoolsOutdatedWarning: your setuptools is too old (<12)
 Complete output from command python setup.py egg_info:
 /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'

 warnings.warn(msg)

Traceback (most recent call last):

 File ""<string>"", line 17, in <module>

 File ""/tmp/pip_build_root/pylint/setup.py"", line 177, in <module>

 install()

 File ""/tmp/pip_build_root/pylint/setup.py"", line 174, in install

 **kwargs)

 File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup

 _setup_distribution = dist = klass(attrs)

 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 239, in __init__

 self.fetch_build_eggs(attrs.pop('setup_requires'))

 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 264, in fetch_build_eggs

 replace_conflicting=True
 File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 620, in resolve

 dist = best[req.key] = env.best_match(req, ws, installer)

 File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 858, in best_match

 return self.obtain(req, installer) # try and download/install

 File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 870, in obtain

 return installer(requirement)

 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 314, in fetch_build_egg

 return cmd.easy_install(req)

 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 616, in easy_install

 return self.install_item(spec, dist.location, tmpdir, deps)

 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 646, in install_item

 dists = self.install_eggs(spec, download, tmpdir)

 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 834, in install_eggs

 return self.build_and_install(setup_script, setup_base)

 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1040, in build_and_install

 self.run_setup(setup_script, setup_base, args)

 File ""/usr/lib/python2.7/dist-packages/setuptools/command/easy_install.py"", line 1025, in run_setup

 run_setup(setup_script, args)

 File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 50, in run_setup

 lambda: execfile(

 File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 100, in run

 return func()

 File ""/usr/lib/python2.7/dist-packages/setuptools/sandbox.py"", line 52, in <lambda>

 {'__file__':setup_script, '__name__':'__main__'}

 File ""setup.py"", line 76, in <module>



 File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup

 _setup_distribution = dist = klass(attrs)

 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 243, in __init__

 _Distribution.__init__(self,attrs)

 File ""/usr/lib/python2.7/distutils/dist.py"", line 287, in __init__

 self.finalize_options()

 File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 277, in finalize_options

 ep.load()(self, ep.name, value)

 File ""build/bdist.linux-x86_64/egg/setuptools_scm/integration.py"", line 10, in version_keyword

 File ""build/bdist.linux-x86_64/egg/setuptools_scm/version.py"", line 66, in _warn_if_setuptools_outdated

setuptools_scm.version.SetuptoolsOutdatedWarning: your setuptools is too old (<12)

----------------------------------------
Cleaning up...
Command python setup.py egg_info failed with error code 1 in /tmp/pip_build_root/pylint
Storing debug log for failure in /root/.pip/pip.log
The command '/bin/sh -c pip install pylint' returned a non-zero code: 1l
 
 Hadoop Docker Image Pip Install Fails on branch-2",yes
13184385,"

mvn test -Dtest=TestHDFSContractAppend#testAppendDirectory,TestRouterWebHDFSContractAppend#testAppendDirectory

In case ofTestHDFSContractAppend the test excepts FileAlreadyExistsException but HDFS sends the exception wrapped into a RemoteException.
In case of TestRouterWebHDFSContractAppend the append does not even throw exception.
Steve Loughran, Thomas Marqardt, any thoughts? 
 AbstractContractAppendTest fails against HDFS on HADOOP-15407 branch",13186693,"hbase backup create incremental hdfs:///bkpHbase_Test/bkpHbase_Test2 -t bkpHbase_Test2 
2018-09-21 15:35:31,421 INFO [main] impl.TableBackupClient: Backup backup_1537524313995 started at 1537524331419. 2018-09-21 15:35:31,454 INFO [main] impl.IncrementalBackupManager: Execute roll log procedure for incremental backup ... 2018-09-21 15:35:32,985 ERROR [main] impl.TableBackupClient: Unexpected Exception : java.lang.NullPointerException java.lang.NullPointerException at org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.getLogFilesForNewBackup(IncrementalBackupManager.java:309) at org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.getIncrBackupLogFileMap(IncrementalBackupManager.java:103) at org.apache.hadoop.hbase.backup.impl.IncrementalTableBackupClient.execute(IncrementalTableBackupClient.java:276) at org.apache.hadoop.hbase.backup.impl.BackupAdminImpl.backupTables(BackupAdminImpl.java:601) at org.apache.hadoop.hbase.backup.impl.BackupCommands$CreateCommand.execute(BackupCommands.java:347) at org.apache.hadoop.hbase.backup.BackupDriver.parseAndRun(BackupDriver.java:138) at org.apache.hadoop.hbase.backup.BackupDriver.doWork(BackupDriver.java:171) at org.apache.hadoop.hbase.backup.BackupDriver.run(BackupDriver.java:204) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.hadoop.hbase.backup.BackupDriver.main(BackupDriver.java:179) 2018-09-21 15:35:32,989 ERROR [main] impl.TableBackupClient: BackupId=backup_1537524313995,startts=1537524331419,failedts=1537524332989,failedphase=PREPARE_INCREMENTAL,failedmessage=null 2018-09-21 15:35:57,167 ERROR [main] impl.TableBackupClient: Backup backup_1537524313995 failed. 
Backup session finished. Status: FAILURE 2018-09-21 15:35:57,175 ERROR [main] backup.BackupDriver: Error running 
command-line tool java.io.IOException: java.lang.NullPointerException at org.apache.hadoop.hbase.backup.impl.IncrementalTableBackupClient.execute(IncrementalTableBackupClient.java:281) at org.apache.hadoop.hbase.backup.impl.BackupAdminImpl.backupTables(BackupAdminImpl.java:601) at org.apache.hadoop.hbase.backup.impl.BackupCommands$CreateCommand.execute(BackupCommands.java:347) at org.apache.hadoop.hbase.backup.BackupDriver.parseAndRun(BackupDriver.java:138) at org.apache.hadoop.hbase.backup.BackupDriver.doWork(BackupDriver.java:171) at org.apache.hadoop.hbase.backup.BackupDriver.run(BackupDriver.java:204) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.hadoop.hbase.backup.BackupDriver.main(BackupDriver.java:179) Caused by: java.lang.NullPointerException at org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.getLogFilesForNewBackup(IncrementalBackupManager.java:309) at org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.getIncrBackupLogFileMap(IncrementalBackupManager.java:103) at org.apache.hadoop.hbase.backup.impl.IncrementalTableBackupClient.execute(IncrementalTableBackupClient.java:276) ... 7 more 
 Hbase incremental backup fails with null pointer exception",No
13179687,"I see the following NPE in the region server log for a table that is taking heavy writes. 
I am not sure how the memStoreScanners variable gets set to null.


2018-08-17 19:59:23,682 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem - Committing store file ...
2018-08-17 19:59:23,684 INFO  [MemStoreFlusher.1] regionserver.HStore - Added hdfs://...., entries=919170, sequenceid=275114, filesize=22.6 M
2018-08-17 19:59:23,689 FATAL [MemStoreFlusher.1] regionserver.HRegionServer - ABORTING region server iotperf1dchbase1a-dnds22-2-prd.eng.sfdc.net,60020,1533915690501: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: ......
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2581)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2258)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2220)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2106)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:2031)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:508)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:478)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:76)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:264)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:827)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:1160)
        at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1133)
        at org.apache.hadoop.hbase.regionserver.HStore.access$900(HStore.java:120)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2487)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2536)
        ... 9 more
2018-08-17 19:59:23,692 FATAL [MemStoreFlusher.1] regionserver.HRegionServer - RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.AccessController, org.apache.phoenix.coprocessor.ScanRegionObserver, org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver, org.apache.phoenix.hbase.index.Indexer, org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver, org.apache.hadoop.hbase.security.token.TokenProvider, org.apache.phoenix.coprocessor.ServerCachingEndpointImpl]

 
 NPE in StoreScanner.updateReaders causes RS to crash ",13243811,"

2019-07-09 08:02:14,262 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server hostname,16020,1562233574704: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: namespace:table,963,1562296120996.b8e2f19748d374d192b93f106a0f73b3.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2646)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2322)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2284)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2170)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:2095)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:508)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:478)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:76)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:264)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:863)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:1172)
        at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1145)
        at org.apache.hadoop.hbase.regionserver.HStore.access$900(HStore.java:122)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2505)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2600)
        ... 9 more

 
 [Flush] NPE when region flushs",yes
13214966,"If one block gets removed from the block group then the datanode information for the block group comes shows null.


Block Id: blk_-9223372036854775792
Block belongs to: /ec/file1
No. of Expected Replica: 2
No. of live Replica: 2
No. of excess Replica: 0
No. of stale Replica: 0
No. of decommissioned Replica: 0
No. of decommissioning Replica: 0
No. of corrupted Replica: 0
null

Fsck on blockId 'blk_-9223372036854775792

 
 EC : Fsck -blockId shows null for EC Blocks if One Block Is Not Available.",13268026,"EC file blockId location info displaying as ""null"" with hdfs fsck -blockId command

Check the blockId information of an EC enabled file with ""hdfs fsck -blockId"" Check the blockId information of an EC enabled file with ""hdfs fsck -blockId"" blockId location related info will display as null,which needs to be rectified. 

      Check the attachment ""EC_file_block_info""
=======================================================



Check the output of a normal file block to compare
             

===========================================================
       

Actual Output :-  null 
Expected output :- It should display the blockId location related info as (nodes, racks) of the block as specified in the usage info of fsck -blockId option.                  [like : Block replica on datanode/rack: BLR10000xx038/default-rack is HEALTHY]

 
 EC: EC file blockId location info displaying as ""null"" with hdfs fsck -blockId command",yes
13136348,"DistCP issues a delete(file) request even if is underneath an already deleted directory. This generates needless load on filesystems/object stores, and, if the store throttles delete, can dramatically slow down the delete operation.
If the distcp delete operation can build a history of deleted directories, then it will know when it does not need to issue those deletes.
Care is needed here to make sure that whatever structure is created does not overload the heap of the process. 
 DistCp to eliminate needless deletion of files under already-deleted directories",13136346,"There are opportunities to improve distcp delete performance and scalability with object stores, but you need to test with production datasets to determine if the optimizations work, don't run out of memory, etc.
By adding the option to save the sequence files of source, dest listings, people (myself included) can experiment with different strategies before trying to commit one which doesn't scale 
 DistCp to offer -xtrack <path> option to save src/dest filesets as alternative to delete()",yes
13183916,"TestHFileArchiving#testCleaningRace creates HFileCleaner instance within the test.
When SnapshotHFileCleaner.init() is called, there is no master parameter passed in params.
When the chore runs the cleaner during the test, NPE comes out of this line in getDeletableFiles():


      return cache.getUnreferencedFiles(files, master.getSnapshotManager());


since master is null.
We should either check for the null master or, pass master instance properly when constructing the cleaner instance. 
 Partially initialized SnapshotHFileCleaner leads to NPE during TestHFileArchiving",13196411,"
2018-11-06,12:55:25,980 WARN [RpcServer.default.FPBQ.Fifo.handler=251,queue=11,port=17100] org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure: Refresh peer TestPeer for TRANSIT_SYNC_REPLICATION_STATE on c4-hadoop-tst-st54.bj,17200,1541479922465 failed
java.lang.NullPointerException via c4-hadoop-tst-st54.bj,17200,1541479922465:java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.procedure2.RemoteProcedureException.fromProto(RemoteProcedureException.java:124)
	at org.apache.hadoop.hbase.master.MasterRpcServices.lambda$reportProcedureDone$4(MasterRpcServices.java:2303)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
	at org.apache.hadoop.hbase.master.MasterRpcServices.reportProcedureDone(MasterRpcServices.java:2298)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:13149)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:338)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:318)
Caused by: java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.wal.SyncReplicationWALProvider.peerSyncReplicationStateChange(SyncReplicationWALProvider.java:303)
	at org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandlerImpl.transitSyncReplicationPeerState(PeerProcedureHandlerImpl.java:216)
	at org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.call(RefreshPeerCallable.java:74)
	at org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.call(RefreshPeerCallable.java:34)
	at org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler.process(RSProcedureHandler.java:47)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

 
 NPE if RS restarts between REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN and TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE",No
13282077,"The trace doesn't work at all in HBase 2.0 and above after HBASE-18601 (the trace doesn't get picked up at the server side). We should make a note in the user doc stating it is

unusable
deprecated in HBase 2.x because HTrace is in Attic.
removed from HBase 3.0 and above.

 
 [OpenTracing] Migrate from HTrace to OpenTracing (Java code)",13137499,"Currently, if c-e is unable to obtain the PID for a short lived Docker container, the exitcode will not be properly obtained via docker inspect. This results in containers successfully completing when they should fail. 
 Fix exit code handling for short lived Docker containers",No
13203076,"Migrate the submarine installation script document from the hadoop-yarn project. 
 [Submarine] Update submarine installation script document",13193654,"From parent issue, Allan Yang suggests that we not give up on assign unless there a change – an SCP triggers failure – or at the extreme, an operator intervenes. This jibes w/ how we're thinking about assign (or to put it another way, we have no handling for the case where we exhaust retries). 
 Set hbase.assignment.maximum.attempts to Long.MAX",No
13206780,"The current version of bucket pruning skips all the predicates when it detects that one of the predicates is a compound type (e.g. NOT(IS_NULL) ) when evaluating AND logical operators.
This logic is faulty since as long as one of the AND operators is a bucketed column (col = literal), the literal value of that col should be considered in thebucket pruning optimization no matter what. For example:
SELECT * FROM tbl WHERE bucketed_col = 1 AND (some_compound_expr)
Then the the value'1'should be considered for pruning in the query plan. This limitation has manifested into a simpler case where a table that I am trying to optimized using bucketing technique is not effective when IS NOT NULL is used. Since IS NOT NULL is parsed into NOT(IS_NULL) (a compound expression), the pruning phase is completed skipped causing unnecessary tasks to be spawned. For instance:
SELECT * FROM tbl WHERE bucketed_col = 1 AND some_other_col IS NOT NULL
Will not trigger bucket pruning logic and perform a full table scan. 
 Hive bucketed table query pruning does not work for IS NOT NULL condition",13149726,"tpcds#74 is optimized in a way that for date_dim the condition contains IN and = for the same column


|             Map Operator Tree:                     |
|                 TableScan                          |
|                   alias: date_dim                  |
|                   filterExpr: (((d_year) IN (2001, 2002) and (d_year = 2002) and d_date_sk is not null) or ((d_year) IN (2001, 2002) and (d_year = 2001) and d_date_sk is not null)) (type: boolean) |
|                   Statistics: Num rows: 73049 Data size: 876588 Basic stats: COMPLETE Column stats: COMPLETE |
|                   Filter Operator                  |
|                     predicate: ((d_year) IN (2001, 2002) and (d_year = 2002) and d_date_sk is not null) (type: boolean) |
|                     Statistics: Num rows: 4 Data size: 48 Basic stats: COMPLETE Column stats: COMPLETE |


the ""real"" row count will be 365
for separate IN and = the estimation is very good; but if both are present it becomes (very) underestimated.


set hive.query.results.cache.enabled=false;

drop table if exists t1;
drop table if exists t8;

create table t1 (a integer,b integer);
create table t8 like t1;

insert into t1 values (1,1),(2,2),(3,3),(4,4),(5,5);

insert into t8
select * from t1 union all select * from t1 union all select * from t1 union all select * from t1 union all
select * from t1 union all select * from t1 union all select * from t1 union all select * from t1
;

analyze table t1 compute statistics for columns;
analyze table t8 compute statistics for columns;

explain analyze select sum(a) from t8 where b in (2,3) group by b;
explain analyze select sum(a) from t8 where b=2 group by b;

explain analyze select sum(a) from t1 where b in (2,3) and b=2 group by b;
explain analyze select sum(a) from t8 where b in (2,3) and b=2 group by b;


 
 related equals and in operators may cause inaccurate stats estimations",yes
13155200,"Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches. 
 Arrow batch serializer",13158584,"""You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."" 
 Arrow SerDe itest failure",yes
13290343,"now can not achieve overallbalanced when use rsgroup balancer and by table is on,
because balance every tableactually use the clusterload only contain one table's load.
we should use clusterload contain all this rsgroup table's load to balance overall

hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java


  public boolean balance(boolean force) throws IOException {
    ......
    boolean isByTable = getConfiguration().getBoolean(""hbase.master.loadbalance.bytable"", false);
    Map<TableName, Map<ServerName, List<RegionInfo>>> assignments =
      this.assignmentManager.getRegionStates()
        .getAssignmentsForBalancer(tableStateManager, this.serverManager.getOnlineServersList(),
          isByTable);
    for (Map<ServerName, List<RegionInfo>> serverMap : assignments.values()) {
      serverMap.keySet().removeAll(this.serverManager.getDrainingServersList());
    }

//Give the balancer the current cluster state.
    this.balancer.setClusterMetrics(getClusterMetricsWithoutCoprocessor());
    this.balancer.setClusterLoad(assignments);

    List<RegionPlan> plans = new ArrayList<>();
    for (Entry<TableName, Map<ServerName, List<RegionInfo>>> e : assignments.entrySet()) {
      List<RegionPlan> partialPlans = this.balancer.balanceCluster(e.getKey(), e.getValue());
      if (partialPlans != null) {
        plans.addAll(partialPlans);
      }
    }


now do refactor:

add method 'balanceTable' in interfaceLoadBalancer
SimpleLoadBalancer andStochasticLoadBalancer do the real 'balanceTable' , and'balanceTable' is not support inBaseLoadBalancer andRSGroupBasedLoadBalancer
RSGroupBasedLoadBalancer invokebalanceCluster , and pass GroupClusterLoad to internal balacer by group
internal balancer balance cluster invoke 'balanceTable'

 
 refactor  loadBalancer implements for rsgroup balance by table to  achieve overallbalanced",13330185,"The CommonJoinTaskDispatcher#mergeMapJoinTaskIntoItsChildMapRedTask method throws a SemanticException if the number of {{FileSinkOperator}}s it finds is not exactly 1. The exception is valid if zero operators are found, but there can be valid use cases where multiple FileSinkOperators exist.
Example: the MapJoin and it child are used in a common table expression, which is used for multiple inserts. 
 Support changing region replica count without disabling table",No
13129744,"The cause: 


2018-01-10 11:40:44,820 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=42971] ipc.CallRunner(141): callId: 27 service: MasterService methodName: GetReplicationPeerConfig size: 66 connection: 127.0.0.1:32800 deadline: 1515555704819
java.io.IOException
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:463)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.replication.ReplicationPeerConfigUtil.convert(ReplicationPeerConfigUtil.java:324)
	at org.apache.hadoop.hbase.master.MasterRpcServices.getReplicationPeerConfig(MasterRpcServices.java:1941)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:404)
	... 3 more
2018-01-10 11:40:44,826 DEBUG [ProcExecWrkr-15] client.RpcRetryingCallerImpl(132): Call exception, tries=13, retries=13, started=88676 ms ago, cancelled=false, msg=java.io.IOException
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:463)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.replication.ReplicationPeerConfigUtil.convert(ReplicationPeerConfigUtil.java:324)
	at org.apache.hadoop.hbase.master.MasterRpcServices.getReplicationPeerConfig(MasterRpcServices.java:1941)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:404)
	... 3 more
, details=, exception=java.io.IOException: java.io.IOException
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:463)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.replication.ReplicationPeerConfigUtil.convert(ReplicationPeerConfigUtil.java:324)
	at org.apache.hadoop.hbase.master.MasterRpcServices.getReplicationPeerConfig(MasterRpcServices.java:1941)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:404)
	... 3 more

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:93)
	at org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:83)
	at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:363)
	at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:351)
	at org.apache.hadoop.hbase.client.MasterCallable.call(MasterCallable.java:102)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:107)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3054)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3046)
	at org.apache.hadoop.hbase.client.HBaseAdmin.getReplicationPeerConfig(HBaseAdmin.java:3952)
	at org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.setupRegionReplicaReplication(ServerRegionReplicaUtil.java:153)
	at org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.addTableToMeta(CreateTableProcedure.java:351)
	at org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.executeFromState(CreateTableProcedure.java:103)
	at org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.executeFromState(CreateTableProcedure.java:51)
	at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:182)
	at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:845)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1456)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1225)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$800(ProcedureExecutor.java:78)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1735)

 
 TestRegionReplicaFailover and TestRegionReplicaReplicationEndpoint UT hangs",13183769,"
[ERROR] Failures:
[ERROR]   TestMiniLlapLocalCliDriver.testCliDriver:59 Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" column1, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" column2, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" NOT NULL column3) NOT NULL
expression type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" NOT NULL column1, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" NOT NULL column2, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" COLLATE ""ISO-8859-1$en_US$primary"" NOT NULL column3) NOT NULL
set is rel#260:HiveFilter.HIVE.[](input=HepRelVertex#251,condition=<($2, _UTF-16LE'100'))
expression is HiveFilter#262


q file contains examples that may to reproduce failure. 
 HiveFilterSetOpTransposeRule may throw assertion error due to nullability of fields",No
13198048,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
   this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
   if (completedChecks.containsKey(target))
{ 

   // here may be happen garbage collection，and result may be null.

   final LastCheckResult<V> result = completedChecks.get(target);     

   final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;  

   。。。。

}

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.
the solution is：
this.completedChecks = new ReferenceMap(1, 1);
or
 this.completedChecks = new HashMap<>();
 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13234493,"ThrottledAsyncChecker throws NPE during block pool initialization. The error leads the block pool registration failure.
The exception

2019-05-20 01:02:36,003 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool <registering> (Datanode Uuid xxxxx) service to xx.xx.xx.xx/xx.xx.xx.xx
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult.access$000(ThrottledAsyncChecker.java:211)
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:129)
        at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:209)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3387)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1508)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:319)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:272)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:768)
        at java.lang.Thread.run(Thread.java:745)


Looks like this error due to WeakHashMap type map completedChecks has removed the target entry while we still get that entry. Although we have done a check before we get it, there is still a chance the entry is got as null. 
We met a corner case for this: A federation mode, two block pools in DN, ThrottledAsyncChecker schedules two same health checks for same volume.

2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current
2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current


completedChecks cleans up the entry for one successful check after called completedChecks#get. However, after this, another check we get the null. 
 ThrottledAsyncChecker throws NPE during block pool initialization ",yes
13212082,"The project currently depends on libthrift-0.9.3, however thrift released 0.12.0 on 2019-JAN-04. This release includes a security fix for THRIFT-4506 (CVE-2018-1320). Updating thrift to the latest version will remove that vulnerability.
Also note the Apache Thrift project does not publish ""libfb303"" any longer. fb303 is contributed code (in '/contrib') and it has not been maintained.

Ps.: 0.9.3.1 also addresses the CVE, see THRIFT-4506 
 Upgrade Apache Thrift to 0.9.3-1",13213657,"Use 0.12.0 libthrift version in Hive. 
 Use 0.12.0 libthrift version in Hive",yes
13162305,"Currently only the count of regions in transition is exposed thru JMX.
Here is a sample snippet of the /jmx output:


{
  ""beans"" : [ {
...
  }, {
    ""name"" : ""Hadoop:service=HBase,name=Master,sub=AssignmentManager"",
    ""modelerType"" : ""Master,sub=AssignmentManager"",
    ""tag.Context"" : ""master"",
...
    ""ritCount"" : 3


It would be desirable to expose region name, state for the regions in transition as well.
We can place configurable upper bound on the number of entries returned in case there're a lot of regions in transition. 
 Expose regions in transition thru JMX",13232626,"LogsCLI guessAppOwner ignores custom file format suffix yarn.log-aggregation.%s.remote-app-log-dir-suffix / Default IndexedFileController Suffix (
{yarn.nodemanager.remote-app-log-dir-suffix}
-ifile or logs-ifile). It considers only yarn.nodemanager.remote-app-log-dir-suffix or default logs.
Repro:



yarn-site.xml

yarn.log-aggregation.file-formats ifile
yarn.log-aggregation.file-controller.ifile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
yarn.log-aggregation.ifile.remote-app-log-dir app-logs
yarn.resourcemanager.connect.max-wait.ms 1000

core-site.xml:

ipc.client.connect.max.retries 3
ipc.client.connect.retry.interval 10

Run a Job with above configs and Stop the RM.

[ambari-qa@yarn-ats-1 ~]$ yarn logs -applicationId application_1557482389195_0001
2019-05-10 10:03:58,215 INFO client.RMProxy: Connecting to ResourceManager at yarn-ats-1/172.26.81.91:8050
Unable to get ApplicationState. Attempting to fetch logs directly from the filesystem.
Can not find the appOwner. Please specify the correct appOwner
Could not locate application logs for application_1557482389195_0001


[ambari-qa@yarn-ats-1 ~]$ hadoop fs -ls /app-logs/ambari-qa/logs-ifile/application_1557482389195_0001
Found 1 items
-rw-r-----   3 ambari-qa supergroup      18058 2019-05-10 10:01 /app-logs/ambari-qa/logs-ifile/application_1557482389195_0001/yarn-ats-1_45454



 
 Fix LogsCLI guessAppOwner ignores custom file format suffix",No
13179153,"In WALFactory, there is an enum Providers which has a list of supported WALProvider implementations. In addition to list this, there is also a defaultProvider (which the Configuration defaults to), that is meant to be our ""advertised"" default WALProvider.
However, the implementation of getProviderClass in WALFactory doesn't actually adhere to the value of this enum, instead always returning AsyncFSWal if it can be loaded.
Having the default value in the enum but then overriding it in the implementation of getProviderClass is silly and misleading. 
 WALFactory has misleading notion of ""default""",13161059,"Add unit tests for feature that was implemented in HIVE-18788.
The integration tests are present, but it will be useful to catch errors during module build. 
 Clean up inputs in JDBC PreparedStatement. Add unit tests.",No
13233364,"Step 1 for https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node

Configure parameters as follows:
etc/hadoop/mapred-site.xml:

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
<configuration>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/</value>
    </property>
</configuration>
but setting this will throw an error when running yarn : 
2019-05-14 16:32:05,815 ERROR org.apache.hadoop.conf.Configuration: error parsing conf mapred-site.xml
com.ctc.wstx.exc.WstxParsingException: Illegal to have multiple roots (start tag in epilog?).This should be modified to 


<configuration> 
<property> 
<name>mapreduce.framework.name</name> 
<value>yarn</value> 
</property> 
<property> 
<name>mapreduce.application.classpath</name> 
<value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value> 
</property> 
</configuration>
 
 Yarn Docs : single cluster yarn setup - Step 1 configure parameters - multiple roots",13201019,"In https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node, there are two configuration tags in mapred-site.xml. 
 mapred-site.xml is misformatted in single node setup document",yes
13183378,"I havebackport Multi-standby NNs to our own hdfs version. Ifound an issue of EditLog roll.
Reproducible Steps：
1.original state
nn1 active
nn2 standby
nn3 standby
2. stop nn1
3. new state
nn1stopped
nn2active
nn3 standby
4. nn3 unable to trigger a roll of the active NN
[2018-08-22T10:33:38.025+08:00][WARN]namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java 307)[Edit log tailer]: Unable to trigger a roll of the active NN
java.net.ConnectException: Call From<nn3 hostname> to<nn1 hostname> failed on connection exception: java.net.ConnectException: Connection refused; For more details see:http://wiki.apache.org/hadoop/ConnectionRefused
at sun.reflect.GeneratedConstructorAccessor17.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:782)
at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:722)
at org.apache.hadoop.ipc.Client.call(Client.java:1536)
at org.apache.hadoop.ipc.Client.call(Client.java:1463)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:237)
at com.sun.proxy.$Proxy16.rollEditLog(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:148)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:301)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:298)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:414)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:304)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.access$800(EditLogTailer.java:69)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:346)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:315)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:332)
at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:328)
Caused by: java.net.ConnectException: Connection refused
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:521)
at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:485)
at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:658)
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:756)
at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:419)
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1585)
at org.apache.hadoop.ipc.Client.call(Client.java:1502)
... 14 more 
 NameNode: Unable to trigger a roll of the active NN",13179708,"When name node calltriggerActiveLogRoll, and the cachedActiveProxy is a dead name node, it will throws a ConnectTimeoutException, expected behavior is to try next NN, but current logic doesn't do so, instead, it keepstryingthe dead, mistakenly take it as active.

2018-08-17 10:02:12,001 WARN [Edit log tailer] org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
org.apache.hadoop.net.ConnectTimeoutException: Call From SourceMachine001/SourceIP to001 TargetMachine001.ap.gbl:8020 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:298)

C:\Users\rotang>pingTargetMachine001
Pinging TargetMachine001[TargetIP001] with 32 bytes of data:
 Request timed out.
 Request timed out.
 Request timed out.
 Request timed out.
Attachment is a log file saying how it repeatedly retries a dead name node, and a fix patch.
I replaced the actual machine name/ip as SourceMachine001/SourceIP001 and TargetMachine001/TargetIP001.

How to Repro:
In a good running NNs, take down the active NN (don't let it come back during test), and then the stand by NNs willkeep tryingdead (old active) NN, because it is the cached one. 
 triggerActiveLogRoll stuck on dead name node, when ConnectTimeoutException happens. ",yes
13129736,"In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.
Simple test case to reproduce this issue

set hive.vectorized.execution.enabled=true;
create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;
insert into parquettable values (-104, 25), (-112, 24), (54, 9);
select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) < 50 order by diff desc;
+-------+-----+-------+
|  t1   | t2  | diff  |
+-------+-----+-------+
| -104  | 25  | 127   |
| -112  | 24  | 120   |
| 54    | 9   | 45    |
+-------+-----+-------+


When vectorization is turned off the same query produces only one row. 
 Vectorized execution handles overflows in a different manner than non-vectorized execution",13140634,"After YARN-7139, the new application can get correct queue name in its submission context. We need to do the same thing for application recovering. 


      if (isAppRecovering) {
        if (LOG.isDebugEnabled()) {
          LOG.debug(applicationId
              + "" is recovering. Skip notifying APP_ACCEPTED"");
        }
      } else {
        // During tests we do not always have an application object, handle
        // it here but we probably should fix the tests
        if (rmApp != null && rmApp.getApplicationSubmissionContext() != null) {
          // Before we send out the event that the app is accepted is
          // to set the queue in the submissionContext (needed on restore etc)
          rmApp.getApplicationSubmissionContext().setQueue(queue.getName());
        }
        rmContext.getDispatcher().getEventHandler().handle(
            new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));
      }


We can do it by moving the rmApp.getApplicationSubmissionContext().setQueue block out of the if-else block. cc Wilfred Spiegelenburg. 
 Reset the queue name in submission context while recovering an application",No
13155194,"Allows external clients to consume output from LLAP daemons in Arrow stream format. 
 Arrow format for LlapOutputFormatService (umbrella)",13158584,"""You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."" 
 Arrow SerDe itest failure",yes
13155515,"Now that we've branched for 3.0 we need to create SQL install and upgrade scripts for 3.1 
 Create metastore SQL install and upgrade scripts for 3.1",13161817,"metastore schema init scripts are missing for the new release after branch was cut-out. 
 Create schema scripts for Hive 3.1.0 and Hive 4.0.0",yes
13190008,"When testing namespace not enabled account using Oauth, some tests were skipped. So need to update the tests. 
 ABFS: Enable some tests for namespace not enabled account using OAuth",13228148,"In the following scenario, the Block will remain in the COMMITTED but not COMPLETE state and cannot be closed properly:

Client writes Block(bk1) to three data nodes (dn1/dn2/dn3).
bk1 has been completely written to three data nodes, and the data node succeeds FinalizeBlock, joins IBR and waits to report to NameNode.
The client commits bk1 after receiving the ACK.
When the DN has not been reported to the IBR, all three nodes dn1/dn2/dn3 enter Decommissioning.
The DN reports the IBR, but the block cannot be completed normally.


Then it will lead to the following related exceptions:
Exception
2019-04-02 13:40:31,882 INFO namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2790)) - BLOCK* blk_4313483521_3245321090 is COMMITTED but not COMPLETE(numNodes= 3 >= minimum = 1) in file xxx
2019-04-02 13:40:31,882 INFO ipc.Server (Server.java:logException(2650)) - IPC Server handler 499 on 8020, call Call#122552 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from xxx:47615
org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException: Not replicated yet: xxx
 at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2579)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)

And will cause the scenario described in HDFS-12747
The root cause is that addStoredBlock does not consider the case where the replications are in Decommission.
This problem needs to be fixed like HDFS-11499. 
 Please add my public key to committer keys",No
13206348,"In FSImageFormatProtobuf.SectionName#fromString(), as follows:


public static SectionName fromString(String name) {
  for (SectionName n : values) {
    if (n.name.equals(name))
      return n;
  }
  return null;
}


When the code meets an unknown section from the fsimage, the function will return null. Callers always operates the return value with a ""switch"" clause, like FSImageFormatProtobuf.Loader#loadInternal(), as:


switch (SectionName.fromString(n))


NPE will be thrown here. 
 how to ask a question about hdfs？ I don't find the page",13183001,"If you configure OAuth and then use abfs:// instead of abfss:// it will fail, but it takes a very long time, and still isn't very clear why. Good place to put a good human-readable exception message and fail fast. 
 Fail-fast when using OAuth over http",No
13268003,"Environment Cluster of 2 Namenodes and 5 Datanodes and ec policy enabled
fsck -blockId of a block associated with an EC File does not print status at the end and displays null instead
Result :
./hdfs fsck -blockId blk_-xxxxx
 Connecting to namenode via xxxx
 FSCK started by root (auth:SIMPLE) from /x.x.x.x at Wed Nov 13 19:37:02 CST 2019
Block Id: blk_-xxxxx
 Block belongs to: /ecdir/f2
 No. of Expected Replica: 3
 No. of live Replica: 3
 No. of excess Replica: 0
 No. of stale Replica: 2
 No. of decommissioned Replica: 0
 No. of decommissioning Replica: 0
 No. of corrupted Replica: 0
 null
Expected :
./hdfs fsck -blockId blk_-xxxxx
 Connecting to namenode via xxxx
 FSCK started by root (auth:SIMPLE) from /x.x.x.x at Wed Nov 13 19:37:02 CST 2019
Block Id: blk_-xxxxx
 Block belongs to: /ecdir/f2
 No. of Expected Replica: 3
 No. of live Replica: 3
 No. of excess Replica: 0
 No. of stale Replica: 2
 No. of decommissioned Replica: 0
 No. of decommissioning Replica: 0
 No. of corrupted Replica: 0
Block replica on datanode/rack: vm10/default-rack is HEALTHY
 
 FSCK for a block of EC Files doesnt display status at the end",13268026,"EC file blockId location info displaying as ""null"" with hdfs fsck -blockId command

Check the blockId information of an EC enabled file with ""hdfs fsck -blockId"" Check the blockId information of an EC enabled file with ""hdfs fsck -blockId"" blockId location related info will display as null,which needs to be rectified. 

      Check the attachment ""EC_file_block_info""
=======================================================



Check the output of a normal file block to compare
             

===========================================================
       

Actual Output :-  null 
Expected output :- It should display the blockId location related info as (nodes, racks) of the block as specified in the usage info of fsck -blockId option.                  [like : Block replica on datanode/rack: BLR10000xx038/default-rack is HEALTHY]

 
 EC: EC file blockId location info displaying as ""null"" with hdfs fsck -blockId command",yes
13142472,"RMcrashes with NPE during failover becauseACL configurations were changed as a result we no longer have a rights to submit anapplication to a queue.
Scenario:

Submit an application
Change ACL configuration for a queue that accepted the application so that an owner of the application will no longer have a rights to submit this application.
Restart RM.

As a result, we get NPE:
2018-02-27 18:14:00,968 INFO org.apache.hadoop.service.AbstractService: Service ResourceManager failed in state STARTED; cause: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplicationAttempt(FairScheduler.java:738)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1286)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:116)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1098)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1044)
	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385 
 RM crashes with NPE during recovering if ACL configuration was changed",13307183,"Resourcemanager recover failed when fair scheduler queue acl changed. Because of queue acl changed, when recover the application (addApplication() in fairscheduler) is rejected. Then recover the applicationAttempt (addApplicationAttempt() in fairscheduler) get Application is null. This will lead to two RM is at standby. Repeat as follows.

user run a long running application.
change queue acl (aclSubmitApps) so that the user does not have permission.
restart the RM.



2020-05-25 16:04:06,191 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1590393162216_0005 with final state: FAILED
2020-05-25 16:04:06,192 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplicationAttempt(FairScheduler.java:663)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1246)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:116)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1072)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1036)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:789)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:845)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:897)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:850)
        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:723)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:322)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:427)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1173)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:584)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:980)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1021)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1017)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1659)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1017)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:301)
        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:813)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)


 
 Resourcemanager recover failed when fair scheduler queue acl changed",yes
13283760,"The test `testNegativeGroupCaching` can fail if the iteration order of HashSet changes. In detail, the method `assertEquals` (line 331) compares `groups.getGroups(user)` with an ArrayList `myGroups`. The method `getGroups` converts `allGroups` (a HashSet) to a list and it calls iterator in HashSet. However, the iteration is non-deterministic.
This PR proposes to modify HashSet to LinkedHashSet for a deterministic order. 
 Test TestGroupsCaching fail if HashSet iterates in a different order",13177243,"TestMiniDruidCliDriver is failing intermittently a significant percentage of the time.
druid_timestamptz
druidmini_joins
druidmini_masking
druidmini_test1 
 FlakyTest: TestMiniDruidCliDriver",No
13339216,"
https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/317/testReport/junit/org.apache.hadoop.security/TestLdapGroupsMapping/testLdapReadTimeout/

https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java8-linux-x86_64/317/testReport/junit/org.apache.hadoop.security/TestLdapGroupsMapping/testLdapConnectionTimeout/

The tests are failing in open-jdk due to change in exception message(One space character). The tests passes in oracle java(for me)


 Expected to find 'LDAP response read timed out, timeout used:3000ms' but got unexpected exception: javax.naming.NamingException: LDAP response read timed out, timeout used: 3000 ms.





 
 TestLdapGroupsMapping is failing in trunk",13338474,"Looks like a change in the exception strings is breaking the validation code


[ERROR]   TestLdapGroupsMapping.testLdapReadTimeout:447  Expected to find 'LDAP response read timed out, timeout used:4000ms' but got unexpected exception: javax.naming.NamingException: LDAP response read timed out, timeout used: 4000 ms.; remaining name ''
	at com.sun.jndi.ldap.LdapRequest.getReplyBer(LdapRequest.java:129)

 
 TestLdapGroupsMapping failing -string mismatch in exception validation",yes
13222548,"When using DistCP over HTTPFS with data that contains Spark partitions, DistCP fails to access the partitioned parquet files since the ""="" characters in file path gets double encoded:
""/test/spark/partition/year=2019/month=1/day=1""
 to
""/test/spark/partition/year%253D2019/month%253D1/day%253D1""
This happens since fsPathItem containing the character '=' is encoded by URLEncoder.encode(fsPathItem, ""UTF-8"") to '%3D' and then encoded again by new Path(....) to '%253D'. 
 WebHdfsFileSystem.toUrl double encodes characters",13218595,"There was an enhancement to allow semicolon in source/target URLs for distcp use case as part of HDFS-13176 and backward compatibility fix as part ofHDFS-13582 . Still there seems to be an issue when trying to trigger distcp from 3.x cluster to pull webhdfs data from 2.x hadoop cluster. We might need to deal with existing fix as described below by making sure if url is already encoded or not. That fixes it.
diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
index 5936603c34a..dc790286aff 100644
— a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
@@ -609,7 +609,10 @@ URL toUrl(final HttpOpParam.Op op, final Path fspath,
 boolean pathAlreadyEncoded = false;
 try {
 fspathUriDecoded = URLDecoder.decode(fspathUri.getPath(), ""UTF-8"");

pathAlreadyEncoded = true;
+ if(!fspathUri.getPath().equals(fspathUriDecoded))
+ 
{
+ pathAlreadyEncoded = true;
+ }
 } catch (IllegalArgumentException ex) 
{
 LOG.trace(""Cannot decode URL encoded file"", ex);
 }


 
 Distcp fails in Hadoop 3.x when 2.x source webhdfs url has special characters in hdfs file path",yes
13324869,"As of now the default for github is set only to worklog, To enable link and label, We need to add this. 
 Add .asf.yaml to allow github and jira integration",13249239,"

[ERROR] testBlockReportSucceedsWithLargerLengthLimit(org.apache.hadoop.hdfs.server.datanode.TestLargeBlockReport)  Time elapsed: 47.956 s  <<< ERROR!
org.apache.hadoop.ipc.RemoteException(java.io.IOException): java.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.runBlockOp(BlockManager.java:5011)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1581)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:181)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31664)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:529)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1001)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:929)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1891)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2921)
Caused by: java.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:424)
	at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:396)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiffSorted(BlockManager.java:2952)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2787)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2655)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.lambda$blockReport$0(NameNodeRpcServer.java:1582)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.processQueue(BlockManager.java:5089)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.run(BlockManager.java:5068)
Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)
	at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)
	at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)
	at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)
	at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:420)
	... 8 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1553)
	at org.apache.hadoop.ipc.Client.call(Client.java:1499)
	at org.apache.hadoop.ipc.Client.call(Client.java:1396)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy25.blockReport(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:218)
	at org.apache.hadoop.hdfs.server.datanode.TestLargeBlockReport.testBlockReportSucceedsWithLargerLengthLimit(TestLargeBlockReport.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Ref ::https://builds.apache.org/job/PreCommit-HDFS-Build/27416/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt 
 TestLargeBlockReport#testBlockReportSucceedsWithLargerLengthLimit fails in trunk",No
13155612,"Tensorflow could be run onYARN and could leverage YARN's distributed features.
This spec fill will help to run Tensorflow on yarn with GPU/docker

Please go toYARN-8135Submarine for deep learning framework support on YARN. 
 Running Tensorflow on YARN with GPU and Docker - Examples",13156157,"Mxnet could be run on YARN. This jira will help to add examples, yarnfile-, docker files which are needed to run Mxnet on YARN.-

Please go toYARN-8135Submarine for deep learning framework support on YARN. 
 mxnet yarn spec file to add to native service examples",yes
13304868,"

2020-03-25 14:58:58,375 INFO  [RpcServer.default.FPBQ.Fifo.handler=397,queue=77,port=60000] master.HMaster: Client=hbaseadmin//10.196.142.227 create 'extra_500000039', {NAME => 'extra', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'false', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'DIFF', TTL => '2592000 SECONDS (30 DAYS)', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'SNAPPY', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}
2020-03-25 14:58:58,482 INFO  [RpcServer.default.FPBQ.Fifo.handler=397,queue=77,port=60000] rsgroup.RSGroupAdminServer: Moving table extra_500000039 to RSGroup default
2020-03-25 14:58:58,485 ERROR [RpcServer.default.FPBQ.Fifo.handler=397,queue=77,port=60000] master.TableStateManager: Unable to get table extra_500000039 state
org.apache.hadoop.hbase.master.TableStateManager$TableStateNotFoundException: extra_500000039
        at org.apache.hadoop.hbase.master.TableStateManager.getTableState(TableStateManager.java:215)
        at org.apache.hadoop.hbase.master.TableStateManager.isTableState(TableStateManager.java:147)
        at org.apache.hadoop.hbase.master.assignment.AssignmentManager.isTableDisabled(AssignmentManager.java:388)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.moveTableRegionsToGroup(RSGroupAdminServer.java:233)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.moveTables(RSGroupAdminServer.java:347)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.assignTableToGroup(RSGroupAdminEndpoint.java:456)
        at org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.postCreateTable(RSGroupAdminEndpoint.java:479)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost$13.call(MasterCoprocessorHost.java:350)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost$13.call(MasterCoprocessorHost.java:347)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult.callObserver(CoprocessorHost.java:551)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.execOperation(CoprocessorHost.java:625)
        at org.apache.hadoop.hbase.master.MasterCoprocessorHost.postCreateTable(MasterCoprocessorHost.java:347)
        at org.apache.hadoop.hbase.master.HMaster$4.run(HMaster.java:2048)
        at org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.submitProcedure(MasterProcedureUtil.java:134)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:2031)
        at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:658)
        at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:133)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:338)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:318)
2020-03-25 14:58:58,486 INFO  [RpcServer.default.FPBQ.Fifo.handler=397,queue=77,port=60000] rsgroup.RSGroupAdminServer: Moving region(s) for table extra_500000039 to RSGroup default
2020-03-25 14:58:58,486 INFO  [RpcServer.default.FPBQ.Fifo.handler=397,queue=77,port=60000] master.MasterRpcServices: Client=hbaseadmin//10.196.142.227 procedure request for creating table: namespace: ""default""
qualifier: ""extra_500000039""
 procId is: 14019


Latch is released when execute prepareCreate, so MasterCoprocessorHost#postCreateTable can run before the table actually created, when RSGroup enable, it will throw exception like the above.
Looks like the issue will not happen in master, but can happen in branch-2. 
 Assign table to default RSGroup may throw exception when RSGroup enabled",13296897,"IF RS group is enabled then Unable to get table state error is thrown while creating a new table

Stpes:
1: Make sure RS group feature is enabled
2: Create a table say usertable2000
3: Check master log and observe below exception is thrown org.apache.hadoop.hbase.master.TableStateManager$TableStateNotFoundException: usertable2000 
 TableStateNotFoundException happends when table creation if rsgroup is enable",yes
13148162,"When region replication is on, get_splits returns duplicate split points like the following:


hbase(main):001:0> create ""test"", ""cf"", {REGION_REPLICATION => 3}, SPLITS => [""10""]
Created table test
Took 1.0975 seconds
hbase(main):002:0> get_splits ""test""
Total number of splits = 4
10
10
10
Took 0.0941 seconds

 
 Also cleanup last pushed sequence id in ReplicationBarrierCleaner",13146766,"start-ozone.sh/stop-ozone.sh fails because the shell script fails to resolve the full path. Interestingly, the start-dfs.sh script work as expected.


/opt/hadoop/hadoop-3.2.0-SNAPSHOT/sbin/start-ozone.sh 
y129.l42scl.hortonworks.com: bash: /bin/oz: No such file or directory
y129.l42scl.hortonworks.com: bash: /bin/oz: No such file or directory

 
 Scheduled chore task to clean up the last pushed sequence id for the expired regions. ",yes
13296395,"https://builds.apache.org/job/hadoop-qbt-trunk-java8-linux-x86/1460/artifact/out/patch-unit-hadoop-mapreduce-project_hadoop-mapreduce-client_hadoop-mapreduce-client-jobclient.txt

[INFO] Running org.apache.hadoop.mapred.TestNetworkedJob
[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 20.981 s <<< FAILURE! - in org.apache.hadoop.mapred.TestNetworkedJob
[ERROR] testNetworkedJob(org.apache.hadoop.mapred.TestNetworkedJob)  Time elapsed: 4.588 s  <<< FAILURE!
org.junit.ComparisonFailure: expected:<[]default> but was:<[root.]default>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.mapred.TestNetworkedJob.testNetworkedJob(TestNetworkedJob.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

 
 TestNetworkedJob fails",13225585,"
Right now there are no metrics available for Opportunistic Scheduler at ResourceManager. As part of this jira, we will add metrics like number of allocated opportunistic containers, released opportunistic containers, node level allocations, rack level allocations etc. for Opportunistic Scheduler.

 
 Add Opportunistic Scheduler metrics in ResourceManager.",No
13177024,"HBASE-18754 change the serialization of TimeRangeTracker from ""manual way"" to protobuf. However, the change breaks the backward compatibility of hfile. We should revert the change ASAP. 
 Revert the change of serializing TimeRangeTracker",13209008,"When running org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore integration test, I got the following stack trace:


[ERROR] org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore  Time elapsed: 0.333 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore.beforeClassSetup(ITestDynamoDBMetadataStore.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


The NPE happened here:


assertTrue(""Test DynamoDB table name: '""
  + S3GUARD_DDB_TEST_TABLE_NAME_KEY + ""' and production table name: '""
  + S3GUARD_DDB_TABLE_NAME_KEY + ""' can not be the same."",
  !conf.get(S3GUARD_DDB_TABLE_NAME_KEY).equals(testDynamoDBTableName));


The problem is that though we check previously whether the variable testDynamoDBTableName (fs.s3a.s3guard.ddb.test.table config) is not null, but we don't do the same for fs.s3a.s3guard.ddb.table (S3GUARD_DDB_TABLE_NAME_KEY) before calling the .equals(), thus causing an NPE.
Since we don't need the fs.s3a.s3guard.ddb.table config for the test, we should check first whether that config is given or not, and only comparing the two configs if they both exist. 
 NPE in ITestDynamoDBMetadataStore when fs.s3a.s3guard.ddb.table is not set",No
13285794,"Follow-up Jira ofHIVE-22876, enforcing is also present at:


./storage-api/checkstyle/checkstyle.xml
./standalone-metastore/checkstyle/checkstyle.xml


Remove those too. 
 Remove enforcing of package-info.java files from the rest of the checkstyle files",13180000,"The following can be observed in master branch:


java.lang.NullPointerException
	at org.apache.hadoop.hbase.rest.TestTableResource.setUpBeforeClass(TestTableResource.java:134)


The NPE comes from the following in TestEndToEndSplitTransaction :


        compactAndBlockUntilDone(TEST_UTIL.getAdmin(),
          TEST_UTIL.getMiniHBaseCluster().getRegionServer(0), daughterA.getRegionName());


Initial check of the code shows that TestEndToEndSplitTransaction uses TEST_UTIL instance which is created within TestEndToEndSplitTransaction. However, TestTableResource creates its own instance of HBaseTestingUtility.
Meaning TEST_UTIL.getMiniHBaseCluster() would return null, since the instance created by TestEndToEndSplitTransaction has hbaseCluster as null. 
 schematool shall not pollute beeline history",No
13270840,"Found an issue where trying to request GPUs on a newly booted RM cannot schedule. It throws the exception in SchedulerUtils#throwInvalidResourceException:

throw new InvalidResourceRequestException(
    ""Invalid resource request, requested resource type=["" + reqResourceName
        + ""] < 0 or greater than maximum allowed allocation. Requested ""
        + ""resource="" + reqResource + "", maximum allowed allocation=""
        + availableResource
        + "", please note that maximum allowed allocation is calculated ""
        + ""by scheduler based on maximum resource of registered ""
        + ""NodeManagers, which might be less than configured ""
        + ""maximum allocation=""
        + ResourceUtils.getResourceTypesMaximumAllocation());

Upon refreshing scheduler (e.g. via refreshQueues), GPU scheduling works again.
I think the RC is that upon scheduler refresh, resource-types.xml is loaded in CapacitySchedulerConfiguration (as part ofYARN-7738), so when we call ResourceUtils#fetchMaximumAllocationFromConfig in CapacitySchedulerConfiguration#getMaximumAllocationPerQueue, it's able to fetch the yarn.resource-types config. But resource-types.xml is not loaded into the conf in CapacityScheduler#initScheduler, so it doesn't find the custom resource when computing max allocations, and the custom resource max allocation is 0. 
 Max allocation per queue is zero for custom resource types on RM startup",13210167,"In a non-secure cluster. Reproduce it as follows:

Set capacity scheduler in yarn-site.xml
Use default capacity-scheduler.xml
Set custom resource type ""cmp.com/hdw"" in resource-types.xml
Set a value say 10 in node-resources.xml
Start cluster
Submit a distribute shell application which requests some ""cmp.com/hdw""

The AM will get an exception from CapacityScheduler and then failed. This bug doesn't exist in FairScheduler.


2019-01-17 22:12:11,286 INFO distributedshell.ApplicationMaster: Requested container ask: Capability[<memory:2048, vCores:2, cmp.com/hdw: 2>]Priority[0]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[]
2019-01-17 22:12:12,326 ERROR impl.AMRMClientAsyncImpl: Exception on heartbeat
org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request! Cannot allocate containers as requested resource is greater than maximum allowed allocation. Requested resource type=[cmp.com/hdw], Requested resource=<memory:2048, vCores:2, cmp.com/hdw: 2>, maximum allowed allocation=<memory:8192, vCores:4>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:4, cmp.com/hdw: 9223372036854775807>
 at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.throwInvalidResourceException(SchedulerUtils.java:492)
 at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.checkResourceRequestAgainstAvailableResource(SchedulerUtils.java:388)
 at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:315)
 at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:293)
 at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:301)
 at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:250)
at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:240)
at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)
at org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)
at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:424)
at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)
...

Did a roughly debugging, below method return the wrong maximum capacity.
DefaultAMSProcessor.java, Line 234.


Resource maximumCapacity =
 getScheduler().getMaximumResourceCapability(app.getQueue());

The above code seems should return ""<memory:8192, vCores:4, cmp.com/hdw:10>"" but returns ""<memory:8192, vCores:4>"".
This incorrect value might be caused by queue maximum allocation calculation involved in YARN-8720:
AbstractCSQueue.java Line364


this.maximumAllocation =
 configuration.getMaximumAllocationPerQueue(
 getQueuePath());

And this invokes CapacitySchedulerConfiguration.java Line 895:


Resource clusterMax = ResourceUtils.fetchMaximumAllocationFromConfig(this);


Passing a ""this"" which is not a YarnConfiguration instance will cause below code return null for resource names and then only contains mandatory resources. This might be the root cause.


private static Map<String, ResourceInformation> getResourceInformationMapFromConfig(
...
// NULL value here!
String[] resourceNames = conf.getStrings(YarnConfiguration.RESOURCE_TYPES);

 
 When using custom resource type, application will fail to run due to the CapacityScheduler throws InvalidResourceRequestException(GREATER_THEN_MAX_ALLOCATION) ",yes
13326378,"MetaFixer fails to fix overlaps when multiple tables have overlaps
Steps to reproduce from UT.

Create table t1 and t2 with split keys, [""bbb"", ""ccc"", ""ddd"", ""eee""]
Create extra region in both t1 and t2 with start key ""bbb"" and end key ""ddd""
Run catalog janitor, It will report total 4 overlaps, 2 from each table.
Run MetaFixer, wait for merges to finish.
Run the catalog janitor again and verify report, there should not be any overlap
Overlap still exists. Reproduced!!!

Analysis.

When I run the same scenario for just one table t1, overlaps are fixed successfully.
Seems problem with MetaFixer#calculateMerges.
I think merges should be calculated within a table. Across the table merge does not have significance.

 
 MetaFixer fails to fix overlaps when multiple tables have overlaps",13130301,"Here is [~Apache9] 's patch from the parent so it applies on top of what was committed in the parent.
Patch makes it so we we don't close out zk if available Tasks to run and nicer logging. 
 Do not close connection to zk when there are still pending request in ReadOnlyZKClient",No
13206167,"The test TestWebHdfsTimeouts keep failing. 
 Fix TestWebHdfsTimeouts",13203278,"Reference to failure
https://builds.apache.org/job/hadoop-qbt-trunk-java8-linux-x86/982/testReport/junit/org.apache.hadoop.hdfs.web/TestWebHdfsTimeouts/ 
 TestWebHdfsTimeouts Fails intermittently in trunk",yes
13233364,"Step 1 for https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node

Configure parameters as follows:
etc/hadoop/mapred-site.xml:

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
<configuration>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/</value>
    </property>
</configuration>
but setting this will throw an error when running yarn : 
2019-05-14 16:32:05,815 ERROR org.apache.hadoop.conf.Configuration: error parsing conf mapred-site.xml
com.ctc.wstx.exc.WstxParsingException: Illegal to have multiple roots (start tag in epilog?).This should be modified to 


<configuration> 
<property> 
<name>mapreduce.framework.name</name> 
<value>yarn</value> 
</property> 
<property> 
<name>mapreduce.application.classpath</name> 
<value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value> 
</property> 
</configuration>
 
 Yarn Docs : single cluster yarn setup - Step 1 configure parameters - multiple roots",13256676,"官网说明错误


正确的应该是下面的
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/</value>
    </property>
</configuration>
 
 官网代码片段错误",yes
13156418,"Some of the MR examples either don't work or underperform on cloud infrastructure, all straightforward to fix. Of course, that means the cloud connectors all get an opportunity to add more integration tests... 
 MR examples to work better against cloud stores",13329416,"As now we support altering meta table, it will be better to also deal with changing meta replica number using altering meta, i.e, we could unify the logic in MasterMetaBootstrap to ModifyTableProcedure, and another benefit is that we do not need to restart master when changing the replica number for meta. 
 Change meta replica count by altering meta table descriptor",No
13320779,"As perBlockPlacementPolicyRackFaultTolerant, one block per rack is placed but due to this Heterogeneous datanodes are not supported in Hadoop3.So we need to change theBlockPlacementPolicyRackFaultTolerant to place one block on a rack with the AvailableSpaceBlockPlacementPolicy feature. 
 Support AvailableSpaceBlockPlacementPolicy in BlockPlacementPolicyRackFaultTolerant",13299778,"The Present AvailableSpaceBlockPlacementPolicy extends the Default Block Placement policy, which makes it apt for Replicated files. But not very efficient for EC files, which by default use. BlockPlacementPolicyRackFaultTolerant. So propose a to add new BPP having similar optimization as ASBPP where as keeping the spread of Blocks to max racks, i.e as RackFaultTolerantBPP.
This could extendBlockPlacementPolicyRackFaultTolerant, rather than the BlockPlacementPOlicyDefault like ASBPP and keep other logics of optimization same as ASBPP 
 Add Available Space Rack Fault Tolerant BPP",yes
13239287,"When you try to run all tests from TestTimelineReaderWebServicesHBaseStorage, the result is the following:

[ERROR] Failures: 
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetAppNotPresent:2222->AbstractTimelineReaderHBaseTestBase.verifyHttpResponse:140 Response from server should have been Not Found
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunNotPresent:2192->AbstractTimelineReaderHBaseTestBase.verifyHttpResponse:140 Response from server should have been Not Found
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testUIDNotProperlyEscaped:905->AbstractTimelineReaderHBaseTestBase.verifyHttpResponse:140 Response from server should have been Bad Request
[ERROR] Errors: 
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testForFlowAppsPagination:2375->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testForFlowRunAppsPagination:2420->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testForFlowRunsPagination:2465->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGenericEntitiesForPagination:2272->verifyEntitiesForPagination:2288->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetApp:1024->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetAppWithoutFlowInfo:1064->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetAppsMetricsRange:2516->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesByUID:662->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesConfigFilters:1263->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesDataToRetrieve:1154->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesEventFilters:1640->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesInfoFilters:1380->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricFilters:1494->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesMetricsTimeRange:1820->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesRelationFilters:1696->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntitiesWithoutFlowInfo:1130->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntityDataToRetrieve:1905->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetEntityWithoutFlowInfo:1113->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowApps:2047->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsFilters:2153->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowAppsNotPresent:2253->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowRun:443->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunApps:1984->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunAppsNotPresent:2235->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowRuns:488->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowRunsMetricsToRetrieve:616->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlows:918->verifyFlowEntites:2349->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowsForPagination:990->verifyFlowEntites:2349->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testGetFlowsNotPresent:2204->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[ERROR]   TestTimelineReaderWebServicesHBaseStorage.testUIDQueryWithAndWithoutFlowContextInfo:821->AbstractTimelineReaderHBaseTestBase.getResponse:129 » IO
[INFO] 
[ERROR] Tests run: 33, Failures: 3, Errors: 30, Skipped: 0

 
 All testcase fails in TestTimelineReaderWebServicesHBaseStorage on branch-3.2",13190230,"TestTimelineReaderWebServicesHBaseStorage has been failing in nightly builds with NoClassDefFoundError in the tests.  Sample error and stacktrace to follow. 
 TestTimelineReaderWebServicesHBaseStorage tests failing with NoClassDefFoundError",yes
13167213,"HADOOP-13660 upgraded from commons-configuration 1.x to 2.x. commons-configuration is used when parsing the metrics configuration properties file. The new builder API used in the new version apparently makes use of a bunch of very bloated reflection and classloading nonsense to achieve the same goal, and this results in a regression of >100ms of CPU time as measured by a program which simply initializes DefaultMetricsSystem.
This isn't a big deal for long-running daemons, but for MR tasks which might only run a few seconds on poorly-tuned jobs, this can be noticeable. 
 Upgrade to commons-configuration 2.1 regresses task CPU consumption",13303535,"hive.server2.authentication.kerberos.principal set in the form of hive/_HOST@REALM,
Tez task can start at the random NM host and unfold the value of _HOST with the value of fqdn where it is running. this leads to an authentication issue.
for LLAP there is fallback for LLAP daemon keytab/principal, Kafka 1.1 onwards support delegation token and we should take advantage of it for hive on tez. 
 Release 2.2.5",No
13130654,"Appy's dashboard for 2.0 https://builds.apache.org/job/HBase-Find-Flaky-Tests-branch2.0/lastSuccessfulBuild/artifact/dashboard.html and a GCE run last night showed how bad of a state our unit tests are in on tip of branch-2. This issue does triage fixing and disabling so I can push out the beta-1. Its done as a subtask of HBASE-19694 because a few of the tests were broke by this commit. 
 Fix or disable tests broken in branch-2 so can cut beta-1",13197198,"I get this when I run bin/hbase classpath whether a built checkout or an undone tarball. 
 Error: Could not find or load main class org.apache.hadoop.hbase.util.GetJavaProperty",No
13216563,"If external tables are enabled for replication on an existing repl policy, then bootstrapping of external tables are combined with incremental dump.
If incremental bootstrap load fails with non-retryable error for which user will have to manually drop all the external tables before trying with another bootstrap dump. For full bootstrap, to retry with different dump, we suggested user to drop the DB but in this case they need to manually drop all the external tables which is not so user friendly. So, need to handle it in Hive side as follows.
REPL LOAD takes additional config (passed by user in WITH clause) that says, drop all the tables which are bootstrapped from previous dump. 
hive.repl.clean.tables.from.bootstrap=<previous_bootstrap_dump_dir>
Hive will use this config only if the current dump is combined bootstrap in incremental dump.
Caution to be taken by user that this config should not be passed if previous REPL LOAD (with bootstrap) was successful or any successful incremental dump+load happened after ""previous_bootstrap_dump_dir"". 
 Hive should support clean-up of previously bootstrapped tables when retry from different dump.",13272023,"Make class PrunedPartitionList.java more immutable.  It kinda is already.  Immutable classes are easier to reason about.
Use Collections.emptyXXX in PartitionPruner to save an object instantiation at runtime. 
 PartitionPruner use Collections Class",No
13345988,"
AcidUtils.getAcidState is doing a recursive listing on S3 FileSystem, it already knows the content of each delta and base directory, this could be returned to OrcInputFormat, to avoid listing each delta directory again there.
AcidUtils.getAcidstate submethods are collecting more and more infos about the state of the data directory. This could be done directly to the final Directory object to avoid 10+ parameters in methods.
AcidUtils.Directory, OrcInputFormat.AcidDirInfo and AcidUtils.TxnBase can be merged to one class, to clean up duplications.

 
 Upgrade ORC to 1.6.6",13307664,"Apache Hive is currently on 1.5.X version and in order to take advantage of the latest ORC improvements such as column encryption we have to bump to 1.6.X.
https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12343288&styleName=&projectId=12318320&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED_4ae78f19321c7fb1e7f337fba1dd90af751d8810_lin
Even though ORC reader could work out of the box, HIVE LLAP is heavily depending on internal ORC APIs e.g., to retrieve and store File Footers, Tails, streams – un/compress RG data etc. As there ware many internal changes from 1.5 to 1.6 (Input stream offsets, relative BufferChunks etc.) the upgrade is not straightforward.
This Umbrella Jira tracks this upgrade effort. 
 Upgrade ORC version to 1.6.7",yes
13161856,"https://builds.apache.org/job/PreCommit-HIVE-Build/11180/testReport/junit/org.apache.hadoop.hive.ql.plan.mapping/TestReOptimization/testStatCachingMetaStore/ 
 Re-enable TestReOptimization",13159775,"there is a slight chance that the runtimestats are not loaded when the check is done 
 TestReoptimization ensure that runtimestats cache is loaded",yes
13128365,"NameNode crashes repeatedly with NPE at the startup when trying to find the total number of under construction blocks. This crash happens after an open file, which was also part of a snapshot gets deleted along with the snapshot.

Failed to start namenode.
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:146)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCompleteBlocksTotal(FSNamesystem.java:6537)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startCommonServices(FSNamesystem.java:1232)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:706)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:692)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)


 
 NameNode crashes during restart after an OpenForWrite file present in the Snapshot got deleted",13201551,"See this in the outout and then the test hang

2018-11-29 20:47:50,061 WARN  [MockRSProcedureDispatcher-pool5-t10] assignment.AssignmentManager(894): The region server localhost,102,1 is already dead, skip reportRegionStateTransition call

 
 TestAssignmentManager is flakey",No
13198046,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13198035,"A vendor might need a customized scheduling policy for their devices. It could be scheduled based on topology, resource utilization, virtualization, device attribute and so on.
We'll provide another optional interface ""DevicePluginScheduler"" for the vendor device plugin to implement. Once it's implemented, the framework will prefer it to the default scheduler.
Thiswould bring more flexibility to the framework's scheduling mechanism. 
 DataNode runs async disk checks  maybe  throws NullPointerException In ThrottledAsyncChecker.java  ",yes
13188096,"Test case attached. The following query fail:


SELECT * FROM ext_auth1 JOIN ext_auth2 ON ext_auth1.ikey = ext_auth2.ikey


Error message:


2018-09-28T00:36:23,860 DEBUG [17b954d9-3250-45a9-995e-1b3f8277a681 main] dao.GenericJdbcDatabaseAccessor: Query to execute is [SELECT *
FROM (SELECT *
FROM ""SIMPLE_DERBY_TABLE1""
WHERE ""ikey"" IS NOT NULL) AS ""t""
INNER JOIN (SELECT *
FROM ""SIMPLE_DERBY_TABLE2""
WHERE ""ikey"" IS NOT NULL) AS ""t0"" ON ""t"".""ikey"" = ""t0"".""ikey"" {LIMIT 1}]
2018-09-28T00:36:23,864 ERROR [17b954d9-3250-45a9-995e-1b3f8277a681 main] dao.GenericJdbcDatabaseAccessor: Error while trying to get column names.
java.sql.SQLSyntaxErrorException: Table/View 'SIMPLE_DERBY_TABLE2' does not exist.
        at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement42.<init>(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.jdbc.Driver42.newEmbedPreparedStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source) ~[derby-10.14.1.0.jar:?]
        at org.apache.commons.dbcp.DelegatingConnection.prepareStatement(DelegatingConnection.java:281) ~[commons-dbcp-1.4.jar:1.4]
        at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.prepareStatement(PoolingDataSource.java:313) ~[commons-dbcp-1.4.jar:1.4]
        at org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.getColumnNames(GenericJdbcDatabaseAccessor.java:74) [hive-jdbc-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.storage.jdbc.JdbcSerDe.initialize(JdbcSerDe.java:78) [hive-jdbc-handler-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) [hive-serde-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:540) [hive-serde-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:90) [hive-metastore-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.getDeserializer(HiveMetaStoreUtils.java:77) [hive-metastore-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:295) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:277) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genTablePlan(SemanticAnalyzer.java:11100) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11468) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11427) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:525) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12319) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:356) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:289) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:669) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1872) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1819) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1814) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335) [hive-cli-4.0.0-SNAPSHOT.jar:?]


Hive is pushing the join into jdbc driver though the table refer to different data source.
 
 Schema change in HIVE-19166 should also go to hive-schema-4.0.0.hive.sql",13274887,"Looks like it was copied from a UT class and forgot to change it. 
 MetricsRegionServerWrapperImpl.getL1CacheHitCount always returns 200",No
13307154,"Run the 'catalogjanitor_run' and the 'hbck_chore_run' when we draw the 'HBCK Report' page.
Its PITA running command on the shell every time you need a new report.
Can avoid new behavior by passing '?cache=true' on URL. 
 Missing regionName while logging warning in HBCKServerCrashProcedure",13190008,"When testing namespace not enabled account using Oauth, some tests were skipped. So need to update the tests. 
 ABFS: Enable some tests for namespace not enabled account using OAuth",No
13285216,"Trunk does not compiler on Mac OS X with Java ""1.8.0_242"".


[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-maven-plugins: Compilation failure
[ERROR] /home/ahussein/hadoop/hadoop-maven-plugins/src/main/java/org/apache/hadoop/maven/plugin/protoc/ProtocRunner.java:[16,39] package com.fasterxml.jackson.core.type does not exist

 
 Trunk is broken on OS X",13146654,"Noticed this poking at metrics. The MBean ""Hadoop:service=HBase,name=RegionServer,sub=Regions"" carries per region metrics. They are all formatted like so:


Namespace_default_table_IntegrationTestBigLinkedList_metric_incrementTime_98th_percentile: 0,
Namespace_default_table_IntegrationTestBigLinkedList_metric_incrementTime_99th_percentile: 0,
Namespace_default_table_IntegrationTestBigLinkedList_metric_incrementTime_99.9th_percentile: 0,
Namespace_default_table_IntegrationTestBigLinkedList_metric_appendTime_num_ops: 0,
Namespace_default_table_IntegrationTestBigLinkedList_metric_appendTime_min: 0,
Namespace_default_table_IntegrationTestBigLinkedList_metric_appendTime_max: 0,
Namespace_default_table_IntegrationTestBigLinkedList_metric_appendTime_mean: 0,


In the middle of them all is a metric named...


numRegions: 15,


It has a different format and it is out of scope when compared to the other metrics in this mbean; it belongs better elsewhere, perhaps in the Server bean.
I noticed it because it breaks the parse done by tcollector scraping our metrics from /jmx
It was added long ago in HBASE-14166 
 [metrics] Ill-formatted numRegions metric in ""Hadoop:service=HBase,name=RegionServer,sub=Regions"" mbean",No
13252784,"we've been regularly getting 4-5 concurrent builds of PRs. 
 [Backport] HBASE-22867 to branch-1 to avoid ForkJoinPool to spawn thousands of threads",13273889,"The current permission checker of #MountTableStoreImpl is not very restrict. In some case, any user could add/update/remove MountTableEntry without the expected permission checking.
The following code segment try to check permission when operate MountTableEntry, however mountTable object is from Client/RouterAdmin MountTable mountTable = request.getEntry();, and user could pass any mode which could bypass the permission checker.


  public void checkPermission(MountTable mountTable, FsAction access)
      throws AccessControlException {
    if (isSuperUser()) {
      return;
    }

    FsPermission mode = mountTable.getMode();
    if (getUser().equals(mountTable.getOwnerName())
        && mode.getUserAction().implies(access)) {
      return;
    }

    if (isMemberOfGroup(mountTable.getGroupName())
        && mode.getGroupAction().implies(access)) {
      return;
    }

    if (!getUser().equals(mountTable.getOwnerName())
        && !isMemberOfGroup(mountTable.getGroupName())
        && mode.getOtherAction().implies(access)) {
      return;
    }

    throw new AccessControlException(
        ""Permission denied while accessing mount table ""
            + mountTable.getSourcePath()
            + "": user "" + getUser() + "" does not have "" + access.toString()
            + "" permissions."");
  }


I just propose revoke WRITE MountTableEntry privilege to super user only. 
 RBF: Impose directory level permissions for Mount entries",No
13163898,"Per findbugs report in YARN-8390, there is some inconsistent locking of reloadListener

Warnings
Click on a warning row to see full context information.
Multithreaded correctness Warnings



Code
Warning


IS
Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener; locked 75% of time



Bug type IS2_INCONSISTENT_SYNC (click for details) 
In class org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService
Field org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener
Synchronized 75% of the time
Unsynchronized access at AllocationFileLoaderService.java:[line 117]
Synchronized access at AllocationFileLoaderService.java:[line 212]
Synchronized access at AllocationFileLoaderService.java:[line 228]
Synchronized access at AllocationFileLoaderService.java:[line 269]



Details
IS2_INCONSISTENT_SYNC: Inconsistent synchronization
The fields of this class appear to be accessed inconsistently with respect to synchronization. This bug report indicates that the bug pattern detector judged that

The class contains a mix of locked and unlocked accesses,
The class is not annotated as javax.annotation.concurrent.NotThreadSafe,
At least one locked access was performed by one of the class's own methods, and
The number of unsynchronized field accesses (reads and writes) was no more than one third of all accesses, with writes being weighed twice as high as reads

A typical bug matching this bug pattern is forgetting to synchronize one of the methods in a class that is intended to be thread-safe.
You can select the nodes labeled ""Unsynchronized access"" to show the code locations where the detector believed that a field was accessed without synchronization.
Note that there are various sources of inaccuracy in this detector; for example, the detector cannot statically detect all situations in which a lock is held. Also, even when the detector is accurate in distinguishing locked vs. unlocked accesses, the code in question may still be correct. 
 Investigate AllocationFileLoaderService.reloadListener locking issue",13165507,"This is reported by findbugs. See: https://builds.apache.org/job/PreCommit-YARN-Build/21007/artifact/out/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-warnings.html

Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService.reloadListener; locked 75% of time

 
 Multithreaded correctness Warning occurs in AllocationFileLoaderService",yes
13151411,"SeveralTestNegativeCliDriver tests are failing because error messages have changed. They need their expected results updated. 
 Update expected result for some TestNegativeCliDriver tests",13151249,"Missed in HIVE-18859 
 Update golden files for negative tests",yes
13220414,"We've some useful discussion in HBASE-22005, so open an new JIRA for the ByteBuffer block reading.   
 Rewrite the block reading methods by using hbase.nio.ByteBuff",13156418,"Some of the MR examples either don't work or underperform on cloud infrastructure, all straightforward to fix. Of course, that means the cloud connectors all get an opportunity to add more integration tests... 
 MR examples to work better against cloud stores",No
13290843,"Our tests can create thousands of threads all up in the one JVM. Using less means less memory, less contention, and hopefully, likelier passes.
I've been studying the likes of TestNamespaceReplicationWithBulkLoadedData to see what it does as it runs (this test puts up 4 clusters with replication between). It peaks at 2k threads. After some configuration and using less HDFS, can get it down to ~800 threads and about 1/2 the memory-used.
(HDFS is a profligate offender. DataXceivers (Server and Client), jetty threads, Volume threads (async disk 'worker' then another for cleanup...), image savers, ipc clients – new thread per incoming connection w/o bound (or reuse), block responder threads, anonymous threads, and so on. Many are not configurable or boundable or are hard-coded; e.g. each volume gets 4 workers. Biggest impact was to be had by downing the count of data nodes. TODO: a follow-on that turns down DN counts in all tests)
I've been using Java Flight Recorder during this study. Here is how you get a flight recorder for the a single test run:


MAVEN_OPTS="" -XX:StartFlightRecording=disk=true,dumponexit=true,filename=recording.jfr,settings=profile,path-to-gc-roots=true,maxsize=1024m""  mvn  test -Dtest=TestNamespaceReplicationWithBulkLoadedData -Dsurefire.firstPartForkCount=0 -Dsurefire.secondPartForkCount=0 

i.e. start recording on mvn launch, bound the size of the recording, and have the test run in the mvn context (DON'T fork). Useful is connecting to the running test at the same time from JDK Mission Control. We do the latter because the thread reporting screen is overwhelmed by the count of running threads and if you connect live, you can at least get a 'live threads' graph w/ count as the test progresses. Useful.
When the test finishes, it dumps a .jfr file which can be opened in JDK MC. I've been compiling w/ JDK8 and then running w/ JDK11 so I can use JDK MC Version 7, the non-commercial latest. Works pretty well.
Let me put up a patch for tests that cuts down thread counts where we can.

 
 Have test runs use less resources",13286898,"Add row format OpenCSVSerde to the metastore column managed list 
 Scale *MiniCluster config for the environment it runs in.",yes
13157703,"

CREATE VIEW view_s1 AS select 1;

-- FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table

 
 Create View - Table not found _dummy_table",13196621,"it took quite some time to figure out how to install the ""information_schema"" and ""sys"" schemas (thanks to https://issues.apache.org/jira/browse/HIVE-16941) into a hive 3.1.0/3.1.1 on hdfs/hadoop 2.9.1 and I am still unsure if it is the proper way of doing it.
when I execute:


hive@hive-server ~> schematool -metaDbType derby -dbType hive -initSchema -url jdbc:hive2://localhost:10000/default -driver org.apache.hive.jdbc.HiveDriver""


I receive an error (from --verbose log):


[...]
Error: Error while compiling statement: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table (state=42000,code=40000)
org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!
[...]



It seems to be the last statement during setup of the sys-schema causes the issue. When executing it manually:



0: jdbc:hive2://localhost:10000> CREATE OR REPLACE VIEW `VERSION` AS SELECT 1 AS `VER_ID`, '3.1.0' AS `SCHEMA_VERSION`, 'Hive release version 3.1.0' AS `VERSION_COMMENT`;
Error: Error while compiling statement: FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table (state=42000,code=40000)



I have tried to switch the metastore_db from derby embedded to derby server to postgresql and made sure the changed metadatabases each worked, but setting up the information_schema and sys schemas always delivers the same error.
Executing only the select part without the create view works:


0: jdbc:hive2://localhost:10000> SELECT 1 AS `VER_ID`, '3.1.0' AS `SCHEMA_VERSION`, 'Hive release version 3.1.0' AS `VERSION_COMMENT`;
+---------+-----------------+-----------------------------+
| ver_id | schema_version | version_comment |
+---------+-----------------+-----------------------------+
| 1 | 3.1.0 | Hive release version 3.1.0 |
+---------+-----------------+-----------------------------+
1 row selected (0.595 seconds)


It seems to be related to: HIVE-19444
 
 Creating information_schema and sys schema via schematool fails with parser error",yes
13165921,"Tried creating a test that set metastore.create.as.acid/hive.create.as.insert.only, and I found that the built-in table default.src was being created as an insert-only transactional table, which will cause errors in other tests that do not set the TxnManager to one that supports transactional tables.
It appears that initDataset() uses the old CliDriver that was used for the previous test, which has any settings used during that test:

java.lang.Exception: Creating src
        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4926) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:428) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2659) [hive-exec-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2311) [hive-exec-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1982) [hive-exec-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1683) [hive-exec-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1677) [hive-exec-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:218) [hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:335) [hive-cli-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.QTestUtil.initDataset(QTestUtil.java:1277) [classes/:?]
        at org.apache.hadoop.hive.ql.QTestUtil.initDataSetForTest(QTestUtil.java:1259) [classes/:?]
        at org.apache.hadoop.hive.ql.QTestUtil.cliInit(QTestUtil.java:1328) [classes/:?]
        at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:176) [classes/:?]
        at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:104) [classes/:?]
        at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:59) [test-classes/:?]


A new CliDriver is created for the new test, but only after we've created the dataset tables for the next test (see the line numbers for QTestUtil.cliInit() in both stack traces).

CliSessionState(SessionState).getConf() line: 317	
CliDriver.<init>() line: 110	
QTestUtil.cliInit(File, boolean) line: 1360	
CoreCliDriver.runTest(String, String, String) line: 176	
CoreCliDriver(CliAdapter).runTest(String, File) line: 104	
TestMiniLlapLocalCliDriver.testCliDriver() line: 59	


I think fix is to move the creation of the new CliDriver higher up in QTestUtil.cliInit(), before we call initDataset(). 
 QTestUtil: initDataset() can be affected by the settings of the previous test",13165878,"there are a number of strange come and go failing tests; it was always strange to me that qtestutil cleans up at some questionable points - this seems to be leading to executing some commands with the previous qfiles session...
ideally the session/etc should start/reused in before
and it should be closed in after
seems like configuration is handled probably incorrectly; saving the conf after initialization - and restoring it for a new session should ensure consistency 
 Fix QTestUtil session lifecycle",yes
13193952,"Reference:
https://builds.apache.org/job/PreCommit-HDFS-Build/25322/testReport/junit/org.apache.hadoop.hdfs.server.blockmanagement/TestPendingReconstruction/testPendingAndInvalidate/
Error Message :


java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReconstruction.testPendingAndInvalidate(TestPendingReconstruction.java:457)


 
 TestPendingReconstruction.testPendingAndInvalidate fails",13190077,"currently IN extraction stops at the first level; as a result for the following query, the nested INs are not closed back.


create table t(a integer);
create table t2(b integer);

insert into t values (1),(2),(3),(4);
insert into t2 values (1),(2),(3),(4);

explain
select * from t,t2 where
	a*a=b+3
	and
	a in (1,2,3,4)
	and
	b in (1,2,3,4)

	and (
		(a in (1,2) and b in (1,2) ) or 
		(a in (2,3) and b in (2,3) )
			)
	;

 
 HivePointLookupOptimizer should extract deep cases",No
13142324,"The following error can be observed when running tests in hbase-spark module against hadoop3:


HBaseDStreamFunctionsSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: org/apache/hadoop/ipc/ExternalCall
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStorageDirs(FSNamesystem.java:1464)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs(FSNamesystem.java:1444)
  at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:939)
  at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:815)
  at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:746)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:668)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:640)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:979)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:859)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:853)
  ...
  Cause: java.lang.ClassNotFoundException: org.apache.hadoop.ipc.ExternalCall
  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStorageDirs(FSNamesystem.java:1464)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs(FSNamesystem.java:1444)
  at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:939)
  at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:815)
  at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:746)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:668)


The dependency tree shows mixture of hadoop 2.7.4 and hadoop3 for the hbase-spark module.
This should be addressed by adding proper profile in pom.xml 
 Add UT for serial replication after region merge",13142323,"The following error can be observed when running tests in hbase-spark module against hadoop3:


HBaseDStreamFunctionsSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: org/apache/hadoop/ipc/ExternalCall
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStorageDirs(FSNamesystem.java:1464)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs(FSNamesystem.java:1444)
  at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:939)
  at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:815)
  at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:746)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:668)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:640)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:979)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:859)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:853)
  ...
  Cause: java.lang.ClassNotFoundException: org.apache.hadoop.ipc.ExternalCall
  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getStorageDirs(FSNamesystem.java:1464)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNamespaceDirs(FSNamesystem.java:1444)
  at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:939)
  at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:815)
  at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:746)
  at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:668)


The dependency tree shows mixture of hadoop 2.7.4 and hadoop3 for the hbase-spark module.
This should be addressed by adding proper profile in pom.xml 
 Add UT for serial replication after region split and merge",yes
13338162,"We are seeing the following two kinds of intermittent exceptions when using S3AInputSteam:
1.


Caused by: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 156463674; received: 150001089
at com.amazonaws.thirdparty.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:178)
at com.amazonaws.thirdparty.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:181)
at java.io.DataInputStream.readFully(DataInputStream.java:195)
at java.io.DataInputStream.readFully(DataInputStream.java:169)
at org.apache.parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:779)
at org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)
at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:130)
at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:214)
at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:208)
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:63)
at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
... 15 more


2.


Caused by: javax.net.ssl.SSLException: SSL peer shut down incorrectly
at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:596)
at sun.security.ssl.InputRecord.read(InputRecord.java:532)
at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:990)
at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:948)
at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)
at com.amazonaws.thirdparty.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:137)
at com.amazonaws.thirdparty.apache.http.impl.io.SessionInputBufferImpl.read(SessionInputBufferImpl.java:198)
at com.amazonaws.thirdparty.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:176)
at com.amazonaws.thirdparty.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:135)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:180)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:107)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at com.amazonaws.services.s3.internal.S3AbortableInputStream.read(S3AbortableInputStream.java:125)
at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:82)
at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:181)
at java.io.DataInputStream.readFully(DataInputStream.java:195)
at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:70)
at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:120)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2361)
at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2493)
at org.apache.hadoop.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:82)
at cascading.tap.hadoop.io.CombineFileRecordReaderWrapper.next(CombineFileRecordReaderWrapper.java:70)
at org.apache.hadoop.mapred.lib.CombineFileRecordReader.next(CombineFileRecordReader.java:58)
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)
at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)
... 10 more


Inspired by
https://stackoverflow.com/questions/9952815/s3-java-client-fails-a-lot-with-premature-end-of-content-length-delimited-messa
 and
https://forums.aws.amazon.com/thread.jspa?threadID=83326, we got a solution that has helped us, would like to put the fix to the community version.
The problem is that S3AInputStream had a short-lived S3Object which is used to create the wrappedSteam, and this object got garbage collected at random time, which caused the stream to be closed, thus the symptoms reported.
https://github.com/aws/aws-sdk-java/blob/1.11.295/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/model/S3Object.java#L225 is the s3 code that closes the stream when S3 object is garbage collected:
Here is the code in S3AInputStream that creates temporary S3Object and uses it to create the wrappedStream:


   S3Object object = Invoker.once(text, uri,
        () -> client.getObject(request));

    changeTracker.processResponse(object, operation,
        targetPos);
    wrappedStream = object.getObjectContent();

 
 Intermittent S3AInputStream failures: Premature end of Content-Length delimited message body etc",13336044,"Stack overflow issue complaining about ConnectionClosedException during S3AInputStream close(), seems triggered by an EOF exception in abort. That is: we are trying to close the stream and it is failing because the stream is closed. oops.
https://stackoverflow.com/questions/64412010/pyspark-org-apache-http-connectionclosedexception-premature-end-of-content-leng
Looking @ the stack, we aren't translating AWS exceptions in abort() to IOEs, which may be a factor. 
 S3AInputStream to be resilient to faiures in abort(); translate AWS Exceptions",yes
13276828,"Test environment: hadoop version 3.1.0, 5 datanode
step to repo:
1: Set the ec policyRS-3-2-1024k on all of hdfs paths:
hdfs ec -setPolicy -path / RS-3-2-1024k
2.Put the small orc file into hdfs on host which is running datanode dn1:
hdfs dfs -put orcfile /tmp/testec/
3.Shut down the datanode dn1, and execute the following command to verify the orc data:
hive --orcfiledump /tmp/testec/orcfile
4. The error log should be output on the client side:


Exception in thread ""main"" org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer, not of length 974814
        at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:138)
        at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:48)
        at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
        at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
        at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:423)
        at org.apache.hadoop.hdfs.PositionStripeReader.decode(PositionStripeReader.java:74)
        at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:382)
        at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:479)
        at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1442)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1400)
        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
        at org.apache.orc.impl.RecordReaderUtils.readDiskRanges(RecordReaderUtils.java:557)
        at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readFileData(RecordReaderUtils.java:276)
        at org.apache.orc.impl.RecordReaderImpl.readAllDataStreams(RecordReaderImpl.java:1099)
        at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1055)
        at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1208)
        at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1243)
        at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:273)
        at org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:633)
        at org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:627)
        at org.apache.orc.tools.FileDump.printMetaDataImpl(FileDump.java:309)
        at org.apache.orc.tools.FileDump.printMetaData(FileDump.java:274)
        at org.apache.orc.tools.FileDump.main(FileDump.java:135)
        at org.apache.orc.tools.FileDump.main(FileDump.java:142)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:308)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:222)


 
 Erasure Coding: some ORC data can not be recovery  when partial DataNodes  are shut down",13221914,"Space Quota: Compaction is not working for super user. Compaction command is issued successfully at client but actually compaction is not happening.
In debug log below message is printed:
as an active space quota violation policy disallows compaction.
 Reference: 
https://lists.apache.org/thread.html/d09aa7abaacf1f0be9d59fa9260515ddc0c17ac0aba9cc0f2ac569bf@%3Cuser.hbase.apache.org%3E
Actually in requestCompactionInternal method of CompactSplit class ,there is no check for super user and compcations are disallowed

  RegionServerSpaceQuotaManager spaceQuotaManager =
        this.server.getRegionServerSpaceQuotaManager();
    if (spaceQuotaManager != null &&
        spaceQuotaManager.areCompactionsDisabled(region.getTableDescriptor().getTableName())) {
      String reason = ""Ignoring compaction request for "" + region +
          "" as an active space quota violation "" + "" policy disallows compactions."";
      tracker.notExecuted(store, reason);
      completeTracker.completed(store);
      LOG.debug(reason);
      return;
    }



 
 EC : Decoding is failing when block group last incomplete cell fall in to AlignedStripe",yes
13283135,"I see this test fail a lot in my environments. It also uses such a large array that it seems particularly memory wasteful and difficult to get good contention in the test as well. 
 TestSyncTimeRangeTracker fails quite easily and allocates a very expensive array.",13324237,"When the current SqlOperator is SqlCastFunction, FunctionRegistry.getFunctionInfo would return null, 
but when hive.allow.udf.load.on.demand is enabled, HiveServer2 will refer to metastore for the function definition, an exception stack trace can be seen here in HiveServer2 log:
INFO exec.FunctionRegistry: Unable to look up default.cast in metastore
org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Function @hive#default.cast does not exist)
 at org.apache.hadoop.hive.ql.metadata.Hive.getFunction(Hive.java:5495) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Registry.getFunctionInfoFromMetastoreNoLock(Registry.java:788) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Registry.getQualifiedFunctionInfo(Registry.java:657) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Registry.getFunctionInfo(Registry.java:351) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:597) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.getHiveUDF(SqlFunctionConverter.java:158) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:112) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:68) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:68) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:68) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]

So it's may be better to handle explicit cast before geting the FunctionInfo from Registry. Even if there is no cast in the query, the method handleExplicitCast returns null quickly when op.kind is not aSqlKind.CAST.
 
 SqlFunctionConverter#getHiveUDF handles cast before geting FunctionInfo",No
13187195,"A part of HBASE-20705 is still unresolved. In that Jira it was assumed that problem is: when table having both rpc & space quotas is dropped (with hbase.quota.remove.on.table.delete set as true),the rpc quota is not set to be dropped along with table-drops, and space quota was not being able to be removed completely because of the ""EMPTY"" row that rpc quota left even after removing.
The proposed solution for that was to make sure that rpc quota didn't leave empty rows after removal of quota. And setting automatic removal of rpc quota with table drops. That made sure that space quotas can be recreated/removed.
But all this was under the assumption thathbase.quota.remove.on.table.delete is set as true. When it is set as false, the same issue can reproduced. Also the below shown steps can used to reproduce the issue without table-drops.

hbase(main):005:0> create 't2','cf'
Created table t2
Took 0.7619 seconds
=> Hbase::Table - t2
hbase(main):006:0> set_quota TYPE => THROTTLE, TABLE => 't2', LIMIT => '10M/sec'
Took 0.0514 seconds
hbase(main):007:0> set_quota TYPE => SPACE, TABLE => 't2', LIMIT => '1G', POLICY => NO_WRITES
Took 0.0162 seconds
hbase(main):008:0> list_quotas
OWNER                      QUOTAS
 TABLE => t2               TYPE => THROTTLE, THROTTLE_TYPE => REQUEST_SIZE, LIMIT => 10M/sec, SCOPE =>
                           MACHINE
 TABLE => t2               TYPE => SPACE, TABLE => t2, LIMIT => 1073741824, VIOLATION_POLICY => NO_WRIT
                           ES
2 row(s)
Took 0.0716 seconds
hbase(main):009:0> set_quota TYPE => SPACE, TABLE => 't2', LIMIT => NONE
Took 0.0082 seconds
hbase(main):010:0> list_quotas
OWNER                           QUOTAS
 TABLE => t2                    TYPE => THROTTLE, THROTTLE_TYPE => REQUEST_SIZE, LIMIT => 10M/sec, SCOPE => MACHINE
 TABLE => t2                    TYPE => SPACE, TABLE => t2, REMOVE => true
2 row(s)
Took 0.0254 seconds
hbase(main):011:0> set_quota TYPE => SPACE, TABLE => 't2', LIMIT => '1G', POLICY => NO_WRITES
Took 0.0082 seconds
hbase(main):012:0> list_quotas
OWNER                           QUOTAS
 TABLE => t2                    TYPE => THROTTLE, THROTTLE_TYPE => REQUEST_SIZE, LIMIT => 10M/sec, SCOPE => MACHINE
 TABLE => t2                    TYPE => SPACE, TABLE => t2, REMOVE => true
2 row(s)
Took 0.0411 seconds

 
 Having RPC & Space quota on a table/Namespace doesn't allow space quota to be removed using 'NONE'",13212658,"Backport HBASE-21225 to branch-2.0 and branch-2.1. The specified Jira is closed, hence filing this one to track the backport. 
 Backport HBASE-21225 to branch-2.0 and branch-2.1",yes
13158584,"""You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."" 
 Arrow SerDe itest failure",13158906,"Arrow batch serializer doesn't handle null values well in complex nested data types. 
 Null value error with complex nested data type in Arrow batch serializer",yes
13318895,"Absolute Resource [memory=0] is considered as Percentage config type. This causes failure while converting queues from Percentage to Absolute Resources automatically. 
Repro:
1. Queue A = 100% and child queues Queue A.B = 0%, A.C=100%
2. While converting above to absolute resource automatically, capacity of queue A = [memory=<cluster resource>], A.B = [memory=0]
This fails with below as A is considered as Absolute Resource whereas B is considered as Percentage config type.


2020-07-23 09:36:40,499 WARN org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices: CapacityScheduler configuration validation failed:java.io.IOException: Failed to re-init queues : Parent queue 'root.A' and child queue 'root.A.B' should use either percentage based capacityconfiguration or absolute resource together for label:


 
 Absolute Resource [memory=0] is considered as Percentage config type",13244618,"in deleteSubtree(path), the DynamoDB metastore walks down the tree, returning elements to delete. But it will delete parent entries before children, so if an operation fails partway through, there will be orphans
Better: DescendantsIterator to return all the leaf nodes before their parents so the deletion is done bottom up
Also: push the deletions off into their own async queue/pool so that they don't become the bottleneck on the process 
 DynamoDBMetaStore deleteSubtree to delete leaf nodes first",No
13185672,"In JDK10, sun.net.dns.ResolverConfiguration is encapsulated and not accessible from unnamed modules. This issue is to remove the usage of ResolverConfiguration. 
 [JDK10] Migrate from sun.net.dns.ResolverConfiguration to the replacement",13179538,"


public boolean isTableState(TableName tableName, TableState.State... states) {
 try {
 TableState tableState = getTableState(tableName);
 return tableState.isInStates(states);
 } catch (IOException e) {
 LOG.error(""Unable to get table "" + tableName + "" state"", e);
 // XXX: is it safe to just return false here?
 return false;
 }
 }




When cannot get table state, returning false is not always safe or correct. 
 Improve isTableState() method to ensure caller gets correct info",No
13216287,"The issue is there is no HttpRequestDecoder in InboundHandler of netty, appearunexpected message type when read message.

 
DEBUG org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Proxy failed. Cause:
 com.xiaomi.infra.thirdparty.io.netty.handler.codec.EncoderException: java.lang.IllegalStateException: unexpected message type: PooledUnsafeDirectByteBuf
 at com.xiaomi.infra.thirdparty.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106)
 at com.xiaomi.infra.thirdparty.io.netty.channel.CombinedChannelDuplexHandler.write(CombinedChannelDuplexHandler.java:348)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:816)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:723)
 at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:304)
 at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.flush(ChunkedWriteHandler.java:137)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:802)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:831)
 at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1051)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:300)
 at org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder.channelRead(SimpleHttpProxyHandler.java:80)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
 at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1414)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
 at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:945)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:146)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
 at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886)
 at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.lang.Thread.run(Thread.java:745)
 Caused by: java.lang.IllegalStateException: unexpected message type: PooledUnsafeDirectByteBuf
 at com.xiaomi.infra.thirdparty.io.netty.handler.codec.http.HttpObjectEncoder.encode(HttpObjectEncoder.java:123)
 at com.xiaomi.infra.thirdparty.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88)
 ... 30 more
 2018-12-04,14:23:28,690 DEBUG org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Proxy failed. Cause:
 java.nio.channels.ClosedChannelException
 at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:188)
 at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:198)
 at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.flush(ChunkedWriteHandler.java:137)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:802)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:831)
 at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1051)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:300)
 at org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder.channelRead(SimpleHttpProxyHandler.java:80)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
 at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1414)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
 at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
 at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:945)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:146)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
 at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
 at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886)
 at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.lang.Thread.run(Thread.java:745)
 2018-12-04,14:23:28,690 DEBUG org.mortbay.log: EOF
Unexpected message type: PooledUnsafeDirectByteBuf when get datanode infobyDatanodeWebHdfsMethods.
 
 Unexpected message type: PooledUnsafeDirectByteBuf when get datanode info by DatanodeWebHdfsMethods",13216285,"DEBUG org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Proxy failed. Cause:
com.xiaomi.infra.thirdparty.io.netty.handler.codec.EncoderException: java.lang.IllegalStateException: unexpected message type: PooledUnsafeDirectByteBuf
at com.xiaomi.infra.thirdparty.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106)
at com.xiaomi.infra.thirdparty.io.netty.channel.CombinedChannelDuplexHandler.write(CombinedChannelDuplexHandler.java:348)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:730)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:816)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:723)
at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:304)
at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.flush(ChunkedWriteHandler.java:137)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:802)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:831)
at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1051)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:300)
at org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder.channelRead(SimpleHttpProxyHandler.java:80)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1414)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:945)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:146)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886)
at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unexpected message type: PooledUnsafeDirectByteBuf
at com.xiaomi.infra.thirdparty.io.netty.handler.codec.http.HttpObjectEncoder.encode(HttpObjectEncoder.java:123)
at com.xiaomi.infra.thirdparty.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88)
... 30 more
2018-12-04,14:23:28,690 DEBUG org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Proxy failed. Cause:
java.nio.channels.ClosedChannelException
at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:188)
at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.doFlush(ChunkedWriteHandler.java:198)
at com.xiaomi.infra.thirdparty.io.netty.handler.stream.ChunkedWriteHandler.flush(ChunkedWriteHandler.java:137)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:776)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:802)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:831)
at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1051)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:300)
at org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder.channelRead(SimpleHttpProxyHandler.java:80)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1414)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
at com.xiaomi.infra.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
at com.xiaomi.infra.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:945)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:146)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
at com.xiaomi.infra.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:886)
at com.xiaomi.infra.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Thread.java:745)
2018-12-04,14:23:28,690 DEBUG org.mortbay.log: EOF
Unexpected message type: PooledUnsafeDirectByteBuf when get datanode infobyDatanodeWebHdfsMethods.
Because no HttpRequestDecoder in InboundHandler of netty, appearunexpected message type when read message. 
 Unexpected message type: PooledUnsafeDirectByteBuf when get datanode info by DatanodeWebHdfsMethods",yes
13152600,"Upgrade to Hadoop 3.1.0 
 Upgrade  to Hadoop 3.1.0",13153641,"Given that Hadoop 3.1.0 has been released, we need to upgrade hadoop.version to 3.1.0. This change is requiredforHIVE-18037sinceit depends on YARN Service which had its first release in 3.1.0 (and is non-existent in 3.0.0). 
 Upgrade hadoop.version to 3.1.0",yes
13208073,"hdfs ec -verifyClusterSetup command verifies if there are enough data nodes and racks for the enabled erasure coding policies
I think it would be beneficial if it could accept an erasure coding policy as a parameter optionally. For example the following commandwould run the verify for only the RS-6-3-1024k policy.


hdfs ec -verifyClusterSetup -policy RS-6-3-1024k

 
 Port YARN-7033 NM recovery of assigned resources to branch-3.0/branch-2",13139830,"org.apache.hadoop.hive.cli.control.TestDanglingQOuts 
 Remove unconnected q.out-s",No
13262271,"Spark rely on the TokenUtil.obtainToken(conf) API which is removed in HBase-2.0, though it has been fixed in SPARK-26432 to use the new API but planned for Spark-3.0, hence we need the fix in HBase until they release it and we upgrade it


18/03/20 20:39:07 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.hbase.HBaseIOException: com.google.protobuf.ServiceException: Error calling method hbase.pb.AuthenticationService.GetAuthenticationToken
org.apache.hadoop.hbase.HBaseIOException: com.google.protobuf.ServiceException: Error calling method hbase.pb.AuthenticationService.GetAuthenticationToken
        at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:360)
        at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:346)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:86)
        at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:121)
        at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:118)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:313)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:118)
        at org.apache.hadoop.hbase.security.token.TokenUtil.addTokenForJob(TokenUtil.java:272)
        at org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:533)
        at org.apache.hadoop.hbase.spark.HBaseContext.<init>(HBaseContext.scala:73)
        at org.apache.hadoop.hbase.spark.JavaHBaseContext.<init>(JavaHBaseContext.scala:46)
        at org.apache.hadoop.hbase.spark.example.hbasecontext.JavaHBaseBulkDeleteExample.main(JavaHBaseBulkDeleteExample.java:64)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:706)
Caused by: com.google.protobuf.ServiceException: Error calling method hbase.pb.AuthenticationService.GetAuthenticationToken
        at org.apache.hadoop.hbase.client.SyncCoprocessorRpcChannel.callBlockingMethod(SyncCoprocessorRpcChannel.java:71)
        at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:81)
        ... 17 more


 
 Yarn unable to acquire delegation token for HBase Spark jobs",13195331,"I am try using hadoop distcp command to copy a file (000079_0) to a directory (target_directory/part_date=2018-10-28/), and the directory is not exist, like this


$ hadoop fs -ls /user/hive/warehouse/migration_chang.db/target_directory/
$
$ hadoop distcp hdfs://sdg/user/hive/warehouse/migration_chang.db/source_directory/part_date=2018-10-28/000079_0 hdfs://sdg/user/hive/warehouse/migration_chang.db/target_directory/part_date=2018-10-28/


It will copy the source file '000079_0' to a file called ""part_date=2018-10-28"".


$ hadoop fs -ls /user/hive/warehouse/migration_chang.db/target_directory/
Found 1 items
-rw-r--r-- 3 hadoop supergroup 353024605 2018-10-31 19:51 /user/hive/warehouse/migration_chang.db/snda_game_user_profile_mid_5/part_date=2018-10-28

I think itis confusing, and better way is remind error like ""No such directory"" .('hadoop fs -cp' command or Linux 'cp' command do like this way.)

 
  hadoop distcp command fail without reminder",No
13222377,"Jackson-databind 2.9.8 has a few fixes which are important to include. 
 Upgrade Jackson-databind version to 2.9.8",13203378,"Now Jackson 2.9.5 is used and it is vulnerable (CVE-2018-11307). Let's upgrade to the latest version. 
 Upgrade Jackson2 to 2.9.8",yes
13189749,"Reference
https://builds.apache.org/job/hadoop-qbt-trunk-java8-linux-x86/916/testReport/junit/org.apache.hadoop.fs.contract.router.web/TestRouterWebHDFSContractAppend/testRenameFileBeingAppended/ 
 RBF: TestRouterWebHDFSContractAppend fails with No Active Namenode under nameservice",13171862,"
Remove code
Remove exception handling
Remove printStackTrace call

 
 Simplify StringSubstrColStart Initialization",No
13222569,"Bump guava to 24.1.1 
 Preparation for bumping guava version",13197004,"

        } else {
          assert (!rqst.isSetSrcTxnToWriteIdList());
          assert (rqst.isSetTxnIds());
          txnIds = rqst.getTxnIds();
        }

        Collections.sort(txnIds); //easier to read logs and for assumption done in replication flow


when the input comes from


  @Override
  public long allocateTableWriteId(long txnId, String dbName, String tableName) throws TException {
    return allocateTableWriteIdsBatch(Collections.singletonList(txnId), dbName, tableName).get(0).getWriteId();
  }




java.lang.UnsupportedOperationException: null
    at java.util.AbstractList.set(AbstractList.java:132) ~[?:1.8.0]
    at java.util.AbstractList$ListItr.set(AbstractList.java:426) ~[?:1.8.0]
    at java.util.Collections.sort(Collections.java:170) ~[?:1.8.0]
    at org.apache.hadoop.hive.metastore.txn.TxnHandler.allocateTableWriteIds(TxnHandler.java:1523) ~[hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.allocate_table_write_ids(HiveMetaStore.java:7349) ~[hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]


 
 TxnHandler: sort() called on immutable lists",No
13322743,"Move hbase-operator-tools GitHub Pull Request build to ci-hadoop Jenkins master.
The new job should be based on a Jenkinsfile similarly to the main repository. 
 Migrate hbase-operator-tools testing to ci-hadoop",13155987,"This could be limited to assert exceptions; but might interfere with other exceptions...discovered while ""fixing"" testreopt after HIVE-19269


create table tu(id_uv int,id_uw int,u int);
create table tv(id_uv int,v int);
create table tw(id_uw int,w int);

insert into tu values (10,10,10),(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5),(6,6,6);
insert into tv values (10,10),(1,1),(2,2),(3,3);
insert into tw values (10,10),(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9);

set zzz=0;
set hive.vectorized.execution.enabled=false;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- fails as expected

set hive.vectorized.execution.enabled=true;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- there is a result set

 
 Vectorization: assert_true HiveException erroneously gets suppressed to NULL",No
13191001,"I just found a testConcurrentTableCreations DDB table lurking in a region, presumably from an interrupted test. Luckily test/resources/core-site.xml forces the r/w capacity to be 10, but it could still run up bills.
Recommend

explicitly set capacity = 1 for the test
and add comments in the testing docs about keeping cost down.

I think we may also want to make this a scale-only test, so it's run less often 
 TestJdbcWithMiniLlapArrow.testKillQuery fail frequently",13177243,"TestMiniDruidCliDriver is failing intermittently a significant percentage of the time.
druid_timestamptz
druidmini_joins
druidmini_masking
druidmini_test1 
 FlakyTest: TestMiniDruidCliDriver",No
13129033,"As mentioned by Arun Suresh, SELF means target allocation tag same as allocation tag of the scheduling request itself. So this is not a new type for sure, it is still ALLOCATION_TAG type.
If we really want this functionality, we can build this in PlacementConstraints, but I'm doubtful about this since copying allocation tags from source is just a trivial work.
 
 Remove SELF from TargetExpression type",13193952,"Reference:
https://builds.apache.org/job/PreCommit-HDFS-Build/25322/testReport/junit/org.apache.hadoop.hdfs.server.blockmanagement/TestPendingReconstruction/testPendingAndInvalidate/
Error Message :


java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReconstruction.testPendingAndInvalidate(TestPendingReconstruction.java:457)


 
 TestPendingReconstruction.testPendingAndInvalidate fails",No
13156416,"Terasort is very slow on S3, because it still uses the classic rename-to-commit algorithm on the sort, even while teragen and the reporting can use the new committer
Reason: org.apache.hadoop.examples.terasort.TeraOutputFormathas overriden getOutputCommitter even though it doesn't need to. 
 Terasort on S3A to switch to new committers",13218533,"TeraGen is hard-coded to use FileOutputCommitter to commit the job. This patch is to allow TeraGen to use schema-specific committer for optimization. For example, using S3A committers for s3a:// object storage. 
 Allow TeraGen to use schema-specific output committer",yes
13258068,"We are using Jackson to emit JSON in at least one place in common and client. We don't need all of Jackson and all the associated trouble just to do that. Use a suitably licensed JSON library with no known vulnerability. This will avoid problems downstream because we are trying to avoid having them pull in a vulnerable Jackson via us so Jackson is a 'provided' scope transitive dependency of client and its in-project dependencies (like common). 
Here's where I am referring to:
org.apache.hadoop.hbase.util.JsonMapper.<clinit>(JsonMapper.java:37)
       at org.apache.hadoop.hbase.client.Operation.toJSON(Operation.java:70)
       at org.apache.hadoop.hbase.client.Operation.toString(Operation.java:96) 
 Replace use of Jackson for JSON serde in hbase common and client modules",13256025,"HBASE-22728 moved out jackson transitive dependencies. mostly good, but moving jackson2 to provided in hbase-server broke few things
testing-util needs a transitive jackson 2 in order to start the minicluster, currently fails with CNFE for com.fasterxml.jackson.databind.ObjectMapper when trying to initialize the master.
shaded-testing-util needs a relocated jackson 2 for the same reason
it's not used for any of the mapreduce stuff in hbase-server, so hbase-shaded-server for that purpose should be fine. But it is used by WALPrettyPrinter and some folks might expect that to work from that artifact since it is present. 
 Replace Jackson with relocated gson everywhere but hbase-rest",yes
13149508,"start-ozone.sh calls start-dfs.sh to start the NN and DN in a ozone cluster. Starting of datanode fails because of incomplete classpaths as datanode is unable to load all the plugins.
Setting the class path to the following values does resolve the issue:


export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/ozone/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/hdsl/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/ozone/lib/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/hdsl/lib/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/cblock/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/cblock/lib/*

 
 Ozone: start-ozone.sh fail to start datanode because of incomplete classpaths",13140523,"When git format-patch a patch, a generated *.patch file will be commited with real code changes easily.
Update .gitignore file which ignores *.patch will improve the git workflow 
 update  .gitignore file to ignores *.patch file",No
13226324,"So it does not confuse developers... 
 Add an import order template under the dev-support directory",13173286,"Making it consistent with our checkstyle requirments.


 <module name=""Indentation"">
      <property name=""basicOffset"" value=""2""/>
      *<property name=""caseIndent"" value=""2""/>*
      <property name=""throwsIndent"" value=""2""/>
      <property name=""arrayInitIndent"" value=""2""/>
      <property name=""lineWrappingIndentation"" value=""2""/>
    </module>

 
 Add import order config in dev support for eclipse",yes
13241405,"  使用 HDFS OEV 命令处理 edits 文件,从xml格式的文件转成成binary格式的文件的时候,当xml文件中存在 OP_TRUNCATE 操作并且含有子 BLOCK的时候报错
  我追踪了源码发现blockFromXml 方法需要的是一个block 对象但是被传递的是一个 data对象,我已经解决了这个问题,并且验证了可用性,
  When edits files are processed with HDFS OEV command and converted from XML files to binary files, errors are reported when OP_TRUNCATE operation exists in XML files and BLOCKs are included.
  I tracked the source code and found that the blockFromXml method needed a block object but was passed a data object. I have solved this problem and verified its availability.
  I will submit a patch for this as soon as possible.
   
 
 OEV tool fails to read edit xml file if OP_TRUNCATE has  BLOCK tag",13261993,"my namenode metadata getting fullwhat is the solution 
 Release 2.2.2",No
13198035,"A vendor might need a customized scheduling policy for their devices. It could be scheduled based on topology, resource utilization, virtualization, device attribute and so on.
We'll provide another optional interface ""DevicePluginScheduler"" for the vendor device plugin to implement. Once it's implemented, the framework will prefer it to the default scheduler.
Thiswould bring more flexibility to the framework's scheduling mechanism. 
 DataNode runs async disk checks  maybe  throws NullPointerException In ThrottledAsyncChecker.java  ",13198048,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
   this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
   if (completedChecks.containsKey(target))
{ 

   // here may be happen garbage collection，and result may be null.

   final LastCheckResult<V> result = completedChecks.get(target);     

   final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;  

   。。。。

}

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.
the solution is：
this.completedChecks = new ReferenceMap(1, 1);
or
 this.completedChecks = new HashMap<>();
 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",yes
13130384,"Scenario:
Run ""yarn app -destroy"" cli with a application name which does not exist.
Here, The cli should return a message "" Application does not exists"" instead it is returning a message ""Destroyed cluster httpd-xxx""
 
 Fix logging for destroy yarn service cli when app does not exist and some minor bugs",13128921,"steps:

Enable yarn containerization in cluster
Launch httpd example.

httpd.json and httpd-proxy.conf file are present at <yarn client home>/yarn-service-examples/httpd


[hrt_qa@xxx httpd]$ ls -la
total 8
drwxr-xr-x. 2 root root   46 Jan  5 02:52 .
drwxr-xr-x. 5 root root   51 Jan  5 02:52 ..
-rw-r--r--. 1 root root 1337 Jan  1 04:21 httpd.json
-rw-r--r--. 1 root root 1065 Jan  1 04:21 httpd-proxy.conf



[hrt_qa@xxx yarn-service-examples]$ yarn app -launch httpd-hrtqa httpd/httpd.json
WARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.
WARNING: YARN_LOGFILE has been replaced by HADOOP_LOGFILE. Using value of YARN_LOGFILE.
WARNING: YARN_PID_DIR has been replaced by HADOOP_PID_DIR. Using value of YARN_PID_DIR.
WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.
18/01/05 20:39:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/01/05 20:39:23 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
18/01/05 20:39:23 INFO client.ServiceClient: Loading service definition from local FS: /xxx/yarn-service-examples/httpd/httpd.json
Exception in thread ""main"" java.lang.IllegalArgumentException: Src_file does not exist for config file: httpd-proxy.conf
	at org.apache.hadoop.yarn.service.provider.AbstractClientProvider.validateConfigFiles(AbstractClientProvider.java:105)
	at org.apache.hadoop.yarn.service.utils.ServiceApiUtil.validateComponent(ServiceApiUtil.java:224)
	at org.apache.hadoop.yarn.service.utils.ServiceApiUtil.validateAndResolveService(ServiceApiUtil.java:189)
	at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:213)
	at org.apache.hadoop.yarn.service.client.ServiceClient.actionLaunch(ServiceClient.java:204)
	at org.apache.hadoop.yarn.client.cli.ApplicationCLI.run(ApplicationCLI.java:447)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
	at org.apache.hadoop.yarn.client.cli.ApplicationCLI.main(ApplicationCLI.java:111)
 
 Improve error logging for ""java.lang.IllegalArgumentException: Src_file does not exist for config file: httpd-proxy.conf""",yes
13198047,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13188076,"When use w2rRatio compute fair share, there may be a chancetriggering the problem of Int overflow, andentering an infinite loop.
Since thecomputeshare thread holds the writeLock, it mayblocking scheduling thread.
This issue occurs in a production environment. And we have already fixed it.

added 2018-10-29:elaborate the problem
/**

Compute the resources that would be used given a weight-to-resource ratio
w2rRatio, for use in the computeFairShares algorithm as described in #
 */
 private static int resourceUsedWithWeightToResourceRatio(double w2rRatio,
 Collection<? extends Schedulable> schedulables, String type) { int resourcesTaken = 0; for (Schedulable sched : schedulables) 
{ int share = computeShare(sched, w2rRatio, type); resourcesTaken += share; }
 return resourcesTaken;
 }

The variable resourcesTaken is an integer type. Andit also isaccumulated value ofresult of
computeShare(Schedulable sched, double w2rRatio,String type) whichis a value between the min share and max share of a queue.
For example, when there are 3 queues, each has min share= max share=
Integer.MAX_VALUE, theresourcesTaken will be out of Integer bound, and it will be anegative number.
when resourceUsedWithWeightToResourceRatio(double w2rRatio, Collection<? extends Schedulable> schedulables, String type) return anegative number, the loop in
computeSharesInternal() may never out which got thescheduler lock.

//org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares
while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)
 < totalResource)

{ rMax *= 2.0; }

Thismayblocking scheduling thread. 
 Avoid potential integer overflow when computing fair shares",No
13165239,"
When multiple views are used along with union all, it is resulting in the following error when dynamic partition pruning is enabled in tez.



Exception in thread ""main"" java.lang.AssertionError: No work found for tablescan TS[8]
 at org.apache.hadoop.hive.ql.parse.GenTezUtils.processAppMasterEvent(GenTezUtils.java:408)
 at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:383)
 at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:205)
 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10371)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:208)
 at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:239)
 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)
 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:347)
 at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1203)
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1257)
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1140)
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1130)
 at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
 at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:204)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:433)
 at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:894)
 at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:825)
 at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:726)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


Steps to reproduce:
set hive.execution.engine=tez;
set hive.tez.dynamic.partition.pruning=true;
CREATE TABLE t1(key string, value string, c_int int, c_float float, c_boolean boolean) partitioned by (dt string);
CREATE TABLE t2(key string, value string, c_int int, c_float float, c_boolean boolean) partitioned by (dt string);
CREATE TABLE t3(key string, value string, c_int int, c_float float, c_boolean boolean) partitioned by (dt string);

insert into table t1 partition(dt='2018') values ('k1','v1',1,1.0,true);
insert into table t2 partition(dt='2018') values ('k2','v2',2,2.0,true);
insert into table t3 partition(dt='2018') values ('k3','v3',3,3.0,true);

CREATE VIEW `view1` AS select `t1`.`key`,`t1`.`value`,`t1`.`c_int`,`t1`.`c_float`,`t1`.`c_boolean`,`t1`.`dt` from `t1` union all select `t2`.`key`,`t2`.`value`,`t2`.`c_int`,`t2`.`c_float`,`t2`.`c_boolean`,`t2`.`dt` from `t2`;
CREATE VIEW `view2` AS select `t2`.`key`,`t2`.`value`,`t2`.`c_int`,`t2`.`c_float`,`t2`.`c_boolean`,`t2`.`dt` from `t2` union all select `t3`.`key`,`t3`.`value`,`t3`.`c_int`,`t3`.`c_float`,`t3`.`c_boolean`,`t3`.`dt` from `t3`;
create table t4 as select key,value,c_int,c_float,c_boolean,dt from t1 union all select v1.key,v1.value,v1.c_int,v1.c_float,v1.c_boolean,v1.dt from view1 v1 join view2 v2 on v1.dt=v2.dt;
CREATE VIEW `view3` AS select `t4`.`key`,`t4`.`value`,`t4`.`c_int`,`t4`.`c_float`,`t4`.`c_boolean`,`t4`.`dt` from `t4` union all select `t1`.`key`,`t1`.`value`,`t1`.`c_int`,`t1`.`c_float`,`t1`.`c_boolean`,`t1`.`dt` from `t1`;

select count(0) from view2 v2 join view3 v3 on v2.dt=v3.dt; // ThrowsNo work found for tablescan error 
 Dynamic partition pruning in Tez is leading to 'No work found for tablescan' error",13247248,"Add the following configuration

error:


//
[2019-07-17T11:50:21.048+08:00] [INFO] [Edit log tailer] : replaying edit log: 1/20512836 transactions completed. (0%) [2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Edits file http://ip/getJournal?jid=ns1003&segmentTxId=232056426162&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?ipjid=ns1003&segmentTxId=232056426162&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?ipjid=ns1003&segmentTxId=232056426162&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH of size 3126782311 edits # 500 loaded in 3 seconds [2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@51ceb7bc expecting start txid #232056752162 [2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Start loading edits file http://ip/getJournal?ipjid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?ipjid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?ipjid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH maxTxnipsToRead = 500 [2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Fast-forwarding stream 'http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?ipjid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?ipjid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH' to transaction ID 232056751662 [2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] ip: Fast-forwarding stream 'http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH' to transaction ID 232056751662 [2019-07-17T11:50:21.061+08:00] [ERROR] [Edit log tailer] : Unknown error encountered while tailing edits. Shutting down standby NN. java.io.IOException: There appears to be a gap in the edit log. We expected txid 232056752162, but got txid 232077264498. at org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt(MetaRecoveryContext.java:94) at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:239) at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:161) at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:895) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:321) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:460) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:410) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:427) at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:414) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:423) [2019-07-17T11:50:21.064+08:00] [INFO] [Edit log tailer] : Exiting with status 1 [2019-07-17T11:50:21.066+08:00] [INFO] [Thread-1] : SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode atip ************************************************************/




if dfs.ha.tail-edits.max-txns-per-lock value is 500,when the namenode load the editlog util 500,the current namenode will load the next editlog,but editlog more than 500.So,namenode got an unexpected txid when tail editlog.




//
[2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Edits file http://ip/getJournal?jid=ns1003&segmentTxId=232056426162&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?jid=ns1003&segmentTxId=232056426162&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?jid=ns1003&segmentTxId=232056426162&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH of size 3126782311 edits # 500 loaded in 3 seconds
[2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@51ceb7bc expecting start txid #232056752162
[2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Start loading edits file http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH maxTxnsToRead = 500
[2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Fast-forwarding stream 'http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH, http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH' to transaction ID 232056751662
[2019-07-17T11:50:21.059+08:00] [INFO] [Edit log tailer] : Fast-forwarding stream 'http://ip/getJournal?jid=ns1003&segmentTxId=232077264498&storageInfo=-63%3A1902204348%3A0%3ACID-hope-20180214-20161018-SQYH' to transaction ID 232056751662
[2019-07-17T11:50:21.061+08:00] [ERROR] [Edit log tailer] : Unknown error encountered while tailing edits. Shutting down standby NN.
java.io.IOException: There appears to be a gap in the edit log.  We expected txid 232056752162, but got txid 232077264498.


Read data from JN twice in the same second,changedsegmentTxId,finally quit because of txid mismatch. 
 [HBCK2] Add more log for hbck operations at master side",No
13155200,"Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches. 
 Arrow batch serializer",13158906,"Arrow batch serializer doesn't handle null values well in complex nested data types. 
 Null value error with complex nested data type in Arrow batch serializer",yes
13289579,"The parent issue upped the parallelism of the flakey reruns – see the second panel on this page https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2/lastSuccessfulBuild/artifact/dashboard.html This upped the flakie list length. It also may have overcommitted the test-running machine. Let me down the rate to something more mild. 
 [Flakey Tests] Down the flakies re-run ferocity; it makes for too many fails.",13231874,"We refactored access checking so other components can reuse permissions checks formerly encapsulated by the AccessController coprocessor. The new API is AccessChecker, committed as far back as branch-1.4. This should be backported to branch-1.3 as well so any potential user of AccessChecker can address changes and fixes for HBase versions 1.3 and up.  
 Backport AccessChecker refactor to branch-1.3 ",No
13127992,"According to LogCleaner extends CleanerChore<BaseLogCleanerDelegate>, TimeToLiveLogCleaner should extends BaseLogCleanerDelegate instead of BaseFileCleanerDelegate 
 TimeToLiveProcedureWALCleaner should extends BaseLogCleanerDelegate",13127990,"According to LogCleaner extends CleanerChore<BaseLogCleanerDelegate>, TimeToLiveLogCleaner should extends BaseLogCleanerDelegate instead of BaseFileCleanerDelegate 
 TimeToLiveProcedureWALCleaner should extends BaseLogCleanerDelegate",yes
13284871,"
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA.testObserverReadProxyProviderWithDT(TestDelegationTokensWithHA.java:156)
 
 Fix TestDelegationTokensWithHA",13283312,"The unit TestDelegationTokensWithHA.testObserverReadProxyProviderWithDT is failing consistently. Seems this is due to a log message change. We should fix it. 
 TestDelegationTokensWithHA.testObserverReadProxyProviderWithDT fails intermittently",yes
13136371,"Error is:


java.lang.AssertionError: found exception: java.lang.NullPointerException via CODE-BUG: Uncaught runtime exception: pid=154, state=RUNNABLE:SPLIT_TABLE_REGION_CREATE_DAUGHTER_REGIONS; SplitTableRegionProcedure table=testRecoveryAndDoubleExecution, parent=3d8d459ba395c2cf6b1e5c71aca92cfd, daughterA=c6531c10effa8e542159ab82a87bd75e, daughterB=ee34a9af88273b6c06e1a688fc50ed6e:java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.master.assignment.TestSplitTableRegionProcedure.testRecoveryAndDoubleExecution(TestSplitTableRegionProcedure.java:411)

Exception from the output file:


2018-02-05 18:00:48,205 ERROR [PEWorker-1] procedure2.ProcedureExecutor(1480): CODE-BUG: Uncaught runtime exception: pid=19, state=RUNNABLE:SPLIT_TABLE_REGION_CREATE_DAUGHTER_REGIONS; SplitTableRegionProcedure table=testSplitWithoutPONR, parent=57114194fb486a3988b232bcf10eb177, daughterA=749aa83c03b8f7c6b642cd73c5b51e43, daughterB=a53ec69e8dd2cfa6c0be2b9a7eb271bb
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.splitStoreFiles(SplitTableRegionProcedure.java:617)
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.createDaughterRegions(SplitTableRegionProcedure.java:541)
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.executeFromState(SplitTableRegionProcedure.java:241)
	at org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.executeFromState(SplitTableRegionProcedure.java:89)
	at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:180)
	at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:845)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:1455)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.executeProcedure(ProcedureExecutor.java:1224)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$800(ProcedureExecutor.java:78)
	at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$WorkerThread.run(ProcedureExecutor.java:1734)

Value of 'htd' is null asitis initialized in the constructor but when the object is deserialized its null.
 
 TestSplitTableRegionProcedure#testSplitWithoutPONR() and testRecoveryAndDoubleExecution() are failing with NPE",13288079,"We have a cluster with big nodes running lots of Docker containers.
When the NM was restarted and certain Docker containers were reacquired, their exit code files are not generated within 2 secs which is the timeout value for this process. Let's make this configurable, so we could wait a little bit longer. 
 Make pid file generation timeout configurable in case of reacquired container ",No
13224805,"Running the attached q file the result is not what is expected: the rollup group by missing the one summary row. The summary row should be there even if the source dataset is empty. 
 Group by rollup returns with empty result when the grouped dataset is empty",13327655,"There is a constructor of PathLocation as follows, it's for creating a new PathLocation with a prioritised nsId.



public PathLocation(PathLocation other, String firstNsId) {
  this.sourcePath = other.sourcePath;
  this.destOrder = other.destOrder;
  this.destinations = orderedNamespaces(other.destinations, firstNsId);
}


When I was reading the code ofMultipleDestinationMountTableResolver, I thought this constructor was to create a PathLocation with an override destination. It took me a while before I realize this is a constructor to sort the destinations inside.
Maybe I think this constructor can be more clear about its usage?
 
 [Flaky Test] TestReplicationDisableInactivePeer#testDisableInactivePeer",No
13305961,"   At present, when we execute `hdfs oev -p stats -i edits -o edits.stats`, the output format is as follows, and all the opcodes will be output once.
VERSION : -65
 OP_ADD ( 0): 2
 OP_RENAME_OLD ( 1): 2
 OP_DELETE ( 2): 0
 OP_MKDIR ( 3): 5
 OP_SET_REPLICATION ( 4): 0
 OP_DATANODE_ADD ( 5): 0
 OP_DATANODE_REMOVE ( 6): 0
 OP_SET_PERMISSIONS ( 7): 4
 OP_SET_OWNER ( 8): 1
 OP_CLOSE ( 9): 2
 OP_SET_GENSTAMP_V1 ( 10): 0
 OP_SET_NS_QUOTA ( 11): 0
 OP_CLEAR_NS_QUOTA ( 12): 0
 OP_TIMES ( 13): 0
 OP_SET_QUOTA ( 14): 0
 OP_RENAME ( 15): 0
 OP_CONCAT_DELETE ( 16): 0
 OP_SYMLINK ( 17): 0
 OP_GET_DELEGATION_TOKEN ( 18): 0
 OP_RENEW_DELEGATION_TOKEN ( 19): 0
 OP_CANCEL_DELEGATION_TOKEN ( 20): 0
 OP_UPDATE_MASTER_KEY ( 21): 0
 OP_REASSIGN_LEASE ( 22): 0
 OP_END_LOG_SEGMENT ( 23): 1
 OP_START_LOG_SEGMENT ( 24): 1
 OP_UPDATE_BLOCKS ( 25): 0
 OP_CREATE_SNAPSHOT ( 26): 0
 OP_DELETE_SNAPSHOT ( 27): 0
 OP_RENAME_SNAPSHOT ( 28): 0
 OP_ALLOW_SNAPSHOT ( 29): 0
 OP_DISALLOW_SNAPSHOT ( 30): 0
 OP_SET_GENSTAMP_V2 ( 31): 2
 OP_ALLOCATE_BLOCK_ID ( 32): 2
 OP_ADD_BLOCK ( 33): 2
 OP_ADD_CACHE_DIRECTIVE ( 34): 0
 OP_REMOVE_CACHE_DIRECTIVE ( 35): 0
 OP_ADD_CACHE_POOL ( 36): 0
 OP_MODIFY_CACHE_POOL ( 37): 0
 OP_REMOVE_CACHE_POOL ( 38): 0
 OP_MODIFY_CACHE_DIRECTIVE ( 39): 0
 OP_SET_ACL ( 40): 0
 OP_ROLLING_UPGRADE_START ( 41): 0
 OP_ROLLING_UPGRADE_FINALIZE ( 42): 0
 OP_SET_XATTR ( 43): 0
 OP_REMOVE_XATTR ( 44): 0
 OP_SET_STORAGE_POLICY ( 45): 0
 OP_TRUNCATE ( 46): 0
 OP_APPEND ( 47): 0
 OP_SET_QUOTA_BY_STORAGETYPE ( 48): 0
 OP_ADD_ERASURE_CODING_POLICY ( 49): 0
 OP_ENABLE_ERASURE_CODING_POLIC ( 50): 0
 OP_DISABLE_ERASURE_CODING_POLI ( 51): 0
 OP_REMOVE_ERASURE_CODING_POLIC ( 52): 0
 OP_INVALID ( -1): 0
But in general, the edits file we parse does not involve all the operation codes. If all the operation codes are output, it is unfriendly for the cluster administrator to view the output.
  we usually only care about what opcodes appear in the edits file.We can output the opcodes that appeared in the edits file and sort them.
For example, we can execute the following command：
hdfs oev -p stats -i edits_0000000000000001321-0000000000000001344 -sort -o edits.stats -v
The output format is as follows：
VERSION : -65
 OP_MKDIR ( 3): 5
 OP_SET_PERMISSIONS ( 7): 4
 OP_ADD ( 0): 2
 OP_RENAME_OLD ( 1): 2
 OP_CLOSE ( 9): 2
 OP_SET_GENSTAMP_V2 ( 31): 2
 OP_ALLOCATE_BLOCK_ID ( 32): 2
 OP_ADD_BLOCK ( 33): 2
 OP_SET_OWNER ( 8): 1
 OP_END_LOG_SEGMENT ( 23): 1
 OP_START_LOG_SEGMENT ( 24): 1 
 Support sort the output according to the number of occurrences of the opcode for StatisticsEditsVisitor",13291672,"See this discussion on mailing list
https://lists.apache.org/thread.html/rbab2e79f25b38c85fe205390b3fdbc6711984881a994a499c54aee97%40%3Cdev.hbase.apache.org%3E 
 NettyRpcClientConfigHelper will not share event loop by default which is incorrect",No
13192602,"Change the log message ""START"" to ""END"" in some test cases. 
 Fix a document error in UsingFPGA.md",13192597,"Change the log message ""START"" to ""END"" in some test cases. 
 Fix errors in yarn-default.xml related to GPU/FPGA",yes
13305422,"It is used to read thrift data files. AFAIK no one uses thrift for data serialization. 
 Ensure result order for special_character_in_tabnames_1.q",13305298,"While testinghttps://issues.apache.org/jira/browse/HIVE-23354special_character_in_tabnames_quotes_1 failed. Searching for the test, it seems other patches have also had failures. I noticed thatspecial_character_in_tabnames_1 andspecial_character_in_tabnames_quotes_1 use the same database/table names. I suspect this is responsible for some of the flakiness. 
 Fix flaky special_character_in_tabnames_quotes_1 test",yes
13131483,"Hive metastore schema on postgres is broken after the commit forHIVE-14498. Following error is seen during schema initialization:

0: jdbc:postgresql://localhost.localdomain:54> ALTER TABLE ONLY ""MV_CREATION_MET
ADATA"" ADD CONSTRAINT ""MV_CREATION_METADATA_FK"" FOREIGN KEY (""TBL_ID"") REFERENCE
S ""TBLS""(""TBL_ID"") DEFERRABLE
Error: ERROR: there is no unique constraint matching given keys for referenced table ""TBLS"" (state=42830,code=0)
Closing: 0: jdbc:postgresql://localhost.localdomain:5432/hive
org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!
Underlying cause: java.io.IOException : Schema script failed, errorcode 2
org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!
 at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:586)
 at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:559)
 at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1183)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:239)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:153)
Caused by: java.io.IOException: Schema script failed, errorcode 2
 at org.apache.hive.beeline.HiveSchemaTool.runBeeLine(HiveSchemaTool.java:957)
 at org.apache.hive.beeline.HiveSchemaTool.runBeeLine(HiveSchemaTool.java:935)
 at org.apache.hive.beeline.HiveSchemaTool.doInit(HiveSchemaTool.java:582)
 ... 8 more
*** schemaTool failed ***

Inthe file metastore/scripts/upgrade/postgres/hive-schema-3.0.0.postgres.sql the ordering of statement

ALTER TABLE ONLY ""MV_CREATION_METADATA""
 ADD CONSTRAINT ""MV_CREATION_METADATA_FK"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS""(""TBL_ID"") DEFERRABLE;

is before the definition of unique constraints for TBLS which is causing the issue. 
 Hive metastore schema initialization failing on postgres",13151276,"Uses current_date() which is prone to fail. 
 Stabilize statsoptimizer.q test",No
13210328,"visting http://rm_hostname:8088/cluster/nodes,  there are one ""Active Nodes"" in the area of ""Cluster Nodes Metrics"" , but  detailed info of node does not display. 
Just as showed in screenshot-1.png 
 RM nodes web page can not display node info",13204841,"It is observed in trunk build YARN cluster node pages is broken even though data exist.  
 UI cluster nodes page is broken",yes
13138196,"Enable Hive on TEZ. (MR works fine).
STEP 1. Create test data


nano /home/test/users.txt


Add to file:


Peter,34
John,25
Mary,28




hadoop fs -mkdir /bug
hadoop fs -copyFromLocal /home/test/users.txt /bug
hadoop fs -ls /bug


EXPECTED RESULT:


Found 2 items                                                                   
-rwxr-xr-x   3 root root         25 2015-10-15 16:11 /bug/users.txt


STEP 2. Upload data to hive


create external table bug(name string, age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' LOCATION '/bug';
select * from bug;


EXPECTED RESULT:


OK
Peter   34
John    25
Mary    28




create external table bug1(name string, age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' LOCATION '/bug1';
insert overwrite table bug select * from bug1;
select * from bug;


EXPECTED RESULT:


OK
Time taken: 0.097 seconds


ACTUAL RESULT:


hive>  select * from bug;
OK
Peter	34
John	25
Mary	28
Time taken: 0.198 seconds, Fetched: 3 row(s)

 
 INSERT OVERWRITE TABLE doesn't clean the table directory before overwriting",13249239,"

[ERROR] testBlockReportSucceedsWithLargerLengthLimit(org.apache.hadoop.hdfs.server.datanode.TestLargeBlockReport)  Time elapsed: 47.956 s  <<< ERROR!
org.apache.hadoop.ipc.RemoteException(java.io.IOException): java.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.runBlockOp(BlockManager.java:5011)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1581)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:181)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:31664)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:529)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1001)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:929)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1891)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2921)
Caused by: java.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:424)
	at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:396)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiffSorted(BlockManager.java:2952)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2787)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:2655)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.lambda$blockReport$0(NameNodeRpcServer.java:1582)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.processQueue(BlockManager.java:5089)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.run(BlockManager.java:5068)
Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)
	at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)
	at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)
	at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)
	at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:420)
	... 8 more

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1553)
	at org.apache.hadoop.ipc.Client.call(Client.java:1499)
	at org.apache.hadoop.ipc.Client.call(Client.java:1396)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy25.blockReport(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:218)
	at org.apache.hadoop.hdfs.server.datanode.TestLargeBlockReport.testBlockReportSucceedsWithLargerLengthLimit(TestLargeBlockReport.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Ref ::https://builds.apache.org/job/PreCommit-HDFS-Build/27416/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt 
 TestLargeBlockReport#testBlockReportSucceedsWithLargerLengthLimit fails in trunk",No
13141661,"With Sentry enabled, commands like explain drop table foo fail with


explain drop table foo;
Error: Error while compiling statement: FAILED: SemanticException No valid privileges
Required privilege( Table) not available in input privileges
The required privileges: (state=42000,code=40000)


Sentry fails to authorize because the ExplainSemanticAnalyzer uses an instance of DDLSemanticAnalyzer to analyze the explain query.


BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, input);
sem.analyze(input, ctx);
sem.validate()


The inputs/outputs entities for this query are set in the above code. However, these are never set on the instance of ExplainSemanticAnalyzer itself and thus is not propagated into the HookContext in the calling Driver code.


sem.analyze(tree, ctx); --> this results in calling the above code that uses DDLSA
hookCtx.update(sem); --> sem is an instance of ExplainSemanticAnalyzer, this code attempts to update the HookContext with the input/output info from ESA which is never set.

 
 Inputs/Outputs are not propagated to SA hooks for explain commands. ",13140375,"With Sentry enabled, commands like explain drop table foo fail with explain drop table foo;


Error: Error while compiling statement: FAILED: SemanticException No valid privileges
 Required privilege( Table) not available in input privileges
 The required privileges: (state=42000,code=40000)


Sentry fails to authorize because the ExplainSemanticAnalyzer uses an instance of DDLSemanticAnalyzer to analyze the explain query.


BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, input);
sem.analyze(input, ctx);
sem.validate()


The inputs/outputs entities for this query are set in the above code. However, these are never set on the instance of ExplainSemanticAnalyzer itself and thus is not propagated into the HookContext in the calling Driver code.


sem.analyze(tree, ctx); --> this results in calling the above code that uses DDLSA
hookCtx.update(sem); --> sem is an instance of ExplainSemanticAnalyzer, this code attempts to update the HookContext with the input/output info from ESA which is never set.

 
 Needs to capture input/output entities in explain",yes
13145792,"When ui2 is accessed behind proxy like knox/nginx, trailing path name should not be skipped. However trim of ""ui2"" if its there. 
 [UI2] New YARN UI webapp does not respect current pathname for REST api",13251946,"HDFS-13699 changed a debug log line into an info log line and this line is printed during hadoop fs -cat operations. This make it very difficult to figure out where the log line ends and where the catted file begins, especially when the output is sent to a tool for parsing. 

[ebadger@foobar bin]$ hadoop fs -cat /foo 2>/dev/null
2019-08-20 22:09:45,907 INFO  [main] sasl.SaslDataTransferClient (SaslDataTransferClient.java:checkTrustAndSend(230)) - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
bar

 
 HDFS cat logs an info message",No
13206231,"TestCheckTestClasses is failing in branch-2, observed in HBASE-9888.


TestCheckTestClasses.checkClasses:52 There are 1 test classes without category: [class org.apache.hadoop.hbase.TestKeyValue]


 
 TestCheckTestClasses is failing in branch-2",13142697,"hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java misses ClassRule and Category annotations.
TestKeyValue was flagged by internal RE which used mvn command that is different from what Apache Jenkins uses.


mvn -B -nsu test -Dtest=TestKeyValue --projects :hbase-common


ArrayIndexOutOfBoundsException from HBaseClassTestRuleChecker was observed in snippet from dump file:


# Created on 2018-03-05T22:25:50.252
org.apache.maven.surefire.testset.TestSetFailedException: Test mechanism :: 0
	at org.apache.maven.surefire.common.junit4.JUnit4RunListener.rethrowAnyTestMechanismFailures(JUnit4RunListener.java:223)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:168)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:373)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:334)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:119)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:407)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.hbase.HBaseClassTestRuleChecker.testStarted(HBaseClassTestRuleChecker.java:45)
	at org.junit.runner.notification.RunNotifier$3.notifyListener(RunNotifier.java:121)
	at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)
	at org.junit.runner.notification.RunNotifier.fireTestStarted(RunNotifier.java:118)


This issue adds the annotations to this test.
The test passes with annotations added.
I have searched code base but didn't find other test which misses test category. 
 TestKeyValue misses ClassRule and Category annotations",yes
13223771,"Hive show tables command does not list views - they're invisible when we're using ODBC.
We could operate on this views, for example select records but they're invisible on list of data structures. We're connecting with Hive by using Hortonworks ODBC Driver for Apache Hive. Any of tools that we're using (Aginity for Hadoop, Microsoft Excel, Tableau Desktop) does not list views but only tables. This views are visiblein HUE.  
 Invisible views using ODBC ",13217063,"HIVE-19974 introduced backwards incompatible change, with SHOW TABLES statement showing only managed/external tables in the system.
This issue will restore old behavior, with SHOW TABLES showing all queryable entities, including views and materialized views.
Instead, to provide information about table types, SHOW EXTENDED TABLES statement is introduced, which includes an additional column with the table type for each of the tables listed.
Besides, the possibility to filter the show tables statements with a WHERE `table_type` = 'ANY_TYPE' clause is introduced. 
 Show tables statement to include views and materialized views",yes
13313740,"I believe it is useless after we move split operation to master. Let's purge the uesless code. 
 Remove the legacy 'forceSplit' related code at region server side",13225364,"It appears that MultiRowRangeFilter was never written to function with reverse scans. There is too much logic that operates with the assumption that we are always moving ""forward"" through increasing ranges. It needs to be rewritten to ""traverse"" forward or backward, given how the context of the scan being used. 
 windows hbase-env causes hbase cli/etc to ignore HBASE_OPTS",No
13247655,"RegionServer exists when error happen


2019-07-29 20:51:09,726 INFO [RS_LOG_REPLAY_OPS-regionserver/h1:16020-0] wal.WALSplitter: Processed 0 edits across 0 regions; edits skipped=0; log file=hdfs://cluster1/hbase/WALs/h2,16020,1564216856546-splitting/h2%2C16020%2C1564216856546.1564398538121, length=615233, corrupted=false, progress failed=false
2019-07-29 20:51:09,726 INFO [RS_LOG_REPLAY_OPS-regionserver/h1:16020-0] handler.WALSplitterHandler: Worker h1,16020,1564404572589 done with task org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails@577da0d3 in 84892ms. Status = null
2019-07-29 20:51:09,726 ERROR [RS_LOG_REPLAY_OPS-regionserver/h1:16020-0] executor.EventHandler: Caught throwable while processing event RS_LOG_REPLAY
java.lang.ArrayIndexOutOfBoundsException: 16403
at org.apache.hadoop.hbase.KeyValue.getFamilyLength(KeyValue.java:1365)
at org.apache.hadoop.hbase.KeyValue.getFamilyLength(KeyValue.java:1358)
at org.apache.hadoop.hbase.PrivateCellUtil.matchingFamily(PrivateCellUtil.java:735)
at org.apache.hadoop.hbase.CellUtil.matchingFamily(CellUtil.java:816)
at org.apache.hadoop.hbase.wal.WALEdit.isMetaEditFamily(WALEdit.java:143)
at org.apache.hadoop.hbase.wal.WALEdit.isMetaEdit(WALEdit.java:148)
at org.apache.hadoop.hbase.wal.WALSplitter.splitLogFile(WALSplitter.java:297)
at org.apache.hadoop.hbase.wal.WALSplitter.splitLogFile(WALSplitter.java:195)
at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:100)
at org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler.process(WALSplitterHandler.java:70)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
2019-07-29 20:51:09,730 ERROR [RS_LOG_REPLAY_OPS-regionserver/h1:16020-0] regionserver.HRegionServer: ***** ABORTING region server h1,16020,1564404572589: Caught throwable while processing event RS_LOG_REPLAY *****
java.lang.ArrayIndexOutOfBoundsException: 16403
at org.apache.hadoop.hbase.KeyValue.getFamilyLength(KeyValue.java:1365)
at org.apache.hadoop.hbase.KeyValue.getFamilyLength(KeyValue.java:1358)
at org.apache.hadoop.hbase.PrivateCellUtil.matchingFamily(PrivateCellUtil.java:735)
at org.apache.hadoop.hbase.CellUtil.matchingFamily(CellUtil.java:816)
at org.apache.hadoop.hbase.wal.WALEdit.isMetaEditFamily(WALEdit.java:143)
at org.apache.hadoop.hbase.wal.WALEdit.isMetaEdit(WALEdit.java:148)
at org.apache.hadoop.hbase.wal.WALSplitter.splitLogFile(WALSplitter.java:297)
at org.apache.hadoop.hbase.wal.WALSplitter.splitLogFile(WALSplitter.java:195)
at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:100)
at org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler.process(WALSplitterHandler.java:70)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)



 
 Caught ArrayIndexOutOfBoundsException while processing event RS_LOG_REPLAY",13237478,"Summary
We had been chasing a WAL corruption issue reported on one of our customers deployments running release 2.1.1 (CDH 6.1.0). After providing a custom modified jar with the extra sanity checks implemented byHBASE-21401 applied on some code points, plus additional debugging messages, we believe it is related to DirectByteBuffer usage, and Unsafe copy from offheap memory to on-heap array triggered here, such as when writing into a non ByteBufferWriter type, as done here.
More details on the following comment.
 
 WAL corruption due to early DBBs re-use when Durability.ASYNC_WAL is used",yes
13153688,"Scenario:

Create a Dir
Create EZ for the above dir with Key1
Again you can try to create ZONE for same directory with the same Key1


hadoopclient> hadoop key list
Listing keys for KeyProvider: org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider@152aa092
key2
key1
hadoopclient> hdfs dfs -mkdir /kms
hadoopclient> hdfs crypto -createZone -keyName key1 -path /kms
Added encryption zone /kms
hadoopclient> hdfs crypto -createZone -keyName key1 -path /kms
RemoteException: Attempt to create an encryption zone for a non-empty directory.

Actual Output:
 ===========
RemoteException:Attempt to create an encryption zone for non-empty directory
Expected Output:
 =============
 Exception should be like EZ is already created with the same key 
 Crypto command should give proper exception when user is trying to create an EZ with the same key with which it is already created",13153687,"Scenario:

Create a Dir
Create EZ for the above dir with Key1
Again you can try to create ZONE for same directory with the same Key1


hadoopclient> hadoop key list
Listing keys for KeyProvider: org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider@152aa092
key2
key1
hadoopclient> hdfs dfs -mkdir /kms
hadoopclient> hdfs crypto -createZone -keyName key1 -path /kms
Added encryption zone /kms
hadoopclient> hdfs crypto -createZone -keyName key1 -path /kms
RemoteException: Attempt to create an encryption zone for a non-empty directory.

Actual Output:
 ===========
 Exception should be like EZ is already created with the same key
Expected Output:
 =============
RemoteException:Attempt to create an encryption zone for non-empty directory 
 Crypto command should give proper exception when user is trying to create an EZ with the same key with which it is already created",yes
13160597,"Steps to reproduce


add_rsgroup 'test_rsgroup'



Add a server to the new rsgroup

hbase(main):002:0> list_rsgroups
NAME                    SERVER / TABLE                                                      
test_rsgroup                                                                            
default                  server dob2-bach-r3n13:16020                                        
                     server dob2-bach-r3n13:16022                                        
                     server dob2-bach-r3n13:16023                                        
                     server dob2-bach-r3n13:16024                                        
                     server dob2-bach-r3n13:16025                                        
                     server dob2-bach-r3n13:16026                                        
                     table hbase:meta                                                     
                     table hbase:acl                                                     
                     table hbase:namespace                                                  
                     table hbase:rsgroup
move_servers_rsgroup 'test_rsgroup',['dob2-bach-r3n13:16020']



Move hbase namespace to the new rsgroup

hbase(main):005:0> move_namespaces_rsgroup 'test_rsgroup',['hbase']



List Rsgroups to verify all the hbasetables are moved

hbase(main):006:0> list_rsgroups
NAME                    SERVER / TABLE                                                      
test_rsgroup               server dob2-bach-r3n13:16020                                        
                     table hbase:meta                                                     
                     table hbase:namespace                                                  
                     table hbase:rsgroup                                                   
default                  server dob2-bach-r3n13:16022                                        
                     server dob2-bach-r3n13:16023                                        
                     server dob2-bach-r3n13:16024                                        
                     server dob2-bach-r3n13:16025                                        
                     server dob2-bach-r3n13:16026                                        
                     table hbase:acl



hbase:acl table is not moved to the new rsgroup

 
 hbase:acl table is listed in list_rsgroups output even when acl is not enabled",13159838,"Regionserver groups needs to specially handle what it calls ""special tables"", tables upon which core or other modular functionality depends. They need to be excluded from normal rsgroup processing during bootstrap to avoid circular dependencies or errors due to insufficiently initialized state. I think we also want to ensure that such tables are always given a rsgroup assignment with nonzero servers. (IIRC another issue already raises that point, we can link it later.)
Special tables include:

The system tables in the 'hbase:' namespace
The ACL table if the AccessController coprocessor is installed
The Labels table if the VisibilityController coprocessor is installed
The Quotas table if the FS quotas feature is active

Either we need a facility where ""special tables"" can be registered, which should be in core. Or, we institute a blanket rule that core and all extensions that need a ""special table"" must put them into the 'hbase:' namespace, so the TableName#isSystemTable() test will return TRUE for all, and then rsgroups simply needs to test for that. 
 Remove the concept of 'special tables' from rsgroups",yes
13183839,"Right now, in the table basic statistics, the raw data size for a row with any data type in the Parquet format is 1. This is an underestimated value when columns are complex data structures, like arrays.
Having tables with underestimated raw data size makes Hive assign less containers (mappers/reducers) to it, making the overall query slower. 
Heavy underestimation also makes Hive choose MapJoin instead of the ShuffleJoin that can fail with OOM errors.
In this patch, I compute the columns data size better, taking into account complex structures. I followed the Writer implementation for the ORC format. 
 Improve table statistics for Parquet format",13216403,"Now we just throw a RetriesExhaustedException. 
 StatsWork should use footer scan for Parquet",yes
13298527,"Admin.getRegionServers() returns all live RS of the cluster. It should consider optionally excluding decommissioned RS for operators to get live non-decommissioned RS list from single API. 
 Admin.getRegionServers() should return live servers excluding decom RS optionally",13208469,"The latest version of Spotbugs is 3.1.10 and it can be available via spotbugs-maven-plugin. 
 Upgrade Spotbugs to the latest version",No
13301098,"BackportingYARN-9848 to branch-3.2. 
 backport YARN-9848 to branch-3.2",13257847,"InYARN-4946, we've been discussing a revert due to the potential for keeping more applications in the state store than desired, and the potential to greatly increase RM recovery times.

I'm in favor of reverting the patch, but other ideas along the lines ofYARN-9571 would work as well. 
 revert YARN-4946",yes
13207333,"Now that ORC 1.5.4 is released, we should update Hive's version of ORC so that HIVE-20699 can use it 
 Update Hive to use ORC-1.5.4",13206242,"TestCheckTestClasses is failing in branch-2, observed in HBASE-9888.


TestCheckTestClasses.checkClasses:52 There are 1 test classes without category: [class org.apache.hadoop.hbase.TestKeyValue]


 
 Upgrade Hive to use ORC 1.5.4",yes
13163404,"This is the current description.

$ hdfs dfsadmin -help setSpaceQuota
...
		Available storageTypes are
		- RAM_DISK
		- DISK
		- SSD
		- ARCHIVE


Actually, space quota doesn't support RAM_DISK and support PROVIDED. Looks like the document has a same bug. 
 Fix the description of storageType option for space quota",13248267,"The command hadoop fs -test support seven options : -d / -e / -f / -s / -w / -r / -z.
However when see the usage of this command, only see five options. w and r are missing.
 
 Wrong usage hint for hadoop fs command test",No
13301873,"Running tests locally last night, I get this failure.

org.apache.hadoop.yarn.exceptions.YarnRuntimeException: 
org.apache.hadoop.service.ServiceStateException: ExitCodeException exitCode=1: chmod: /private/tmp/hadoop-yarn-ndimiduk/node-attribute/nodeattribute.mirror.writing: No such file or directory

	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.setUpBeforeClass(TestExportSnapshotWithTemporaryDirectory.java:43)
Caused by: org.apache.hadoop.service.ServiceStateException: 
ExitCodeException exitCode=1: chmod: /private/tmp/hadoop-yarn-ndimiduk/node-attribute/nodeattribute.mirror.writing: No such file or directory

	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.setUpBeforeClass(TestExportSnapshotWithTemporaryDirectory.java:43)
Caused by: org.apache.hadoop.util.Shell$ExitCodeException: 
chmod: /private/tmp/hadoop-yarn-ndimiduk/node-attribute/nodeattribute.mirror.writing: No such file or directory

	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotWithTemporaryDirectory.setUpBeforeClass(TestExportSnapshotWithTemporaryDirectory.java:43)

 
 TestExportSnapshotWithTemporaryDirectory fails with ""No such file or directory""",13299426,"Happened in two builds... 2.3 and 2:
jdk8 hadoop3 nightly just now https://builds.apache.org/view/H-L/view/HBase/job/HBase%20Nightly/job/branch-2.3/44/


[ERROR] org.apache.hadoop.hbase.rest.TestSecureRESTServer  Time elapsed: 0.01 s  <<< ERROR!
java.lang.NoClassDefFoundError: com/sun/jersey/core/spi/factory/AbstractRuntimeDelegate
	at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:202)
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.core.spi.factory.AbstractRuntimeDelegate
	at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:202)


Then again in branch2 https://builds.apache.org/view/H-L/view/HBase/job/HBase%20Nightly/job/branch-2/2619/


[ERROR] org.apache.hadoop.hbase.rest.TestSecureRESTServer  Time elapsed: 0.017 s  <<< ERROR!
java.lang.NoClassDefFoundError: com/sun/jersey/core/spi/factory/AbstractRuntimeDelegate
	at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:202)
Caused by: java.lang.ClassNotFoundException: com.sun.jersey.core.spi.factory.AbstractRuntimeDelegate
	at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:202)

 
 [Flakey Tests] [ERROR] TestSecureRESTServer  java.lang.NoClassDefFoundError: com/sun/jersey/core/spi/factory/AbstractRuntimeDelegate",yes
13141903,"1G is just a bit too low for JDK8+surefire 2.20+hdfs unit tests running in parallel.  Bump it up a bit more. 
 increase maven heap size recommendations",13128005,"Refernce:
https://builds.apache.org/job/PreCommit-HDFS-Build/22528/consoleFull
https://builds.apache.org/job/PreCommit-HDFS-Build/22528/artifact/out/branch-mvninstall-root.txt

[ERROR] unable to create new native thread -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/OutOfMemoryError

 
 [branch-2.8] Increase heap memory to avoid the OOM in pre-commit",yes
13344658,"When no value is specified for MVN_PROPERTIES, the mvn invocation is broken, passing in an explicit empty string argument, that is interpreted by maven as a phase specification. 
 hbase-vote.sh mvn invocation broken with empty MVN_PROPERTIES variable",13344315,"HBASE-25085 added support for maven options to be passed into the hbase-vote script. However, with the quoting on my machine it fails if I do not pass in any maven opts:

Signature: ok
Checksum : ok
Rat check (1.8.0_92): failed


mvn clean apache-rat:check """"


Built from source (1.8.0_92): failed


mvn clean install -DskipTests """"


Unit tests pass (1.8.0_92): failed


mvn package -P runAllTests """" -Dsurefire.rerunFailingTestsCount=3

Notice the """" and the build failed due to unknown lifecycle """" 
 Quoting in hbase-vote script fails build with empty """"",yes
13178199,"dfs.getQuotaUsage() on non-existent path should throw FileNotFoundException.

java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getQuotaUsageInt(FSDirStatAndListingOp.java:573)
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getQuotaUsage(FSDirStatAndListingOp.java:554)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getQuotaUsage(FSNamesystem.java:3221)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getQuotaUsage(NameNodeRpcServer.java:1404)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getQuotaUsage(ClientNamenodeProtocolServerSideTranslatorPB.java:1861)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)

 
 dfs.getQuotaUsage() throws NPE on non-existent dir instead of FileNotFoundException",13157471,"If the directory isnonexistent, getQuotaUsage rpc call willrun into
NullPointerException throwed by
FSDirStatAndListingOp.getQuotaUsageInt() .
I thinkFSDirStatAndListingOp.getQuotaUsageInt() should throwFileNotFoundExceptionwhen the directory isnonexistent. 
 NameNode: Fix NullPointerException when getQuotaUsageInt() invoked",yes
13256284,"Use ""protoc-maven-plugin"" to dynamically download protobuf executable to generate protobuf classes from proto file 
 [HDFS] use protobuf-maven-plugin to generate protobuf classes",13256283,"Use ""protoc-maven-plugin"" to dynamically download protobuf executable to generate protobuf classes from proto files. 
 [COMMON+HDFS] use protobuf-maven-plugin to generate protobuf classes",yes
13164721,"Steps to reproduce are:
 a. Setup a KMS with any OpenJDK 1.8 before u171 and create few KMS keys.
 b. Update KMS to run with OpenJDK 1.8u171 JDK and keys can't be read anymore, as can be seen below


hadoop key list -metadata
<keyname> : null


c. Going back to earlier JDK version fixes the issue.

There are no direct error / stacktrace in kms.log when it is not able to read the key metadata. Only Java serialization INFO messages are printed, followed by this one empty line in log which just says:


ERROR RangerKeyStore - 


In some cases, kms.log can also have these lines:


2018-05-18 10:40:46,438 DEBUG RangerKmsAuthorizer - <== RangerKmsAuthorizer.assertAccess(null, rangerkms/node1.host.com@ENV.COM (auth:KERBEROS), GET_METADATA) 
2018-05-18 10:40:46,598 INFO serialization - ObjectInputFilter REJECTED: class org.apache.hadoop.crypto.key.RangerKeyStoreProvider$KeyMetadata, array length: -1, nRefs: 1, depth: 1, bytes: 147, ex: n/a
2018-05-18 10:40:46,598 ERROR RangerKeyStore - 

 
 KMS fails to read the existing key metadata after upgrading to JDK 1.8u171 ",13154791,"There is a new feature in JDK 8u171 called Enhanced KeyStore Mechanisms (http://www.oracle.com/technetwork/java/javase/8u171-relnotes-4308888.html#JDK-8189997).
This is the cause of the following errors in the TestKeyProviderFactory:

Caused by: java.security.UnrecoverableKeyException: Rejected by the jceks.key.serialFilter or jdk.serialFilter property
	at com.sun.crypto.provider.KeyProtector.unseal(KeyProtector.java:352)
	at com.sun.crypto.provider.JceKeyStore.engineGetKey(JceKeyStore.java:136)
	at java.security.KeyStore.getKey(KeyStore.java:1023)
	at org.apache.hadoop.crypto.key.JavaKeyStoreProvider.getMetadata(JavaKeyStoreProvider.java:410)
	... 28 more


This issue causes errors and failures in hbase tests right now (using hdfs) and could affect other products running on this new Java version. 
 Configure serialFilter in KeyProvider to avoid UnrecoverableKeyException caused by JDK-8189997",yes
13339782,"The RESTServer currently relies on the following parameters to configure SSL on the REST API:

hbase.rest.ssl.enabled
hbase.rest.ssl.keystore.store
hbase.rest.ssl.keystore.password
hbase.rest.ssl.keystore.keypassword
hbase.rest.ssl.exclude.cipher.suites
hbase.rest.ssl.include.cipher.suites
hbase.rest.ssl.exclude.protocols
hbase.rest.ssl.include.protocols

In this patch I want to introduce the following new parameters:

hbase.rest.ssl.keystore.type
hbase.rest.ssl.truststore.store
hbase.rest.ssl.truststore.password
hbase.rest.ssl.truststore.type

If any of the new the parameter is not provided, then we should fall-back to the current behaviour (e.g. assuming JKS keystore/truststore types, or no passwords, or no custom trust store file). 
 Add SSL keystore type and truststore related configs for HBase RESTServer",13325501,"
add or improve some logs for adding local & global deadnodes
logic improve
fix typo

 
 Tiny Improve for DeadNode detector",No
13144911,"Correct the logic of mount validate to avoid the bad mountPoint. 
 RBF: Correct the logic of mount validate to avoid the bad mountPoint",13142459,"one of the mount entry source path rule is that the source path must start with '\', somebody didn't follow the rule and execute the following command:


$ hdfs dfsrouteradmin -add addnode/ ns1 /addnode/


But, the console show we are successful add this entry. 
 RBF: Throw the exception if mount table entry validated failed",yes
13250345,"Currently, explanation aboutCustom WAL Directoryconfiguration is a sub-topic ofBulk Loading,chapter, yet this subject has not much relation with bulk loading at all. It should rather be moved to a sub-section of theWrite Ahead Log (WAL)chapter. 
 Move ""Custom WAL Directory"" section from ""Bulk Loading"" to ""Write Ahead Log (WAL)"" chapter",13339328,"And also mention that this feature will be removed in 3.0.0.
This is the first step to undo master carrys regions. 
 Rewrite TestMultiLogThreshold to remove the LogDelegate in RSRpcServices",No
13302014,"And exclude a transitive dependency on com.fasterxml.jackson.core:jackson-databind:2.6.5 
 Bump jackson version to 2.10.0",13246278,"Bump the following jackson versions:
 - jackson version to 2.9.9
 - jackson-databind version to 2.9.9.3 
 Bumping jackson version to 2.9.9 and 2.9.9.3 (jackson-databind)",yes
13278051,"SM4 (formerly SMS4)is a block cipher used in the Chinese National Standard for Wireless LAN WAPI (Wired Authentication and Privacy Infrastructure).
 SM4 was a cipher proposed to for the IEEE 802.11i standard, but has so far been rejected by ISO. One of the reasons for the rejection has been opposition to the WAPI fast-track proposal by the IEEE. please see:
https://en.wikipedia.org/wiki/SM4_(cipher)

Use sm4 on hdfs as follows:
1.Configure Hadoop KMS
 2.test HDFS sm4
 hadoop key create key1 -cipher 'SM4/CTR/NoPadding'
 hdfs dfs -mkdir /benchmarks
 hdfs crypto -createZone -keyName key1 -path /benchmarks
requires:
 1.openssl version >=1.1.1 
 Add SM4 encryption method for HDFS",13258442,"In cases where users care a lot about read performance for tables that are small enough to fit into a cache (or the cache is large enough), prefetchOnOpen can be enabled to make the entire table available in cache after the initial region opening is completed. Any new data can also be guaranteed to be in cache with the cacheBlocksOnWrite setting.
However, the missing piece is when all blocks are evicted after a compaction. We found very poor performance after compactions for tables under heavy read load and a slower backing filesystem (S3). After a compaction the prefetching threads need to compete with threads servicing read requests and get constantly blocked as a result.
This is a proposal to introduce a new cache configuration option that would cache blocks on write during compaction for any column family that has prefetch enabled. This would virtually guarantee all blocks are kept in cache after the initial prefetch on open is completed allowing for guaranteed steady read performance despite a slow backing file system. 
 add sm4 crypto to hdfs",yes
13184065,"MRAppBenchmark.benchmark1() fails with NullPointerException:
1. We do not set any queue for this test. As the result we got the following exception:

2018-09-10 17:04:23,486 ERROR [Thread-0] rm.RMCommunicator (RMCommunicator.java:register(177)) - Exception while registering
java.lang.NullPointerException
at org.apache.avro.util.Utf8$2.toUtf8(Utf8.java:123)
at org.apache.avro.util.Utf8.getBytesFor(Utf8.java:172)
at org.apache.avro.util.Utf8.<init>(Utf8.java:39)
at org.apache.hadoop.mapreduce.jobhistory.JobQueueChangeEvent.<init>(JobQueueChangeEvent.java:35)
at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.setQueueName(JobImpl.java:1167)
at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.register(RMCommunicator.java:174)
at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.serviceStart(RMCommunicator.java:122)
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.serviceStart(RMContainerAllocator.java:280)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1293)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
at org.apache.hadoop.mapreduce.v2.app.MRApp.submit(MRApp.java:301)
at org.apache.hadoop.mapreduce.v2.app.MRApp.submit(MRApp.java:285)
at org.apache.hadoop.mapreduce.v2.app.MRAppBenchmark.run(MRAppBenchmark.java:72)
at org.apache.hadoop.mapreduce.v2.app.MRAppBenchmark.benchmark1(MRAppBenchmark.java:194)



2. We overridecreateSchedulerProxy method and do not set application priority that was added later by MAPREDUCE-6515. We got the following error:

java.lang.NullPointerException
 at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.handleJobPriorityChange(RMContainerAllocator.java:1025)
 at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:880)
 at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:286)
 at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:280)
 at java.lang.Thread.run(Thread.java:748)


In both cases, the job never will be run and the test stuck and will not be finished. 
 MRAppBenchmark.benchmark1() fails with NullPointerException",13184063,"MRAppBenchmark.benchmark1() fails with NullPointerException:
1. We do not set any queue for this test. As the result we got the following exception:

2018-09-10 17:04:23,486 ERROR [Thread-0] rm.RMCommunicator (RMCommunicator.java:register(177)) - Exception while registering
java.lang.NullPointerException
at org.apache.avro.util.Utf8$2.toUtf8(Utf8.java:123)
at org.apache.avro.util.Utf8.getBytesFor(Utf8.java:172)
at org.apache.avro.util.Utf8.<init>(Utf8.java:39)
at org.apache.hadoop.mapreduce.jobhistory.JobQueueChangeEvent.<init>(JobQueueChangeEvent.java:35)
at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.setQueueName(JobImpl.java:1167)
at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.register(RMCommunicator.java:174)
at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator.serviceStart(RMCommunicator.java:122)
at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.serviceStart(RMContainerAllocator.java:280)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1293)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
at org.apache.hadoop.mapreduce.v2.app.MRApp.submit(MRApp.java:301)
at org.apache.hadoop.mapreduce.v2.app.MRApp.submit(MRApp.java:285)
at org.apache.hadoop.mapreduce.v2.app.MRAppBenchmark.run(MRAppBenchmark.java:72)
at org.apache.hadoop.mapreduce.v2.app.MRAppBenchmark.benchmark1(MRAppBenchmark.java:194)



2. We overridecreateSchedulerProxy method and do not set application priority that was added later by MAPREDUCE-6515. We got the following error:

java.lang.NullPointerException
 at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.handleJobPriorityChange(RMContainerAllocator.java:1025)
 at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:880)
 at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:286)
 at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:280)
 at java.lang.Thread.run(Thread.java:748)


In both cases, the job never will be run and the test stuck and will not be finished. 
 MRAppBenchmark.benchmark1() fails with NullPointerException",yes
13216222,"Create new synonym for the existing function

Mid for substr
postiion for locate 
 Create Synonym mid for  substr, position for  locate",13254526,"When trying to launch the infrastructure application to begin the startup of the internal HDFS cluster as shown in the Manual Workload Launch section in here



$ ./dynamometer-infra/bin/start-dynamometer-cluster.sh \
 -hadoop_binary_path hadoop-3.0.2.tar.gz \
 -conf_path my-hadoop-conf \
 -fs_image_dir hdfs:///fsimage \
 -block_list_path hdfs:///dyno/blocks



 its usage is always shown even if correct arguments are given, if `-hadoop_binary_path` is placed as a first argument for the script. 
 [Dynamometer] start-dynamometer-cluster.sh shows its usage even if correct arguments are given.",No
13128887,"In one of the internal testing, observed the following exception


java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) ~[?:1.8.0_152]
	at java.util.ArrayList$Itr.next(ArrayList.java:859) ~[?:1.8.0_152]
	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1042) ~[?:1.8.0_152]
	at org.apache.logging.log4j.message.ParameterFormatter.appendCollection(ParameterFormatter.java:596) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.message.ParameterFormatter.appendPotentiallyRecursiveValue(ParameterFormatter.java:504) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.message.ParameterFormatter.recursiveDeepToString(ParameterFormatter.java:429) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.message.ParameterFormatter.formatMessage2(ParameterFormatter.java:189) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.message.ParameterizedMessage.formatTo(ParameterizedMessage.java:224) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.message.ParameterizedMessage.getFormattedMessage(ParameterizedMessage.java:200) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.RingBufferLogEvent.setMessage(RingBufferLogEvent.java:126) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.RingBufferLogEvent.setValues(RingBufferLogEvent.java:104) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.RingBufferLogEventTranslator.translateTo(RingBufferLogEventTranslator.java:56) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.RingBufferLogEventTranslator.translateTo(RingBufferLogEventTranslator.java:34) ~[log4j-core-2.6.2.jar:2.6.2]
	at com.lmax.disruptor.RingBuffer.translateAndPublish(RingBuffer.java:930) ~[disruptor-3.3.0.jar:?]
	at com.lmax.disruptor.RingBuffer.tryPublishEvent(RingBuffer.java:456) ~[disruptor-3.3.0.jar:?]
	at org.apache.logging.log4j.core.async.AsyncLoggerDisruptor.tryPublish(AsyncLoggerDisruptor.java:190) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.AsyncLogger.publish(AsyncLogger.java:160) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.AsyncLogger.logWithThreadLocalTranslator(AsyncLogger.java:156) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.core.async.AsyncLogger.logMessage(AsyncLogger.java:126) ~[log4j-core-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:2011) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1884) ~[log4j-api-2.6.2.jar:2.6.2]
	at org.apache.logging.slf4j.Log4jLogger.info(Log4jLogger.java:189) ~[log4j-slf4j-impl-2.6.2.jar:2.6.2]
	at org.apache.hadoop.hive.druid.security.KerberosHttpClient.inner_go(KerberosHttpClient.java:96) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91]
	at org.apache.hadoop.hive.druid.security.KerberosHttpClient.access$100(KerberosHttpClient.java:50) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91]
	at org.apache.hadoop.hive.druid.security.KerberosHttpClient$2.onSuccess(KerberosHttpClient.java:144) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91]
	at org.apache.hadoop.hive.druid.security.KerberosHttpClient$2.onSuccess(KerberosHttpClient.java:134) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91]
	at org.apache.hive.druid.com.google.common.util.concurrent.Futures$4.run(Futures.java:1181) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_152]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_152]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_152]


The fix for this went into 2.9.1 LOG4J2-1988 onwards. Updating log4j to latest version should have a fix for this issue.  
 ConcurrentModificationException in log4j2.x library",13186829,"The Date spec in Hive says
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-date

DATE values describe a particular year/month/day, in the form YYYY-­MM-­DD. For example, DATE '2013-­01-­01'


Even though the Hive date & timestamp spec are strict on their format specification, Hive has previously supported "" yyyy-[m]m-[d]d"" formatted dates and it is better to expand the spec rather than drop this compatibility. 
 Backwards Compat: Support non-zero padded date and timestamp entries",No
13149416,"Running the example commands specified in shell docs for ""append"" and ""incr"" throw following error:


ERROR: Failed to provide both column family and column qualifier for append



ERROR: Failed to provide both column family and column qualifier for incr

While running the same command via java does not require the user to provide both column and qualifier and works smoothly.

Steps to reproduce:
1) APPEND


hbase(main):002:0> create 't1', 'c1', 'c2'
Created table t1
Took 0.8151 seconds 
hbase(main):003:0> append 't1', 'r1', 'c1', 'value'

ERROR: Failed to provide both column family and column qualifier for append

Appends a cell 'value' at specified table/row/column coordinates.

 hbase> append 't1', 'r1', 'c1', 'value', ATTRIBUTES=>{'mykey'=>'myvalue'}
 hbase> append 't1', 'r1', 'c1', 'value', {VISIBILITY=>'PRIVATE|SECRET'}

The same commands also can be run on a table reference. Suppose you had a reference
t to table 't1', the corresponding command would be:

 hbase> t.append 'r1', 'c1', 'value', ATTRIBUTES=>{'mykey'=>'myvalue'}
 hbase> t.append 'r1', 'c1', 'value', {VISIBILITY=>'PRIVATE|SECRET'}

Took 0.0326 seconds

hbase(main):004:0> scan 't1'
ROW COLUMN+CELL 
0 row(s)
Took 0.1273 seconds 

While the same command would run if we run the following java code:


 try (Connection connection = ConnectionFactory.createConnection(config);
 Admin admin = connection.getAdmin();) {
 Table table = connection.getTable(TableName.valueOf(""t1""));
 Append a = new Append(Bytes.toBytes(""r1""));
 a.addColumn(Bytes.toBytes(""c1""), null, Bytes.toBytes(""value""));
 table.append(a);
 }

Scan result after executing java code:


hbase(main):005:0> scan 't1'
ROW COLUMN+CELL 
r1 column=c1:, timestamp=1522649623090, value=value 
1 row(s)
Took 0.0188 seconds



2) INCREMENT:
Similarly in case of increment, we get the following error (shell):


hbase(main):006:0> incr 't1', 'r2', 'c1', 111

ERROR: Failed to provide both column family and column qualifier for incr

Increments a cell 'value' at specified table/row/column coordinates.
To increment a cell value in table 'ns1:t1' or 't1' at row 'r1' under column
'c1' by 1 (can be omitted) or 10 do:

 hbase> incr 'ns1:t1', 'r1', 'c1'
 hbase> incr 't1', 'r1', 'c1'
 hbase> incr 't1', 'r1', 'c1', 1
 hbase> incr 't1', 'r1', 'c1', 10
 hbase> incr 't1', 'r1', 'c1', 10, {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> incr 't1', 'r1', 'c1', {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> incr 't1', 'r1', 'c1', 10, {VISIBILITY=>'PRIVATE|SECRET'}

The same commands also can be run on a table reference. Suppose you had a reference
t to table 't1', the corresponding command would be:

 hbase> t.incr 'r1', 'c1'
 hbase> t.incr 'r1', 'c1', 1
 hbase> t.incr 'r1', 'c1', 10, {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> t.incr 'r1', 'c1', 10, {VISIBILITY=>'PRIVATE|SECRET'}

Took 0.0103 seconds 
hbase(main):007:0> scan 't1'
ROW COLUMN+CELL 
r1 column=c1:, timestamp=1522649623090, value=value 
1 row(s)
Took 0.0062 seconds


While the same command would run, if we run the following java code:


 try (Connection connection = ConnectionFactory.createConnection(config);
 Admin admin = connection.getAdmin();) {
 Table table = connection.getTable(TableName.valueOf(""t1""));
 Increment incr = new Increment(Bytes.toBytes(""r2""));
 incr.addColumn(Bytes.toBytes(""c1""), null, 111);
 table.increment(incr);
 scan(table);
 }


Scan result after executing java code:


hbase(main):008:0> scan 't1'
ROW COLUMN+CELL 
r1 column=c1:, timestamp=1522649623090, value=value 
r2 column=c1:, timestamp=1522649933949, value=\x00\x00\x00\x00\x00\x00\x00o 
2 row(s)
Took 0.0133 seconds 

 
 When qualifier is not specified, append and incr operation do not work (shell)",13259874,"It's more than flaky. Recently, in most of the cases it's the only failing test in otherwise green runs.
full hive logs:  hive.log.tar.gz 


Error Message
java.util.concurrent.ExecutionException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
Stacktrace
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.testMTT(TestBuddyAllocator.java:149)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.testMTT(TestBuddyAllocator.java:145)
	... 33 more
Caused by: org.apache.hadoop.hive.common.io.Allocator$AllocatorOutOfMemoryException: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
	at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:454)
	at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:299)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.allocateAndUseBuffer(TestBuddyAllocator.java:254)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.allocateUp(TestBuddyAllocator.java:231)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.access$000(TestBuddyAllocator.java:43)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator$1.call(TestBuddyAllocator.java:119)
	at org.apache.hadoop.hive.llap.cache.TestBuddyAllocator$1.call(TestBuddyAllocator.java:116)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Standard Error
2019-09-30T03:04:43,170  INFO [main] LlapIoImpl: Buddy allocator with direct buffers; memory mapped off /home/hiveptest/104.197.225.41-hiveptest-1/apache-github-source-source/llap-server/target/tmp/llap-5698481178354861395; allocation sizes 8 - 256, arena size 256, total size 6144
2019-09-30T03:04:43,174  WARN [pool-6-thread-3] LlapIoImpl: Failed to allocate [3]X[256] bytes after [15] attempt, evicted [0] bytes and partially allocated [256] bytes
2019-09-30T03:04:43,174  WARN [pool-6-thread-2] LlapIoImpl: Failed to allocate [3]X[256] bytes after [15] attempt, evicted [0] bytes and partially allocated [0] bytes
2019-09-30T03:04:43,175 ERROR [pool-6-thread-2] LlapIoImpl: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
2019-09-30T03:04:43,175 ERROR [pool-6-thread-3] LlapIoImpl: Failed to allocate 255; at 1 out of 3 (entire cache is fragmented and locked, or an internal issue)
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 256, allocated
Arena: 
  free list for size 8: 5, 
  free list for size 16: 0, 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 16, free
  block 2 at 16: size 8, allocated
  block 3 at 24: size 8, allocated
  block 4 at 32: size 8, allocated
  block 5 at 40: size 8, free
  block 6 at 48: size 16, allocated
  block 8 at 64: size 64, allocated
  block 16 at 128: size 128, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 6, 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 16, free
  block 2 at 16: size 16, allocated
  block 4 at 32: size 16, allocated
  block 6 at 48: size 16, free
  block 8 at 64: size 64, allocated
  block 16 at 128: size 128, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 0, 
  free list for size 64: 
  free list for size 128: 16, 
  free list for size 256: 
  block 0 at 0: size 32, free
  block 4 at 32: size 32, allocated
  block 8 at 64: size 32, allocated
  block 12 at 96: size 32, allocated
  block 16 at 128: size 128, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 64, allocated
  block 8 at 64: size 64, allocated
  block 16 at 128: size 128, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 
  block 0 at 0: size 64, allocated
  block 8 at 64: size 64, allocated
  block 16 at 128: size 64, allocated
  block 24 at 192: size 64, allocated
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 8, 
  free list for size 128: 16, 
  free list for size 256: 
  block 0 at 0: size 64, free
  block 8 at 64: size 64, free
  block 16 at 128: size 128, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
Arena: 
  free list for size 8: 
  free list for size 16: 
  free list for size 32: 
  free list for size 64: 
  free list for size 128: 
  free list for size 256: 0, 
  block 0 at 0: size 256, free
2019-09-30T03:04:43,176 ERROR [pool-6-thread-2] cache.TestBuddyAllocator: Failed to allocate 3 of 255; {[256.@0]}, {[16.@0][8*@2][8*@3][8*@4][8.@5][16*@6][64*@8][128*@16]}, {[16.@0][16*@2][16*@4][16.@6][64*@8][128*@16]}, {[32.@0][32*@4][32*@8][32*@12][128.@16]}, {[64.@0][64*@8][128*@16]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, 
2019-09-30T03:04:43,177 ERROR [pool-6-thread-3] cache.TestBuddyAllocator: Failed to allocate 3 of 255; {[256.@0]}, {[16.@0][8*@2][8*@3][8*@4][8.@5][16*@6][64*@8][128*@16]}, {[16.@0][16*@2][16*@4][16.@6][64*@8][128*@16]}, {[32.@0][32*@4][32*@8][32*@12][128.@16]}, {[64.@0][64*@8][128*@16]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, {[256.@0]}, 

 
 In NameNode Web UI's Startup Progress page, Loading edits always shows 0 sec",No
13134637,"Some class has missed the ASF header 
 Some class has missed the ASF header",13339782,"The RESTServer currently relies on the following parameters to configure SSL on the REST API:

hbase.rest.ssl.enabled
hbase.rest.ssl.keystore.store
hbase.rest.ssl.keystore.password
hbase.rest.ssl.keystore.keypassword
hbase.rest.ssl.exclude.cipher.suites
hbase.rest.ssl.include.cipher.suites
hbase.rest.ssl.exclude.protocols
hbase.rest.ssl.include.protocols

In this patch I want to introduce the following new parameters:

hbase.rest.ssl.keystore.type
hbase.rest.ssl.truststore.store
hbase.rest.ssl.truststore.password
hbase.rest.ssl.truststore.type

If any of the new the parameter is not provided, then we should fall-back to the current behaviour (e.g. assuming JKS keystore/truststore types, or no passwords, or no custom trust store file). 
 Add SSL keystore type and truststore related configs for HBase RESTServer",No
13140711,"broken by HIVE-18203 
 fix TestSSL",13131471,"TestSSL test has been failing for a while now. Following three tests fail everytime precommit job runs.
org.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=232)
org.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=232)
org.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=232) 
 Fix TestSSL test failures",yes
13284634,"org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations.testListingDelete[auth=true]
failing because the deleted file can still be read when the s3guard entry has the versionId.
Proposed: if the FS is versioned and the file status has versionID then we switch to tests which assert the file is readable, rather than tests which assert it isn't there 
 ITestS3GuardOutOfBandOperations failing on versioned S3 buckets",13256543,"If you enable versionid tracking for a bucket, those tests in ITestS3ARemoteFileChanged which try to generate failures off etag conflict all fail.
Fix: clear the relevant bucket option before the test run 
 ITestS3ARemoteFileChanged tests fail if you set the bucket to versionid tracking",yes
13327596,"When we enable fallback, rename should success if the src.parent or dst.parent on inernalDir.

org.apache.hadoop.security.AccessControlException: InternalDir of ViewFileSystem is readonly, operation rename not permitted on path /newFileOnRoot.org.apache.hadoop.security.AccessControlException: InternalDir of ViewFileSystem is readonly, operation rename not permitted on path /newFileOnRoot.
 at org.apache.hadoop.fs.viewfs.ViewFileSystem.readOnlyMountTable(ViewFileSystem.java:95) at org.apache.hadoop.fs.viewfs.ViewFileSystem.readOnlyMountTable(ViewFileSystem.java:101) at org.apache.hadoop.fs.viewfs.ViewFileSystem.rename(ViewFileSystem.java:683) at org.apache.hadoop.hdfs.ViewDistributedFileSystem.rename(ViewDistributedFileSystem.java:533) at org.apache.hadoop.hdfs.TestViewDistributedFileSystemWithMountLinks.verifyRename(TestViewDistributedFileSystemWithMountLinks.java:114) at org.apache.hadoop.hdfs.TestViewDistributedFileSystemWithMountLinks.testRenameOnInternalDirWithFallback(TestViewDistributedFileSystemWithMountLinks.java:90) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
 
 Fix the rename issues with fallback fs enabled",13230128,"attempting to build against current hadoop trunk for HBASE-22087 shows that hte byo-hadoop client is trying to package transitive dependencies from the hadoop dependencies that we expressly say we don't need to bring with us.
it's because we don't list those modules as provided, so all of their transitives are also in compile scope. The shading module does simple filtering when excluding things in a given scope, it doesn't e.g. make sure to also exclude the transitive dependencies of things it keeps out.
since we don't want to list all the transitive dependencies of hadoop in our shading exclusion, we should list the needed hadoop modules as provided. 
 shaded byo-hadoop client should list needed hadoop modules as provided scope to avoid inclusion of unnecessary transitive depednencies",No
13228165,"The field shippedKBsKey of the class MetricsReplicationSourceSourceImpl was deprecated in 1.3.0 and should be removed for 3.0.0. 
 Remove deprecated field from MetricsReplicationSourceSourceImpl",13228163,"The field SOURCE_SHIPPED_KBS in the class MetricsReplicationSourceSource is deprecated and should be removed in 3.0.0. 
 Remove deprecated field from MetricsReplicationSourceSource",yes
13159075,"In resource model doc, resource-types.xml and node-resource.xml description table has wrong number of columns defined, seehttps://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceModel.html 
 Fixed the flaky TestEnableRSGroup which was caused by HBASE-20544",13157891,"Attempting to update hbase-downstreamer to use our 2.0.0 release fails with an invalid port in the event that hbase.localcluster.assign.random.ports isn't set (or is set to false, specifically):


2018-05-08 06:10:06,508 ERROR [main] regionserver.HRegionServer (HRegionServer.java:<init>(631)) - Failed construction RegionServer
java.lang.IllegalArgumentException: port out of range:-1
	at java.net.InetSocketAddress.checkPort(InetSocketAddress.java:143)
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:224)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.<init>(RSRpcServices.java:1217)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.<init>(RSRpcServices.java:1184)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.createRpcServices(HRegionServer.java:723)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:561)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.<init>(MiniHBaseCluster.java:147)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.util.JVMClusterUtil.createRegionServerThread(JVMClusterUtil.java:86)
	at org.apache.hadoop.hbase.LocalHBaseCluster.addRegionServer(LocalHBaseCluster.java:184)
	at org.apache.hadoop.hbase.LocalHBaseCluster$1.run(LocalHBaseCluster.java:198)
	at org.apache.hadoop.hbase.LocalHBaseCluster$1.run(LocalHBaseCluster.java:195)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:313)
	at org.apache.hadoop.hbase.LocalHBaseCluster.addRegionServer(LocalHBaseCluster.java:194)
	at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:261)
	at org.apache.hadoop.hbase.MiniHBaseCluster.<init>(MiniHBaseCluster.java:121)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1042)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:988)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:859)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:853)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:782)
	at org.hbase.downstreamer.TestHBaseMiniCluster.testSpinUpMiniHBaseCluster(TestHBaseMiniCluster.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)



This is due to HBASE-15835 mistakenly setting hte REGIONSERVER_PORT to -1 rather than 0 (pick a port) or setting REGIONSERVER_INFO_PORT to -1 (which means ""turn off the UI"").
Ideally I think we don't need the ""change what we do if default is set"" behavior from HBASE-15835. It misses some other ports that will still conflict with a running HBase instance. Instead we should rely on  hbase.localcluster.assign.random.ports=true as a default for HBTU. 
 downstream HBaseTestingUtility fails with invalid port",yes
13155744,"[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.1 s <<< FAILURE! - in org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations
[ERROR] testCardinalityConstraint(org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations)  Time elapsed: 0.007 s  <<< FAILURE!
java.lang.AssertionError: expected: java.util.HashSet<[hb]> but was: java.util.HashSet<[hb]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint(TestPlacementConstraintTransformations.java:116)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498) 
 TestPlacementConstraintTransformations is failing in trunk",13155701,"The HashSet comparison is not working for some reason:

java.lang.AssertionError: expected: java.util.HashSet<[hb]> but was: java.util.HashSet<[hb]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint(TestPlacementConstraintTransformations.java:116)

 
 YARN precommit build failing in TestPlacementConstraintTransformations",yes
13171855,"https://github.com/apache/hive/blob/6d890faf22fd1ede3658a5eed097476eab3c67e9/ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java


// Do not print stack trace to STDERR - remove this, just throw the HiveException
    } catch (Exception e) {
      e.printStackTrace();
      throw new HiveException(e);
    }
...
// Do not log and throw.  log *or* throw.  In this case, just throw. Remove logging.
// Remove explicit 'return' call. No need for it.
      try {
        skewJoinKeyContext.endGroup();
      } catch (IOException e) {
        LOG.error(e.getMessage(), e);
        throw new HiveException(e);
      }
      return;

 
 Do Not Print StackTraces to STDERR in ParseDriver",13171847,"org/apache/hadoop/hive/ql/parse/ParseDriver.java


catch (RecognitionException e) {
      e.printStackTrace();
      throw new ParseException(parser.errors);
    }


Do not use e.printStackTrace() and print to STDERR.  Either remove or replace with a debug-level log statement.  I would vote to simply remove.  There are several occurrences of this pattern in this class. 
 Do Not Print StackTraces to STDERR in ParseDriver",yes
13131573,"currently ConfVars.HIVE_LOG_EXPLAIN_OUTPUT enables 2 features:

log the query to the console (even thru beeline)
shows the query on the web ui

I've enabled it...and ever since then my beeline is always flooded with an explain extended output...which is very verbose; even for simple queries. 
 HS2UI: Introduce separate option to show query on web ui",13183670,"The current hadoop.apache.org doesn't mention Ozone in the ""Modules"" section.
We can add something like this (or better):
Hadoop Ozone is an object store for Hadoop on top of the Hadoop HDDS which provides low-level binary storage layer.
We can also link to the http://ozone.hadoop.apache.org 
 Add Ozone submodule to the hadoop.apache.org",No
13148738,"something changed in the stack of docker image definitions we rely on and we don't have git now, which is causing yetus to fail once we relaunch in the container.
add git, try not to look too hard at what this implies about reproducibility of our builds. 
 test Dockerfile needs to include git",13264443,"Replace ""/"" with constant NodeBase.PATH_SEPARATOR_STR 
 Refactor DFSNetworkTopology#isNodeInScope",No
13155202,"Support pushing arrow batches through org.apache.arrow.vector.ipc.ArrowOutputStream in LllapOutputFormatService. 
 Support ArrowOutputStream in LlapOutputFormatService",13155204,"This is a sub-class of LlapBaseRecordReader that wraps the socket inputStream and produces Arrow batches for an external client. 
 Provide an Arrow stream reader for external LLAP clients ",yes
13312586,"
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  17.982 s
[INFO] Finished at: 2020-06-20T01:56:28Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:javadoc (default-cli) on project hadoop-hdfs: An error has occurred in Javadoc report generation: 
[ERROR] Exit code: 1 - javadoc: warning - You have specified the HTML version as HTML 4.01 by using the -html4 option.
[ERROR] The default is currently HTML5 and the support for HTML 4.01 will be removed
[ERROR] in a future release. To suppress this warning, please ensure that any HTML constructs
[ERROR] in your comments are valid in HTML5, and remove the -html4 option.
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:25197: error: cannot find symbol
[ERROR]       com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                          ^
[ERROR]   symbol:   class GeneratedMessageV3
[ERROR]   location: package com.google.protobuf
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:25319: error: cannot find symbol
[ERROR]         com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                            ^
[ERROR]   symbol:   class GeneratedMessageV3
[ERROR]   location: package com.google.protobuf
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:26068: error: cannot find symbol
[ERROR]         com.google.protobuf.GeneratedMessageV3 implements
[ERROR]                            ^
[ERROR]   symbol:   class GeneratedMessageV3
[ERROR]   location: package com.google.protobuf
[ERROR] /home/jenkins/jenkins-slave/workspace/hadoop-multibranch_PR-2084/src/hadoop-hdfs-project/hadoop-hdfs/target/generated-sources/java/org/apache/hadoop/hdfs/server/namenode/FsImageProto.java:26073: error: package com.google.protobuf.GeneratedMessageV3 does not exist
[ERROR]       private PersistToken(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {

 
 [JDK11] Fix Javadoc errors",13285270,"This issue is to run mvn javadoc:javadoc successfully in Apache Hadoop with Java 11.
Now there are many errors. 
 [JDK11] Support JavaDoc",yes
13337736,"We have observed some failures when launching containers with runc. We have not yet identified the root cause of those failures, but a side-effect of these failures was the Nodemanager marked itself unhealthy. Since these are rare failures that only affect a single launch, they should not cause the Nodemanager to be marked unhealthy.
Here is an example RM log:

resourcemanager.log.2020-10-02-03.bz2:2020-10-02 03:20:10,255 [RM Event dispatcher] INFO rmnode.RMNodeImpl: Node node:8041 reported UNHEALTHY with details: Linux Container Executor reached unrecoverable exception


And here is an example of the NM log:

2020-10-02 03:20:02,033 [ContainersLauncher #434] INFO runtime.RuncContainerRuntime: Launch container failed for container_e25_1601602719874_10691_01_001723
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=24: OCI command has bad/missing local dire
ctories


The problem is that the runc code in container-executor is re-using exit code 24 (INVALID_CONFIG_FILE) which is intended for problems with the container-executor.cfg file, and those failures are fatal for the NM.  We should use a different exit code for these. 
 runc launch failure should not cause nodemanager to go unhealthy",13180937,"There is no directory created as ""DS_APP_ATTEMPT"" so
#Assert.assertTrue(outputDirForEntity.isDirectory()) returning false


org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2
Stacktrace
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628)

org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2CustomizedFlow
 Failing for the past 10 builds (Since#29 )
 
Stacktrace
java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628)

org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithoutDomainV2DefaultFlow
 
 
 Stacktrace
 java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.verifyEntityTypeFileExists(TestDistributedShell.java:628) 
 TestDistributedShell test cases failing giving assertion error",No
13128566,"Both RM were running perfectly fine for many days and switched multiple times. At some point of time when RM is switched from ACTIVE -> STANDBY, UGI information got either changed or to subject new user got added.  
As a result UGI#getShortUserName() is returning wrong user which result in fail to  transition to ACTIVE with AccessControlException!

Caused by: org.apache.hadoop.security.AccessControlException: User odsuser doesn't have permission to call 'refreshAdminAcls' 


odsuser user is application submitted user. 
 
 RM fail to transition to ACTIVE in secure cluster",13128565,"Both RM were running perfectly fine for many days and switched multiple times. At some point of time when RM is switched from ACTIVE -> STANDBY, UGI information got either changed or to subject new user got added.  
As a result UGI#getShortUserName() is returning wrong user which result in fail to  transition to ACTIVE with AccessControlException!

Caused by: org.apache.hadoop.security.AccessControlException: User odsuser doesn't have permission to call 'refreshAdminAcls' 


odsuser user is application submitted user. 
 
 Both RM are in standby in secure cluster",yes
13151551,"Update:
TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] is also failing due to same issue/exception
Query:


select   count(*) cnt
 from
     x1_store_sales s
     ,x1_date_dim d
     ,x1_item i
 where  
	1=1
 	and s.ss_sold_date_sk = d.d_date_sk
 	and s.ss_item_sk = i.i_item_sk
 	and d.d_month_seq in	 
 	     (select distinct (d_month_seq)
 	      from x1_date_dim
               where d_year = 2000 and d_year*d_moy > 200000
 	        and d_moy = 2 )
 	and i.i_current_price > 
            (select min(j.i_current_price) 
 	     from x1_item j 
 	     where j.i_category = i.i_category)

 group by d.d_month_seq
 order by cnt 
 limit 100;


Error stack:

2018-04-10T20:36:49,379 ERROR [2f7066d9-5b29-4558-9a95-63f1da246cae main] ql.Driver: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 7, vertexId=vertex_1523417780216_0001_38_06, diagnostics=[Task failed, taskId=task_1523417780216_0001_38_06_000000, diagnostics=[TaskAttempt 0 killed, TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1523417780216_0001_38_06_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:354)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
	... 15 more
Caused by: java.lang.NullPointerException
	at java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.setValue(DynamicValueRegistryTez.java:78)
	at org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.init(DynamicValueRegistryTez.java:118)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:313)
	... 16 more
], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1523417780216_0001_38_06_000000_2:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:354)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)
	... 15 more
Caused by: java.lang.NullPointerException
	at java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
	at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
	at org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.setValue(DynamicValueRegistryTez.java:78)
	at org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.init(DynamicValueRegistryTez.java:118)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:313)
	... 16 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1523417780216_0001_38_06 [Map 7] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 5, vertexId=vertex_1523417780216_0001_38_08, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1523417780216_0001_38_08 [Reducer 5] killed/failed due to:OTHER_VERTEX_FAILURE]Vertex killed, vertexName=Reducer 6, vertexId=vertex_1523417780216_0001_38_09, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1523417780216_0001_38_09 [Reducer 6] killed/failed due to:OTHER_VERTEX_FAILURE]Vertex killed, vertexName=Map 4, vertexId=vertex_1523417780216_0001_38_07, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1523417780216_0001_38_07 [Map 4] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:3
2018-04-10T20:36:49,379 DEBUG [2f7066d9-5b29-4558-9a95-63f1da246cae main] ql.Driver: Shutting down query 

 
 TestMiniLlapLocalCliDriver groupby_groupingset_bug failure",13151485,"Looks like this test has been broken for some time 
 TestMiniLlapLocalCliDriver.vectorized_dynamic_semijoin_reduction.q is broken",yes
13128899,"mergejoin test fails with java.lang.IllegalStateException when run in MiniLlapLocal.
This is the query for which it fails,
[ERROR]   TestMiniLlapLocalCliDriver.testCliDriver:59 Client execution failed with error code = 2 running ""
select count from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value"" fname=mergejoin.q 
This is the stack trace,
failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: b initializer failed, vertex=vertex_1515180518813_0001_42_05 [Map 8], java.lang.RuntimeException: ORC split generation failed with exception: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_12_a_key_min
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1784)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1872)
    at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:499)
    at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:684)
    at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:196)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_12_a_key_min
    at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    at java.util.concurrent.FutureTask.get(FutureTask.java:192)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1778)
    ... 17 more
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_12_a_key_min
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:142)
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getJavaValue(DynamicValue.java:97)
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getLiteral(DynamicValue.java:93)
    at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.getLiteralList(SearchArgumentImpl.java:120)
    at org.apache.orc.impl.RecordReaderImpl.evaluatePredicateMinMax(RecordReaderImpl.java:553)
    at org.apache.orc.impl.RecordReaderImpl.evaluatePredicateRange(RecordReaderImpl.java:463)
    at org.apache.orc.impl.RecordReaderImpl.evaluatePredicate(RecordReaderImpl.java:440)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.isStripeSatisfyPredicate(OrcInputFormat.java:2163)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.pickStripesInternal(OrcInputFormat.java:2140)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.pickStripes(OrcInputFormat.java:2131)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$3000(OrcInputFormat.java:157)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1476)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2700(OrcInputFormat.java:1261)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1445)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1442)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1442)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1261)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    ... 3 more
Caused by: java.lang.IllegalStateException: Value does not exist in registry: RS_12_a_key_min
    at org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.getValue(DynamicValueRegistryTez.java:76)
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:137)
    ... 23 more 
 mergejoin fails with java.lang.IllegalStateException",13136865,"I'm using latest master branch code and mysql as metastore.
Creating table hits this error:

2018-02-07T22:04:55,438 ERROR [41f91bf4-bc49-4a73-baee-e2a1d79b8a4e main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MTable@28d16af8"" using statement ""INSERT INTO `TBLS` (`TBL_ID`,`CREATE_TIME`,`CREATION_METADATA_MV_CREATION_METADATA_ID_OID`,`DB_ID`,`LAST_ACCESS_TIME`,`OWNER`,`RETENTION`,`IS_REWRITE_ENABLED`,`SD_ID`,`TBL_NAME`,`TBL_TYPE`,`VIEW_EXPANDED_TEXT`,`VIEW_ORIGINAL_TEXT`) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)"" failed : Unknown column 'CREATION_METADATA_MV_CREATION_METADATA_ID_OID' in 'field list'
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)
        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:729)
        at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:749)
        at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:1125)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
        at com.sun.proxy.$Proxy36.createTable(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1506)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1412)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1614)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 
 Cannot create table: ""message:Exception thrown when executing query : SELECT DISTINCT..""",No
13261756,"we can know that the major compaction is skipping from the below regionserver's log, but it is compacting that region.
and read the code and find it is not correct and i add mark""/*** ***/"" below

public boolean shouldPerformMajorCompaction(final Collection<StoreFile> filesToCompact)
 throws IOException {
 if (lowTimestamp > 0L && lowTimestamp < (now - mcTime)) {
 if (filesToCompact.size() == 1) {
 if (sf.isMajorCompaction() && (cfTTL == Long.MAX_VALUE || oldest < cfTTL)) {
 float blockLocalityIndex =
 sf.getHDFSBlockDistribution().getBlockLocalityIndex(
 RSRpcServices.getHostname(comConf.conf, false));
 if (blockLocalityIndex < comConf.getMinLocalityToForceCompact())

{ result = true; }

else

{ LOG.debug(""Skipping major compaction of "" + regionInfo + "" because one (major) compacted file only, oldestTime "" + oldest + ""ms is < TTL="" + cfTTL + "" and blockLocalityIndex is "" + blockLocalityIndex + "" (min "" + comConf.getMinLocalityToForceCompact() + "")""); }

} else if (cfTTL != HConstants.FOREVER && oldest > cfTTL)

{ LOG.debug(""Major compaction triggered on store "" + regionInfo + "", because keyvalues outdated; time since last major compaction "" + (now - lowTimestamp) + ""ms""); result = true; }

} else

{ LOG.debug(""Major compaction triggered on store "" + regionInfo + ""; time since last major compaction "" + (now - lowTimestamp) + ""ms""); }

result = true; /****here it is not right, it should be move to the above *****/
 }
 return result;
 }

2019-09-27 09:09:35,960 DEBUG [st129,16020,1568236573216_ChoreService_1] compactions.RatioBasedCompactionPolicy: Skipping major compaction of 100E_POINT_point_2ddata_z3_geom_GpsTime_v6,\x17,1568215725799.413a563092544e8df480fd601b2de71b. because one (major) compacted file only, oldestTime 3758085589ms is < TTL=9223372036854775807 and blockLocalityIndex is 1.0 (min 0.0)
 2019-09-27 09:09:35,961 DEBUG [st129,16020,1568236573216_ChoreService_1] compactions.SortedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 100 blocking
 2019-09-27 09:09:35,961 DEBUG [st129,16020,1568236573216_ChoreService_1] regionserver.HStore: 413a563092544e8df480fd601b2de71b - d: Initiating major compaction (all files)
 2019-09-27 09:09:35,961 DEBUG [st129,16020,1568236573216_ChoreService_1] regionserver.CompactSplitThread: Large Compaction requested: org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext@4b5582f1; Because: CompactionChecker requests major compaction; use default priority; compaction_queue=(1:0), split_queue=0, merge_queue=0
 2019-09-27 09:09:35,961 INFO [regionserver/st129/10.3.72.129:16020-longCompactions-1568236575579] regionserver.HRegion: Starting compaction on d in region 100E_POINT_point_2ddata_z3_geom_GpsTime_v6,\x17,1568215725799.413a563092544e8df480fd601b2de71b.
 2019-09-27 09:09:35,961 INFO [regionserver/st129/10.3.72.129:16020-longCompactions-1568236575579] regionserver.HStore: Starting compaction of 1 file(s) in d of 100E_POINT_point_2ddata_z3_geom_GpsTime_v6,\x17,1568215725799.413a563092544e8df480fd601b2de71b. into tmpdir=hdfs://st129:8020/hbase/data/default/100E_POINT_point_2ddata_z3_geom_GpsTime_v6/413a563092544e8df480fd601b2de71b/.tmp, totalSize=5.1 G
 2019-09-27 09:09:35,961 DEBUG [regionserver/st129/10.3.72.129:16020-longCompactions-1568236575579] compactions.Compactor: Compacting hdfs://st129:8020/hbase/data/default/100E_POINT_point_2ddata_z3_geom_GpsTime_v6/413a563092544e8df480fd601b2de71b/d/3b4080f9b6f149e1b0a476058c8564e6, keycount=83914030, bloomtype=NONE, size=5.1 G, encoding=FAST_DIFF, compression=SNAPPY, seqNum=2621061, earliestPutTs=1565788490371 
 hbase shouldPerformMajorCompaction logic is not correct",13248267,"The command hadoop fs -test support seven options : -d / -e / -f / -s / -w / -r / -z.
However when see the usage of this command, only see five options. w and r are missing.
 
 Wrong usage hint for hadoop fs command test",No
13245922,"I have managed to adapt SLS wrapper into FIFO scheduler. It is currently half-working (container allocations are not traced, but all else works). Going to upload patch shortly. 
 Add SLS support for FIFO Scheduler",13297287,"Store REST API version on the backend clusters has been upgraded to 2019-12-12. This Jira will align the Driver requests to reflect this latest API version.
 
 ABFS: Upgrade Store REST API Version to 2019-12-12",No
13137288,"Deprecated HostAndPort#getHostText method was deleted in Guava 22.0and newHostAndPort#getHost method is not available before Guava 20.0.
This patch implements getHost(HostAndPort) method that extracts host from HostAndPort#toStringvalue.
This is a little hacky,that's whyI'm not sure if it worth to merge thispatch, but it could be niceif Hadoop will be Guava-neutral.
With this patch Hadoop can be built against latest Guava v24.0. 
 Make Hadoop compatible with Guava 22.0+",13164156,"org.apache.hadoop.hbase.client.Scan#setStopRow javadoc, in ""deprecated"" paragraph, points out to incorrect value. It uses pointer to withStartRow, but should use withStopRow 
 org.apache.hadoop.hbase.client.Scan#setStopRow javadoc uses incorrect method",No
13277809,"S3 does not guarantee latency. Every once in a while a request may straggle and drive latency up for the greater procedure. In these cases, simply timing-out the individual request is beneficial so that the client application can retry. The retry tends to complete faster than the original straggling request most of the time. Others experienced this issue too: https://arxiv.org/pdf/1911.11727.pdf.
S3 configuration already provides timeout facility via `ClientConfiguration#setTimeout`. Exposing this configuration is beneficial for latency sensitive applications. S3 client configuration is shared with DynamoDB client which is also affected from unreliable worst case latency.

 
 Let s3 clients configure request timeout",13139830,"org.apache.hadoop.hive.cli.control.TestDanglingQOuts 
 Remove unconnected q.out-s",No
13205433,"I have seen a couple of precommit builds across JIRAs fail in TestSSLFactory#testServerWeakCiphers with the error:

[ERROR]   TestSSLFactory.testServerWeakCiphers:240 Expected to find 'no cipher suites in common' but got unexpected exception:javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)

 
 TestSSLFactory#testServerWeakCiphers sporadically fails in precommit builds",13322743,"Move hbase-operator-tools GitHub Pull Request build to ci-hadoop Jenkins master.
The new job should be based on a Jenkinsfile similarly to the main repository. 
 Migrate hbase-operator-tools testing to ci-hadoop",No
13128460,"Within a database, to filter for tables with the string 'abc' in its name, I can use something like:


hive> use my_database;
hive> show tables '*abc*';


It would be great if I can do something similar to search within the list of columns in a table.
I have a table with around 3200 columns. Searching for the column of interest is an onerous task after doing a describe on it. 
 Make it easier to search for column name in a table",13308028,"Arrow Serializer does not properly handle Decimal primitive values when selected array is used.
In more detail, decimalValueSetter should be setting the value at arrowIndex[i] as the value at hiveIndex[j], however currently its using the same index!
https://github.com/apache/hive/blob/eac25e711ea750bc52f41da7ed3c32bfe36d4f67/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/Serializer.java#L926
This works fine for cases where i == j (selected is not used) but returns wrong decimal row values when i != j.
This ticket fixes this inconsistency and adds tests with selected indexes for all supported types 
 FIX Arrow Decimal serialization for native VectorRowBatches",No
13242673,"When I use hbck fix overlap, there is data in the region that doesn't belong to it.
reproduction steps:
1、create t1 and set COMPACTION_ENABLED=false
 2、put r001 r002 r003 r004
 3、flush t1 and flush meta
 4、 major_compact meta, and get new meta hfile1 
 5、 split t1, r0022
 6、 flush meta, and get new meta hfile2
 7、 stop cluster，delet all WALs，, delete hfile2
 8、 restart hbase
 9、 split t1,r0022，andhdfs has 5 regions(1 parent, 2 old daughter, 2 new daughter), meta has 3 region(1 parent, 2 new daughter)
 10、usehbckto repair until t1 is healthy
the result：514476289cd49db63ab7ef27e944185bhas no data.

solution:
 
 boundaries errors in the overlap after using hbck fix",13216297,"When running -fixHdfsOverlaps command due to overlap in the regions of the table ,it moves all the hfiles of overlapping regions into new region with start_key and end_key calculating based on minimum and maximum start_key and end_key of all overlapping regions.
When calculating start_key and end_key for new region,end_key with 'empty' is not considered which leads to data loss when scanned using 'startrow'.
For example:
 1.create table 't' 
 2.Insert records {00,111,200} into the table 't'and flush the data
 3.split the table 't' with split-key '100'
 4.Now we have three regions( 1 parent and two daughter regions )
 1.Region-1('Empty','Empty') => {00,111,200}
 2.Region-2('Empty','100')=>{00}
 3.Region-3('100','Empty')=>{111,200}
5.Make sure parent region is not deleted in file system and run -fixHdfsOverlaps command
This -fixHdfsOverlaps command will move all the hfiles of the three regions
{Region-1,Region- 2,Region-3} into a new region(Region-4) created with start_key='Empty' and end_key='100'
This is because it does not consider end_key='Empty' and considers end_key='100'as maximum which in turn makes all the hfiles of three regions to move into new region even if records in hfile is more than the end_key='100'and one empty region Region -5 (100,Empty)will be created because table region end key was not empty.
Now we have 2 regions:
1.Region-4(Empty,100)=>{00,111,200}
2.Region-5(100,Empty)=>{}
when the entire table scan is done, all the records will be displayed, there wont be any data loss but scan with start_key is done below are the results:
1.scan 't', { STARTROW => '00'} => {00,111,200}
2.scan 't', { STARTROW => '100'}=>{}
The second scan will give empty result because it searches the rows in
Region -5(100,Empty) which contains no records but records {111,200} is present in Region-4(Empty,100).
The problem exists only when end_key='Empty' is present in any of the overlapping regions.I think if end_key is present in any of the overlapping regions,we have to consider it as maximum end_key. 
 Ignoring 'empty' end_key while calculating end_key for new region in HBCK -fixHdfsOverlaps command can cause data loss",yes
13303535,"hive.server2.authentication.kerberos.principal set in the form of hive/_HOST@REALM,
Tez task can start at the random NM host and unfold the value of _HOST with the value of fqdn where it is running. this leads to an authentication issue.
for LLAP there is fallback for LLAP daemon keytab/principal, Kafka 1.1 onwards support delegation token and we should take advantage of it for hive on tez. 
 Release 2.2.5",13214022,"
hbase(main):001:0> move ""hbase:meta,,1.1588230740"", ""server1""

ERROR: Unknown region hbase:meta,,1.1588230740!

For usage try 'help ""move""'

Took 1.1780 seconds
hbase(main):003:0> move ""1588230740"", ""server1""
Took 84.3050 seconds

 
 shell commands don't recognize meta region's full name",No
13333250,"Catch up with the newest version. 
 Upgrade yetus to 0.12.0",13298861,"A new Yetus release is imminent. 
 Upgrade to Yetus 0.12.0",yes
13254939,"We should remove the following unsupported versions from docs and core-default.xml appropriately.
TLS v1.0
TLS v1.1
SSL v3
SSLv2Hello
ref: 
https://www.eclipse.org/jetty/documentation/9.3.27.v20190418/configuring-ssl.html
https://github.com/eclipse/jetty.project/issues/866
Akira Ajisaka, I happened to find you left TLSv1.1 in https://issues.apache.org/jira/browse/HADOOP-16000. Should we still keep it? 
 Remove Unsupported SSL/TLS Versions from Docs/Properties",13242225,"When setting a directory's permission with HDFS shell chmod, it changes the ACL mask instead of the permission bits:


$ sudo -u impala hdfs dfs -getfacl /user/hive/warehouse/exttablename/key=1/
# file: /user/hive/warehouse/exttablename/key=1
# owner: hive
# group: hive
user::rwx
user:impala:rwx	#effective:r-x
group::rwx	#effective:r-x
mask::r-x
other::r-x
default:user::rwx
default:user:impala:rwx
default:group::rwx
default:mask::rwx
default:other::rwx

$ sudo -u hdfs hdfs dfs -chmod 777 /user/hive/warehouse/exttablename/key=1/
$ sudo -u impala hdfs dfs -getfacl /user/hive/warehouse/exttablename/key=1/
# file: /user/hive/warehouse/exttablename/key=1
# owner: hive
# group: hive
user::rwx
user:impala:rwx
group::rwx
mask::rwx
other::rwx
default:user::rwx
default:user:impala:rwx
default:group::rwx
default:mask::rwx
default:other::rwx

$ sudo -u hdfs hdfs dfs -chmod 755 /user/hive/warehouse/exttablename/key=1/
$ sudo -u impala hdfs dfs -getfacl /user/hive/warehouse/exttablename/key=1/
# file: /user/hive/warehouse/exttablename/key=1
# owner: hive
# group: hive
user::rwx
user:impala:rwx	#effective:r-x
group::rwx	#effective:r-x
mask::r-x
other::r-x
default:user::rwx
default:user:impala:rwx
default:group::rwx
default:mask::rwx
default:other::rwx

$ sudo -u impala hdfs dfs -touch /user/hive/warehouse/exttablename/key=1/file
touch: Permission denied: user=impala, access=WRITE, inode=""/user/hive/warehouse/exttablename/key=1/file"":hive:hive:drwxr-xr-x


The cluster has dfs.namenode.acls.enabled=true and dfs.namenode.posix.acl.inheritance.enabled=true.
As far as I understand, the chmod should change the permission bits instead of the ACL mask. CMIIW
Might be related to HDFS-14517. István Fajth 
 chmod changes the mask when ACL is enabled",No
13183839,"Right now, in the table basic statistics, the raw data size for a row with any data type in the Parquet format is 1. This is an underestimated value when columns are complex data structures, like arrays.
Having tables with underestimated raw data size makes Hive assign less containers (mappers/reducers) to it, making the overall query slower. 
Heavy underestimation also makes Hive choose MapJoin instead of the ShuffleJoin that can fail with OOM errors.
In this patch, I compute the columns data size better, taking into account complex structures. I followed the Writer implementation for the ORC format. 
 Improve table statistics for Parquet format",13169956,"Run the following queries and you will see the raw data for the table is 4 (that is the number of fields) incorrectly. We need to populate correct data size so data can be split properly.

SET hive.stats.autogather=true;
CREATE TABLE parquet_stats (id int,str string) STORED AS PARQUET;
INSERT INTO parquet_stats values(0, 'this is string 0'), (1, 'string 1');
DESC FORMATTED parquet_stats;



Table Parameters:
	COLUMN_STATS_ACCURATE	true
	numFiles            	1
	numRows             	2
	rawDataSize         	4
	totalSize           	373
	transient_lastDdlTime	1530660523

 
 Populate more accurate rawDataSize for parquet format",yes
13303935,"Standby shutdown when doing failover, and throw IllegalStateException.
getJournaledEdits only return dfs.ha.tail-edits.qjm.rpc.max-txns edits, resulting in failure to replay all edits in catchupDuringFailover.

And check streams.isEmpty()will be throw this exception in FSEditLog#openForWrite
The exception like:



2020-05-10 09:20:02,235 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode IPC Server handler 763 on 8022: Error encountered requiring NN sh
utdown. Shutting down immediately.
java.lang.IllegalStateException: Cannot start writing at txid 173922195318 when there is a stream available for read: org.apache.hadoop.hdfs.se
rver.namenode.RedundantEditLogInputStream@47b73995
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:320)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1352)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1890)
        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1763)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1605)

 
 [SBN Read] IllegalStateException happened when doing failover",13302374,"StandbyNode is asked to transitionToActive(). If it fell too far behind in tailing journal transaction (from QJM) it can crash with IllegalStateException. 
 StandbyNode fails transition to active due to insufficient transaction tailing",yes
13268560,"I do some refactor work in HBASE-23286. Move them to a new issue. 
 Release 2.1.8",13129500,"exposed by: HIVE-18359
in case of vectorization, the summary row object was left as is (presumed null earlier); which may cause it to be inconsistent isNull conditions in .VectorHashKeyWrapperBatch
issue happens only if:

vectorizable groupby
groupping set contains empty
non-trivial empty; mapper is run
groupping key is select ; with a type which is backed by a bytea; ex:string



set hive.vectorized.execution.enabled=true;
create table tx2 (a integer,b integer,c integer,d double,u string,bi binary) stored as orc;

insert into tx2 values
(1,2,3,1.1,'x','b'),
(3,2,3,1.1,'y','b');

select  sum(a),
        u,
        bi,
        'asd',
        grouping(bi),
        'NULL,1' as expected
from    tx2
where   a=2
group by a,u,bi grouping sets ( u, (), bi);


causes:


Caused by: java.lang.NullPointerException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:173)
        at org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignRowColumn(VectorHashKeyWrapperBatch.java:1065)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeSingleRow(VectorGroupByOperator.java:1134)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$800(VectorGroupByOperator.java:74)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.close(VectorGroupByOperator.java:862)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.closeOp(VectorGroupByOperator.java:1176)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:705)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:383)
        ... 16 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1515531021543_0001_12_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
2018-01-09T12:50:30,611 DEBUG [01fdcefd-40b0-45a6-8e5b-b1cd14241088 main] ql.Driver: Shutting down query 

 
 Grouping of an empty result set may only contain null values",No
13347124,"Vulnerability fixes needed for Jetty hadoop dependency library
The jetty jars where CVEs are found are ,
================ =====
Jetty[version 9.4.20.v20190813 ]
jetty-server-9.4.20.v20190813.jar
CVE details :- [ CVE-2020-27216 ]
================ =====
Jetty-http[version 9.4.20.v20190813 ]
jetty-http-9.4.20.v20190813.jar
CVE details :- [ CVE-2020-27216 ] 
 Update Jetty hadoop dependency",13348358,"While I was looking into TestDistributedShell logs, I noticed the following Warning


2020-12-29 16:22:26,379 INFO  [Time-limited test] handler.ContextHandler (ContextHandler.java:doStart(824)) - Started o.e.j.s.ServletContextHandler@75389179{logs,/logs,file:///hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/target/log,AVAILABLE}
2020-12-29 16:22:26,380 INFO  [Time-limited test] handler.ContextHandler (ContextHandler.java:doStart(824)) - Started o.e.j.s.ServletContextHandler@116ed75c{static,/static,jar:file:~/.m2/repository/org/apache/hadoop/hadoop-yarn-common/3.4.0-SNAPSHOT/hadoop-yarn-common-3.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
2020-12-29 16:22:26,390 WARN  [Time-limited test] webapp.WebInfConfiguration (WebInfConfiguration.java:getCanonicalNameForWebAppTmpDir(794)) - Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException
2020-12-29 16:22:26,469 INFO  [Time-limited test] util.TypeUtil (TypeUtil.java:<clinit>(201)) - JVM Runtime does not support Modules


For OS X, it looks like webAppContext.setBaseResource and accessing the sources from a jar file will cause file.resource.toURI().getPath() to return null for jar:-urls
I checked that changing the jetty-version  from 9.4.20.v20190813 to something above 9.4.21 (aka., 9.4.23.v20191118) fixes the warning.
Iñigo Goiri, Akira Ajisaka, Wei-Chiu Chuang, Ayush Saxena
Do you guys think we should consider upgrading Jetty to the latest versions of 9.4.x like 9.4.35?
 
 Jetty 9.4.20 can't generate resourceBase with NPE",yes
13264090,"hive-druid-handler jar has shaded version of druid classes, druid-hdfs-storage also has non-shaded classes. 

 
[hive@hiveserver2-1 lib]$ ls |grep druid
calcite-druid-1.19.0.7.0.2.0-163.jar
druid-bloom-filter-0.15.1.7.0.2.0-163.jar
druid-hdfs-storage-0.15.1.7.0.2.0-163.jar
hive-druid-handler-3.1.2000.7.0.2.0-163.jar
hive-druid-handler.jar


Exception below - 


Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:177)
  ... 22 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:765)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:630)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)
  ... 3 more
Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:96)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:104)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:743)
  ... 6 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.copyFilesWithChecks(HdfsDataSegmentPusher.java:163)
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.push(HdfsDataSegmentPusher.java:145)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$mergeAndPush$4(AppenderatorImpl.java:747)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86)

 
 Add ability to read Druid metastore password from jceks",13284632,"Issue seen by a customer:
The failed requests we were seeing in the AbfsClient logging actually never made it out over the wire. We have found that there’s an issue with ADLS passthrough and the 8 read ahead threads that ADLSv2 spawns in ReadBufferManager.java. We depend on thread local storage in order to get the right JWT token and those threads do not have the right information in their thread local storage. Thus, when they pick up a task from the read ahead queue they fail by throwing anAzureCredentialNotFoundException exception in AbfsRestOperation.executeHttpOperation() where it calls client.getAccessToken(). This exception is silently swallowed by the read ahead threads in ReadBufferWorker.run(). As a result, every read ahead attempt results in a failedexecuteHttpOperation(), but still callsAbfsClientThrottlingIntercept.updateMetrics() and contributes to throttling (despite not making it out over the wire). After the read aheads fail, the main task thread performs the read with the right thread local storage information and succeeds, but first sleeps for up to 10 seconds due to the throttling. 
 ABFS: Send error back to client for Read Ahead request failure",No
13234744,"

beeline> select 1  union all select 2;

+----------+
| _u1._c0  |
+----------+
| 2        |
| 1        |
+----------+


The _u1 is superfluous and harmful to result schema parsing. 
 JDBC: Strip the default union prefix for un-enclosed UNION queries",13201551,"See this in the outout and then the test hang

2018-11-29 20:47:50,061 WARN  [MockRSProcedureDispatcher-pool5-t10] assignment.AssignmentManager(894): The region server localhost,102,1 is already dead, skip reportRegionStateTransition call

 
 TestAssignmentManager is flakey",No
13289579,"The parent issue upped the parallelism of the flakey reruns – see the second panel on this page https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2/lastSuccessfulBuild/artifact/dashboard.html This upped the flakie list length. It also may have overcommitted the test-running machine. Let me down the rate to something more mild. 
 [Flakey Tests] Down the flakies re-run ferocity; it makes for too many fails.",13264090,"hive-druid-handler jar has shaded version of druid classes, druid-hdfs-storage also has non-shaded classes. 

 
[hive@hiveserver2-1 lib]$ ls |grep druid
calcite-druid-1.19.0.7.0.2.0-163.jar
druid-bloom-filter-0.15.1.7.0.2.0-163.jar
druid-hdfs-storage-0.15.1.7.0.2.0-163.jar
hive-druid-handler-3.1.2000.7.0.2.0-163.jar
hive-druid-handler.jar


Exception below - 


Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:177)
  ... 22 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:765)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:630)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)
  ... 3 more
Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:96)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:104)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:743)
  ... 6 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.copyFilesWithChecks(HdfsDataSegmentPusher.java:163)
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.push(HdfsDataSegmentPusher.java:145)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$mergeAndPush$4(AppenderatorImpl.java:747)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86)

 
 Add ability to read Druid metastore password from jceks",No
13284632,"Issue seen by a customer:
The failed requests we were seeing in the AbfsClient logging actually never made it out over the wire. We have found that there’s an issue with ADLS passthrough and the 8 read ahead threads that ADLSv2 spawns in ReadBufferManager.java. We depend on thread local storage in order to get the right JWT token and those threads do not have the right information in their thread local storage. Thus, when they pick up a task from the read ahead queue they fail by throwing anAzureCredentialNotFoundException exception in AbfsRestOperation.executeHttpOperation() where it calls client.getAccessToken(). This exception is silently swallowed by the read ahead threads in ReadBufferWorker.run(). As a result, every read ahead attempt results in a failedexecuteHttpOperation(), but still callsAbfsClientThrottlingIntercept.updateMetrics() and contributes to throttling (despite not making it out over the wire). After the read aheads fail, the main task thread performs the read with the right thread local storage information and succeeds, but first sleeps for up to 10 seconds due to the throttling. 
 ABFS: Send error back to client for Read Ahead request failure",13287428,"https://github.com/apache/hive/pull/927 
 Schedule Repl Dump Task using Hive scheduler",No
13197032,"I setup a single node cluster, then trying to add node-attributes with CLI,
first I tried:


./bin/yarn nodeattributes -add localhost:hostname(STRING)=localhost


this command returns exit code 0, however the node-attribute was not added.
Then I tried to replace ""localhost"" with the host ID, and it worked.
We need to ensure the command fails with proper error message when adding was not succeed.
Similarly, when I remove a node-attribute that doesn't exist, I still get return code 0. 
 Usability improvements node-attributes CLI",13225626,"(Have been using the name HBOSS for HBase / Object Store Semantics)
I've had some thoughts about how to solve the problem of running HBase on object stores. There has been some thought in the past about adding the required semantics to S3Guard, but I have some concerns about that. First, it's mixing complicated solutions to different problems (bridging the gap between a flat namespace and a hierarchical namespace vs. solving inconsistency). Second, it's S3-specific, whereas other objects stores could use virtually identical solutions. And third, we can't do things like atomic renames in a true sense. There would have to be some trade-offs specific to HBase's needs and it's better if we can solve that in an HBase-specific module without mixing all that logic in with the rest of S3A.
Ideas to solve this above the FileSystem layer have been proposed and considered (HBASE-20431, for one), and maybe that's the right way forward long-term, but it certainly seems to be a hard problem and hasn't been done yet. But I don't know enough of all the internal considerations to make much of a judgment on that myself.
I propose a FileSystem implementation that wraps another FileSystem instance and provides locking of FileSystem operations to ensure correct semantics. Locking could quite possibly be done on the same ZooKeeper ensemble as an HBase cluster already uses (I'm sure there are some performance considerations here that deserve more attention). I've put together a proof-of-concept on which I've tested some aspects of atomic renames and atomic file creates. Both of these tests fail reliably on a naked s3a instance. I've also done a small YCSB run against a small cluster to sanity check other functionality and was successful. I will post the patch, and my laundry list of things that still need work. The WAL is still placed on HDFS, but the HBase root directory is otherwise on S3.
Note that my prototype is built on Hadoop's source tree right now. That's purely for my convenience in putting it together quickly, as that's where I mostly work. I actually think long-term, if this is accepted as a good solution, it makes sense to live in HBase (or it's own repository). It only depends on stable, public APIs in Hadoop and is targeted entirely at HBase's needs, so it should be able to iterate on the HBase community's terms alone.
Another idea Steve Loughran proposed to me is that of an inode-based FileSystem that keeps hierarchical metadata in a more appropriate store that would allow the required transactions (maybe a special table in HBase could provide that store itself for other tables), and stores the underlying files with unique identifiers on S3. This allows renames to actually become fast instead of just large atomic operations. It does however place a strong dependency on the metadata store. I have not explored this idea much. My current proof-of-concept has been pleasantly simple, so I think it's the right solution unless it proves unable to provide the required performance characteristics. 
 HBOSS: A FileSystem implementation to provide HBase's required semantics on object stores",No
13245673,"In a HDFS HA cluster with consistent reads enabled (HDFS-12943), clients could be using either ObserverReadProxyProvider, ConfiguredProxyProvider, or something else. Since observer is just a special type of SBN and we allow transitions between them, a client NOT using ObserverReadProxyProvider will need to have dfs.ha.namenodes.<nameservice> include all NameNodes in the cluster, and therefore, it may send request to a observer node.
For this case, we should check whether the stateId in the incoming RPC header is set or not, and throw an StandbyException when it is not. 
 [SBN Read] ObserverNameNode should throw StandbyException for requests not from ObserverProxyProvider",13243584,"
In Observer cluster, will configure the default proxy provider instead of ""org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider"", still Read request going to Observer namenode only.
 
 SBN : If you configure the default proxy provider still read Request going to Observer namenode only.",yes
13302552,"This Jira to track for documentation of ViewFSOverloadScheme usage guide. 
 Document the ViewFSOverloadScheme details in ViewFS guide",13129521,"exposed by: HIVE-18359
in case of vectorization, the summary row object was left as is (presumed null earlier); which may cause it to be inconsistent isNull conditions in .VectorHashKeyWrapperBatch
issue happens only if:

vectorizable groupby
groupping set contains empty
non-trivial empty; mapper is run
groupping key is select ; with a type which is backed by a bytea; ex:string



set hive.vectorized.execution.enabled=true;
create table tx2 (a integer,b integer,c integer,d double,u string,bi binary) stored as orc;

insert into tx2 values
(1,2,3,1.1,'x','b'),
(3,2,3,1.1,'y','b');

select  sum(a),
        u,
        bi,
        'asd',
        grouping(bi),
        'NULL,1' as expected
from    tx2
where   a=2
group by a,u,bi grouping sets ( u, (), bi);


causes:


Caused by: java.lang.NullPointerException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:173)
        at org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignRowColumn(VectorHashKeyWrapperBatch.java:1065)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeSingleRow(VectorGroupByOperator.java:1134)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$800(VectorGroupByOperator.java:74)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.close(VectorGroupByOperator.java:862)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.closeOp(VectorGroupByOperator.java:1176)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:705)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:383)
        ... 16 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1515531021543_0001_12_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
2018-01-09T12:50:30,611 DEBUG [01fdcefd-40b0-45a6-8e5b-b1cd14241088 main] ql.Driver: Shutting down query 

 
 upgrade to tez-0.9.1",No
13245417,"The disk with 12TB capacity is very normal today, which means the FBR size is much larger than before, BlockManager holds the NameSystemLock during processing block report for each storage, which might take quite a long time.
On our production environment, processing large FBR usually cause a longer RPC queue time, which impacts client latency, so we did some simple work on refining the lock usage, which improved the p99 latencysignificantly.
In our solution, BlockManager release the NameSystem write lock and request it again for every 5000 blocks(by default) during processing FBR, with the fair lock, all the RPC request can be processed before BlockManager re-acquire the write lock. 
 Refine NameSystem lock usage during processing FBR",13245416,"The disk with 12TB capacity is very normal today, which means the FBR size is much larger than before, BlockManager holds the NameSystemLock during processing block report for each storage, which might take quite a long time.
On our production environment, processing large FBR usually cause a longer RPC queue time, which impacts client latency, so we did some simple work on refining the lock usage, which improved the p99 latencysignificantly.
In our solution, BlockManager release the NameSystem write lock and request it again for every 5000 blocks(by default) during processing FBR, with the fair lock, all the RPC request can be processed before BlockManager re-acquire the write lock. 
 Refine NameSystem lock usage during processing FBR",yes
13289655,"https://nvd.nist.gov/vuln/detail/CVE-2019-20444

HttpObjectDecoder.java in Netty before 4.1.44 allows an HTTP header that lacks a colon, which might be interpreted as a separate header with an incorrect syntax, or might be interpreted as an ""invalid fold."" 
 Update Netty to 4.1.46Final",13286152,"[CVE-2019-20444 |https://rnd-vulncenter.huawei.com/vuln/toViewOfficialDetail?cveId=CVE-2019-20444]
[CVE-2019-16869|https://rnd-vulncenter.huawei.com/vuln/toViewOfficialDetail?cveId=CVE-2019-16869]
We should upgrade the netty dependency to 4.1.45.Final version 
 Upgrade Netty version to 4.1.45.Final to handle CVE-2019-20444,CVE-2019-16869",yes
13194693,"HADOOP-15864 fix bug about Job/Task execute failure when server (NameNode, KMS, Timeline) domain name can not resolve. meanwhile it change semantic of http status code about webhdfsfilesystem, this ticket will trace to fix TestWebHdfsFileSystemContract#testResponseCode. 
 Fix WebHdfsFileSystemContract test",13277809,"S3 does not guarantee latency. Every once in a while a request may straggle and drive latency up for the greater procedure. In these cases, simply timing-out the individual request is beneficial so that the client application can retry. The retry tends to complete faster than the original straggling request most of the time. Others experienced this issue too: https://arxiv.org/pdf/1911.11727.pdf.
S3 configuration already provides timeout facility via `ClientConfiguration#setTimeout`. Exposing this configuration is beneficial for latency sensitive applications. S3 client configuration is shared with DynamoDB client which is also affected from unreliable worst case latency.

 
 Let s3 clients configure request timeout",No
13154181,"Hive is case insensitive wrt db/table names.  These gets normalized to lower case for SQL processing.
When HiveEndPoint is created it uses db.table strings as is, and they end up propagated this way to transaction metadata tables in the metastore via lock acquisition.  This makes them look like different tables in Cleaner and lock manager. 
 Streaming Ingest API doesn't normalize db.table names",13201224,"HDFS-12130optimizes permission check, and invokes permission checker recursively for each component of the tree, which works well for FSPermission checker.
But for certain external authorizers it may be more efficient to make one call with subaccess, because often they don't have to evaluate for each and every component of the path. 
 Avoid recursive call to external authorizer for getContentSummary.",No
13217420,"I tried to install hadoop-3.2.0 on linux mint. Everything is going fine. Also java 11.0.2 is installed like this:


$ java -version java version ""11.0.2"" 2018-10-16 LTS Java(TM) SE Runtime Environment 18.9 (build 11.0.2+7-LTS) Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.2+7-LTS, mixed mode)

when I use this commandhadoop version, I get this error:


$ hadoop version log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /usr/local/hadoop-3.2.0/logs/fairscheduler-statedump.log (No such file or directory) at java.base/java.io.FileOutputStream.open0(Native Method) at java.base/java.io.FileOutputStream.open(FileOutputStream.java:298) at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237) at java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:158) at org.apache.log4j.FileAppender.setFile(FileAppender.java:294) at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768) at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526) at org.apache.log4j.LogManager.<clinit>(LogManager.java:127) at org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66) at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72) at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45) at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150) at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124) at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412) at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357) at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383) at org.apache.hadoop.util.VersionInfo.<clinit>(VersionInfo.java:37) Hadoop 3.2.0 Source code repository https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf Compiled by sunilg on 2019-01-08T06:08Z Compiled with protoc 2.5.0 From source with checksum d3f0795ed0d9dc378e2c785d3668f39 This command was run using /usr/local/hadoop-3.2.0/share/hadoop/common/hadoop-common-3.2.0.jar

It seems hadoop is properly installed but something is wrong withlog4j. May I ask you to help me to solve this error? 
 hadoop version - fairscheduler-statedump.log (No such file or directory)",13215870,"After the merge of HDFS-7240, YARN-6453 occurred again. 
 fairscheduler-statedump.log gets generated regardless of service again after the merge of HDFS-7240",yes
13171719,"

create external table e3(a integer,b string,c double);
-- goes well
insert into e3 values(1,'2',3);
-- takes a while:
insert into e3 values(1,'2',3);
-- after 2 minutes
----------------------------------------------------------------------------------------------
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.StatsTask
INFO  : Completed executing command(queryId=hive_20180712120342_6893e234-44a0-4e48-8320-f1699557bae3); Time taken: 125.276 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.StatsTask (state=08S01,code=1)



exception in metastore logs:


java.lang.ClassCastException: org.apache.hadoop.hive.metastore.api.LongColumnStatsData cannot be cast to org.apache.hadoop.hive.metastore.columnstats.cache.LongColumnStatsDataInspector
 at org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.merge(LongColumnStatsMerger.java:30) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]
 at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.mergeColStats(MetaStoreUtils.java:1084) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]
 at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.set_aggr_stats_for(HiveMetaStore.java:7514) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]
 at sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source) ~[?:?]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]
 at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]
 at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]
 at com.sun.proxy.$Proxy34.set_aggr_stats_for(Unknown Source) ~[?:?]
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_aggr_stats_for.getResult(ThriftHiveMetastore.java:17017) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_aggr_stats_for.getResult(ThriftHiveMetastore.java:17001) ~[hive-exec-3.1.0.3.0.0.0-1632.jar:3.1.0.3.0.0.0-1632]

 
 External table: exception while storing stats",13155417,"The stack trace:

2018-04-26T20:17:37,674 ERROR [pool-7-thread-11] metastore.RetryingHMSHandler: java.lang.ClassCastException: org.apache.hadoop.hive.metastore.api.LongColumnStatsData cannot be cast to org.apache.hadoop.hive.metastore.columnstats.cache.LongColumnStatsDataInspector
        at org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.merge(LongColumnStatsMerger.java:30)
        at org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.mergeColStats(MetaStoreUtils.java:1052)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.set_aggr_stats_for(HiveMetaStore.java:7202)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy26.set_aggr_stats_for(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_aggr_stats_for.getResult(ThriftHiveMetastore.java:16795)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$set_aggr_stats_for.getResult(ThriftHiveMetastore.java:16779)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

 
 StatsTask fails due to ClassCastException",yes
13279484,"`mvn package -Pdist -DskipTests` fails.

[ERROR] /Users/aajisaka/git/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/forecast/SimpleExponentialSmoothing.java:134: error: bad use of '>'
[ERROR]    * @return true if we have number of samples > kMinimumReads and the record

 
 Fix incorrect javadoc format",13279290,"-Pdist build fails due to javadoc error on SimpleExponentialSmoothing.java.

[ERROR] /ext/srcs/hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/forecast/SimpleExponentialSmoothing.java:134: error: bad use of '>'
[ERROR]    * @return true if we have number of samples > kMinimumReads and the record
[ERROR]                                                ^

 
 Fix javadoc error in SimpleExponentialSmoothing",yes
13211898,"Using murmur hash for bucketing columns was introduced in HIVE-18910, following which 'bucketing_version'='1' stands for the old behaviour (where for example integer columns were partitioned based on mod values). Looks like we have a bug in the old bucketing scheme now. I could repro it when modified the existing schema using an alter table add column and adding new data. Repro:


0: jdbc:hive2://localhost:10010> create transactional table acid_ptn_bucket1 (a int, b int) partitioned by(ds string) clustered by (a) into 2 buckets stored as ORC TBLPROPERTIES('bucketing_version'='1', 'transactional'='true', 'transactional_properties'='default');

No rows affected (0.418 seconds)

0: jdbc:hive2://localhost:10010> insert into acid_ptn_bucket1 partition (ds) values(1,2,'today'),(1,3,'today'),(1,4,'yesterday'),(2,2,'yesterday'),(2,3,'today'),(2,4,'today');
6 rows affected (3.695 seconds)


Data from ORC file (data as expected):


/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000001_0000001_0000/bucket_00000
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 536870912, ""rowId"": 0, ""currentTransaction"": 1, ""row"": {""a"": 2, ""b"": 4}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 536870912, ""rowId"": 1, ""currentTransaction"": 1, ""row"": {""a"": 2, ""b"": 3}}


/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000001_0000001_0000/bucket_00001
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 536936448, ""rowId"": 0, ""currentTransaction"": 1, ""row"": {""a"": 1, ""b"": 3}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 536936448, ""rowId"": 1, ""currentTransaction"": 1, ""row"": {""a"": 1, ""b"": 2}}


Modifying table schema and inserting new data:


0: jdbc:hive2://localhost:10010> alter table acid_ptn_bucket1 add columns(c int);

No rows affected (0.541 seconds)

0: jdbc:hive2://localhost:10010> insert into acid_ptn_bucket1 partition (ds) values(3,2,1000,'yesterday'),(3,3,1001,'today'),(3,4,1002,'yesterday'),(4,2,1003,'today'), (4,3,1004,'yesterday'),(4,4,1005,'today');
6 rows affected (3.699 seconds)


Data from ORC file (wrong partitioning):


/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000003_0000003_0000/bucket_00000
{""operation"": 0, ""originalTransaction"": 3, ""bucket"": 536870912, ""rowId"": 0, ""currentTransaction"": 3, ""row"": {""a"": 3, ""b"": 3, ""c"": 1001}}

/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000003_0000003_0000/bucket_00001
{""operation"": 0, ""originalTransaction"": 3, ""bucket"": 536936448, ""rowId"": 0, ""currentTransaction"": 3, ""row"": {""a"": 4, ""b"": 4, ""c"": 1005}}
{""operation"": 0, ""originalTransaction"": 3, ""bucket"": 536936448, ""rowId"": 1, ""currentTransaction"": 3, ""row"": {""a"": 4, ""b"": 2, ""c"": 1003}}


As seen above, the expected behaviour is that new data with column 'a' being 3 should go to bucket1 and column 'a' being 4 should go to bucket0, but the partitioning is wrong.
 
 Bucketing: Bucketing version 1 is incorrectly partitioning data",13153077,"Commons-httpclient is not supported well anymore.  Remove dependency and move to Apache HTTP client. 
 Remove commons-httpclient 3.x usage",No
13160108,"Straightforward NPE while a RS is going down:

java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1622)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1450)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:104)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.reportFileArchivalForQuotas(HRegionServer.java:3688)
	at org.apache.hadoop.hbase.regionserver.HStore.reportArchivedFilesForQuota(HStore.java:2691)
	at org.apache.hadoop.hbase.regionserver.HStore.removeCompactedfiles(HStore.java:2622)
	at org.apache.hadoop.hbase.regionserver.HStore.close(HStore.java:902)
	at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:1600)
	at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:1597)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more

FYI Michael Stack 
 NPE in HRegionServer.reportFileArchivalForQuotas",13157109,"See also : https://issues.apache.org/jira/browse/HBASE-20475?focusedCommentId=16463322&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16463322
The NPE stack is as following: 


2018-05-03 21:05:58,075 ERROR [RS_CLOSE_REGION-regionserver/instance-2:0-1] helpers.MarkerIgnoringBase(159): ***** ABORTING region server instance-2.c.gcp-hbase.internal,42063,1525381545380: Unrecoverable exception while closing region tes
t,,1525381436038.66de217a470764f3b37d8faebfd8e8c8., still finishing close *****
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1637)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1466)
        at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:104)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.reportFileArchivalForQuotas(HRegionServer.java:3709)
        at org.apache.hadoop.hbase.regionserver.HStore.reportArchivedFilesForQuota(HStore.java:2718)
        at org.apache.hadoop.hbase.regionserver.HStore.removeCompactedfiles(HStore.java:2649)
        at org.apache.hadoop.hbase.regionserver.HStore.close(HStore.java:929)
        at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:1615)
        at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:1612)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        ... 3 more


In HRegionServer#run(),  we have the following: 


@Override
public void run() {
......
    // Stop the quota manager
    if (rsQuotaManager != null) {
      rsQuotaManager.stop();
    }
    if (rsSpaceQuotaManager != null) {
      rsSpaceQuotaManager.stop();
      rsSpaceQuotaManager = null;
    }
......
    // Closing the compactSplit thread before closing meta regions
    if (!this.killed && containsMetaTableRegions()) {
      if (!abortRequested || this.fsOk) {
        if (this.compactSplitThread != null) {
          this.compactSplitThread.join();
          this.compactSplitThread = null;
        }
        closeMetaTableRegions(abortRequested);
      }
    }
......
}


We  stop the rsSpaceQuotaManager firstly, and then close the meta region, but when close meta region, we need to use rsSpaceQuotaManager to reportFileArchivalForQuotas() , just as the stack trace  said ...  
 RS may throw NPE when close meta regions in shutdown procedure. ",yes
13165239,"
When multiple views are used along with union all, it is resulting in the following error when dynamic partition pruning is enabled in tez.



Exception in thread ""main"" java.lang.AssertionError: No work found for tablescan TS[8]
 at org.apache.hadoop.hive.ql.parse.GenTezUtils.processAppMasterEvent(GenTezUtils.java:408)
 at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:383)
 at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:205)
 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10371)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:208)
 at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:239)
 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)
 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:347)
 at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1203)
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1257)
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1140)
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1130)
 at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
 at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:204)
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:433)
 at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:894)
 at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:825)
 at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:726)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


Steps to reproduce:
set hive.execution.engine=tez;
set hive.tez.dynamic.partition.pruning=true;
CREATE TABLE t1(key string, value string, c_int int, c_float float, c_boolean boolean) partitioned by (dt string);
CREATE TABLE t2(key string, value string, c_int int, c_float float, c_boolean boolean) partitioned by (dt string);
CREATE TABLE t3(key string, value string, c_int int, c_float float, c_boolean boolean) partitioned by (dt string);

insert into table t1 partition(dt='2018') values ('k1','v1',1,1.0,true);
insert into table t2 partition(dt='2018') values ('k2','v2',2,2.0,true);
insert into table t3 partition(dt='2018') values ('k3','v3',3,3.0,true);

CREATE VIEW `view1` AS select `t1`.`key`,`t1`.`value`,`t1`.`c_int`,`t1`.`c_float`,`t1`.`c_boolean`,`t1`.`dt` from `t1` union all select `t2`.`key`,`t2`.`value`,`t2`.`c_int`,`t2`.`c_float`,`t2`.`c_boolean`,`t2`.`dt` from `t2`;
CREATE VIEW `view2` AS select `t2`.`key`,`t2`.`value`,`t2`.`c_int`,`t2`.`c_float`,`t2`.`c_boolean`,`t2`.`dt` from `t2` union all select `t3`.`key`,`t3`.`value`,`t3`.`c_int`,`t3`.`c_float`,`t3`.`c_boolean`,`t3`.`dt` from `t3`;
create table t4 as select key,value,c_int,c_float,c_boolean,dt from t1 union all select v1.key,v1.value,v1.c_int,v1.c_float,v1.c_boolean,v1.dt from view1 v1 join view2 v2 on v1.dt=v2.dt;
CREATE VIEW `view3` AS select `t4`.`key`,`t4`.`value`,`t4`.`c_int`,`t4`.`c_float`,`t4`.`c_boolean`,`t4`.`dt` from `t4` union all select `t1`.`key`,`t1`.`value`,`t1`.`c_int`,`t1`.`c_float`,`t1`.`c_boolean`,`t1`.`dt` from `t1`;

select count(0) from view2 v2 join view3 v3 on v2.dt=v3.dt; // ThrowsNo work found for tablescan error 
 Dynamic partition pruning in Tez is leading to 'No work found for tablescan' error",13216222,"Create new synonym for the existing function

Mid for substr
postiion for locate 
 Create Synonym mid for  substr, position for  locate",No
13249240,"Originally reported by Christopher Tubbs: https://lists.apache.org/thread.html/db2f5d5d8600c405293ebfb3bfc415e200e59f72605c5a920a461c09@%3Cgeneral.hadoop.apache.org%3E
None of the artifacts seem to have valid detached checksum files that are in compliance withhttps://www.apache.org/dev/release-distribution There should be some "".shaXXX"" files in there, and not just the (optional) "".mds"" files. 
 Add SHA-256 or SHA-512 checksum to release artifacts to comply with the release distribution policy",13185726,"./dev-support/bin/hadoop.sh
PATCH_NAMING_RULE=""https://wiki.apache.org/hadoop/HowToContribute""


https://wiki.apache.org/hadoop/HowToContribute was moved to https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute. Let's update the personality. 
 Update PATCH_NAMING_RULE in the personality file",No
13317249,"transient failure of 


[ERROR] Tests run: 24, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 369.68 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations
[ERROR] testListingDelete[auth=false](org.apache.hadoop.fs.s3a.ITestS3GuardOutOfBandOperations)  Time elapsed: 19.103 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: No such file or directory: s3a://stevel-ireland/fork-0001/test/dir-e34a122f-a04d-48c3-90c1-9d35427fa939/file-1-e34a122f-a04d-48c3-90c1-9d35427fa939


config is unguarded codepath, s3a status was not passed down. Test run was really, really slow (8 threads)
hypothesis: test run took so long that a TTL expired and the open operation did a HEAD to s3 even when a s3guard record was found.
 
 Use More Java Collections Class",13265076,"

2019-10-28T21:07:27,599  INFO [main] conf.MetastoreConf: Unable to find config file metastore-site.xml
2019-10-28T21:07:27,599  INFO [main] conf.MetastoreConf: Found configuration file null


Prints 'unable to find' followed by 'null'.  Just print one or the other. 
 Improve Logging If Configuration File Not Found",No
13337397,"If the Cleaner didn't remove any files, don't mark the compaction queue entry as ""succeeded"" but instead leave it in ""ready for cleaning"" state for later cleaning. If it removed at least one file, then the compaction queue entry as ""succeeded"". This is a partial fix, HIVE-24291 is the complete fix. 
 compactor.Cleaner should not set state ""mark cleaned"" if it didn't remove any files",13196411,"
2018-11-06,12:55:25,980 WARN [RpcServer.default.FPBQ.Fifo.handler=251,queue=11,port=17100] org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure: Refresh peer TestPeer for TRANSIT_SYNC_REPLICATION_STATE on c4-hadoop-tst-st54.bj,17200,1541479922465 failed
java.lang.NullPointerException via c4-hadoop-tst-st54.bj,17200,1541479922465:java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.procedure2.RemoteProcedureException.fromProto(RemoteProcedureException.java:124)
	at org.apache.hadoop.hbase.master.MasterRpcServices.lambda$reportProcedureDone$4(MasterRpcServices.java:2303)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
	at org.apache.hadoop.hbase.master.MasterRpcServices.reportProcedureDone(MasterRpcServices.java:2298)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:13149)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:338)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:318)
Caused by: java.lang.NullPointerException: 
	at org.apache.hadoop.hbase.wal.SyncReplicationWALProvider.peerSyncReplicationStateChange(SyncReplicationWALProvider.java:303)
	at org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandlerImpl.transitSyncReplicationPeerState(PeerProcedureHandlerImpl.java:216)
	at org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.call(RefreshPeerCallable.java:74)
	at org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.call(RefreshPeerCallable.java:34)
	at org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler.process(RSProcedureHandler.java:47)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

 
 NPE if RS restarts between REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN and TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE",No
13155204,"This is a sub-class of LlapBaseRecordReader that wraps the socket inputStream and produces Arrow batches for an external client. 
 Provide an Arrow stream reader for external LLAP clients ",13158906,"Arrow batch serializer doesn't handle null values well in complex nested data types. 
 Null value error with complex nested data type in Arrow batch serializer",yes
13316175,"
If you want the deletion of a persistent object to cause the deletion of related objects then you need to mark the related fields in the mapping to be ""dependent"".
http://www.datanucleus.org/products/accessplatform/jdo/persistence.html#dependent_fields
http://www.datanucleus.org/products/datanucleus/jdo/persistence.html#_deleting_an_object
The database won't do it:
Derby Schema

ALTER TABLE ""APP"".""COLUMNS_V2"" ADD CONSTRAINT ""COLUMNS_V2_FK1"" FOREIGN KEY (""CD_ID"") REFERENCES ""APP"".""CDS"" (""CD_ID"") ON DELETE NO ACTION ON UPDATE NO ACTION;


https://github.com/apache/hive/blob/65cf6957cf9432277a096f91b40985237274579f/standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql#L452 
 Make ""cols"" dependent so that it cascade deletes",13235441,"Test ITestS3GuardToolDynamoDB.testDynamoDBInitDestroyCycle failing with ""wrong billing mode""
the newly inited test table is coming up with capacity (2, 2) when, because the original is (0, 0), the test now expects a PAYG table.
I think this test is going to fail until the SDK is updated and HADOOP-15563 in, at least when the source table is on demand. 
 ""Wrong Billing mode"" test failure in ITestS3GuardToolDynamoDB",No
13146677,"
2018-03-20T17:49:33,107  INFO [main] conf.MetastoreConf: MetastoreConf object:
Used hive-site file:...
Used hivemetastore-site file: ...
Key: <metastore.added.jars.path> old hive key: <hive.added.jars.path>  value: <>
Key: <metastore.aggregate.stats.cache.clean.until> old hive key: <hive.metastore.aggregate.stats.cache.clean.until>  value: <0.8>
Key: <metastore.aggregate.stats.cache.enabled> old hive key: <hive.metastore.aggregate.stats.cache.enabled>  value: <false>
Key: <metastore.aggregate.stats.cache.fpp> old hive key: <hive.metastore.aggregate.stats.cache.fpp>  value: <0.01>
Key: <metastore.aggregate.stats.cache.max.full> old hive key: <hive.metastore.aggregate.stats.cache.max.full>  value: <0.9>
...  the entire config.


Is it possible to remove this logging or reduce it to trace, or at least debug? 
 metastoreconf logs too much on info level",13132737,"Currently only source files are in the scope of checkstyle testing. We should expand the scope to include our testing code as well. 
 Enable running checkstyle on test sources as well",No
13215844,"java.lang.IllegalArgumentException <file1>,<file2> overlaps with <file3>
For example:
[ERROR] testFIFOCompactionPolicyExpiredEmptyHFiles(org.apache.hadoop.hbase.regionserver.compactions.TestFIFOCompactionPolicy) Time elapsed: 3.321 s <<< ERROR!
java.io.IOException: 
java.io.IOException: [hdfs://localhost:41525/user/apurtell/test-data/734de07d-1f22-46a9-a1f5-96ad4578450b/data/default/testFIFOCompactionPolicyExpiredEmptyHFiles/c4f673438e09d7ef5a9b79b363639cde/f/c0c5836c1f714f78847cf00326586b69, hdfs://localhost:41525/user/apurtell/test-data/734de07d-1f22-46a9-a1f5-96ad4578450b/data/default/testFIFOCompactionPolicyExpiredEmptyHFiles/c4f673438e09d7ef5a9b79b363639cde/f/c65648691f614b2d8dd4b586c5923bfe] overlaps with [hdfs://localhost:41525/user/apurtell/test-data/734de07d-1f22-46a9-a1f5-96ad4578450b/data/default/testFIFOCompactionPolicyExpiredEmptyHFiles/c4f673438e09d7ef5a9b79b363639cde/f/c0c5836c1f714f78847cf00326586b69]
 at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2438)
 at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124)
 at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:297)
 at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:277)
Caused by: java.lang.IllegalArgumentException: [hdfs://localhost:41525/user/apurtell/test-data/734de07d-1f22-46a9-a1f5-96ad4578450b/data/default/testFIFOCompactionPolicyExpiredEmptyHFiles/c4f673438e09d7ef5a9b79b363639cde/f/c0c5836c1f714f78847cf00326586b69, hdfs://localhost:41525/user/apurtell/test-data/734de07d-1f22-46a9-a1f5-96ad4578450b/data/default/testFIFOCompactionPolicyExpiredEmptyHFiles/c4f673438e09d7ef5a9b79b363639cde/f/c65648691f614b2d8dd4b586c5923bfe] overlaps with [hdfs://localhost:41525/user/apurtell/test-data/734de07d-1f22-46a9-a1f5-96ad4578450b/data/default/testFIFOCompactionPolicyExpiredEmptyHFiles/c4f673438e09d7ef5a9b79b363639cde/f/c0c5836c1f714f78847cf00326586b69]
 at com.google.common.base.Preconditions.checkArgument(Preconditions.java:119)
 at org.apache.hadoop.hbase.regionserver.HStore.addToCompactingFiles(HStore.java:1824)
 at org.apache.hadoop.hbase.regionserver.HStore.requestCompaction(HStore.java:1798)
 at org.apache.hadoop.hbase.regionserver.CompactSplitThread.selectCompaction(CompactSplitThread.java:415)
 at org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompactionInternal(CompactSplitThread.java:388)
 at org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompactionInternal(CompactSplitThread.java:317)
 at org.apache.hadoop.hbase.regionserver.CompactSplitThread.requestCompaction(CompactSplitThread.java:306)
 at org.apache.hadoop.hbase.regionserver.RSRpcServices.compactRegion(RSRpcServices.java:1513)
 at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:23649)
 at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2380)
 ... 3 more 
 TestFIFOCompactionPolicy is flaky",13159528,"Currently service upgrade involves 2 steps

initiate upgrade by providing new spec
trigger upgrade of each instance/component


We need to add the ability to upgrade the service in one shot:

Aborting the upgrade will not be supported
Upgrade finalization will be done automatically.

 
 Yarn Service Upgrade: Support express upgrade of a service",No
13145792,"When ui2 is accessed behind proxy like knox/nginx, trailing path name should not be skipped. However trim of ""ui2"" if its there. 
 [UI2] New YARN UI webapp does not respect current pathname for REST api",13289579,"The parent issue upped the parallelism of the flakey reruns – see the second panel on this page https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2/lastSuccessfulBuild/artifact/dashboard.html This upped the flakie list length. It also may have overcommitted the test-running machine. Let me down the rate to something more mild. 
 [Flakey Tests] Down the flakies re-run ferocity; it makes for too many fails.",No
13224088,"I've noticed that for some time that sometimes there are issues with javax.jms:jms:1.1 artifact - because it doesn't seem to be available from maven central for some reason;
https://issues.sonatype.org/browse/MVNCENTRAL-4708
Alternatively; I think we might try to just upgrade to 2.0.2 version of the jms-api. 
 Upgrade jms-api to 2.0.2",13330075,"The comment says it is only for internal use but MetaTableAccessor is IA.Private, so it is OK to keep this method. 
 Remove the deprecated annotation for MetaTableAccessor.getScanForTableName",No
13151276,"Uses current_date() which is prone to fail. 
 Stabilize statsoptimizer.q test",13268007,"RunningDU across lots of disks is very expensive . We applied the patchHDFS-14313 to getused space from ReplicaInfo in memory.However, new du threads throw the exception


// 2019-11-08 18:07:13,858 ERROR [refreshUsed-/home/vipshop/hard_disk/7/dfs/dn/current/BP-1203969992-XXXX-1450855658517] org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed: ReplicaCachingGetSpaceUsed refresh error
java.util.ConcurrentModificationException: Tree has been modified outside of iterator
at org.apache.hadoop.hdfs.util.FoldedTreeSet$TreeSetIterator.checkForModification(FoldedTreeSet.java:311)
at org.apache.hadoop.hdfs.util.FoldedTreeSet$TreeSetIterator.hasNext(FoldedTreeSet.java:256)
at java.util.AbstractCollection.addAll(AbstractCollection.java:343)
at java.util.HashSet.<init>(HashSet.java:120)
at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.deepCopyReplica(FsDatasetImpl.java:1052)
at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed.refresh(ReplicaCachingGetSpaceUsed.java:73)
at org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:178)
at java.lang.Thread.run(Thread.java:748)

 
 ReplicaCachingGetSpaceUsed throws  ConcurrentModificationException",No
13134627,"

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:760)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
	at java.lang.Class.getDeclaredMethods(Class.java:1975)
	at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)
	at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)
	at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)
	at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:60)
	at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)
	at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)
	at com.google.inject.AbstractModule.configure(AbstractModule.java:62)
	at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)
	at com.google.inject.spi.Elements.getElements(Elements.java:110)
	at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)
	at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)
	at com.google.inject.Guice.createInjector(Guice.java:96)
	at com.google.inject.Guice.createInjector(Guice.java:73)
	at com.google.inject.Guice.createInjector(Guice.java:62)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:379)
	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1126)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1236)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.sls.SLSRunner.startRM(SLSRunner.java:284)
	at org.apache.hadoop.yarn.sls.SLSRunner.start(SLSRunner.java:231)
	at org.apache.hadoop.yarn.sls.SLSRunner.run(SLSRunner.java:943)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.yarn.sls.SLSRunner.main(SLSRunner.java:950)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 40 more

 
 SLS failed to startup due to java.lang.NoClassDefFoundError",13133070,"


Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector

    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)

    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)

    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)

    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)

    ... 13 more

Exception in thread ""pool-2-thread-390"" java.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollector

    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:443)

    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:321)

    at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:641)

We are getting this error while running SLS. new patch of timelineservice under share/hadoop/yarn is not loaded in SLS jvm (verified from slsrunner classpath)
cc/ Rohith Sharma K S 
 SLSRunner is not loading timeline service jars causing failure",yes
13321329,"
AWS has, historically, allowed buckets with '.' in their name (along with other non-DNS valid chars)
none of which work with virtual hostname S3 clients -you have to enable path style access
which we can't do on a per-bucket basis, as the logic there doesn't support buckets with '.' in the name (think about it...)
and we can't blindly say ""use path access everywhere"", because all buckets created on/after 2020-10-01 won't work that way      dest.set(Math.max(dest.get(), sourceValue));

 
 add way for s3a to recognise buckets with ""."" in name and switch to path access",13310962,"The YARN CLI logs command line option -logFiles was changed to -log_files  in 2.9 and later releases.   This change was made as part of YARN-5363.
Verizon Media is in the process of moving from Hadoop-2.8 to Hadoop-2.10, and while testing integration with Spark, we ran into this issue.   We are concerned that we will run into more cases of this as we roll out to production, and rather than break user scripts, we'd prefer to add -logFiles as an alias of -log_files.  If both are provided, -logFiles will be ignored. 
 Add support for yarn logs -logFile to retain backward compatibility",No
13198047,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13234493,"ThrottledAsyncChecker throws NPE during block pool initialization. The error leads the block pool registration failure.
The exception

2019-05-20 01:02:36,003 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool <registering> (Datanode Uuid xxxxx) service to xx.xx.xx.xx/xx.xx.xx.xx
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult.access$000(ThrottledAsyncChecker.java:211)
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:129)
        at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:209)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3387)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1508)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:319)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:272)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:768)
        at java.lang.Thread.run(Thread.java:745)


Looks like this error due to WeakHashMap type map completedChecks has removed the target entry while we still get that entry. Although we have done a check before we get it, there is still a chance the entry is got as null. 
We met a corner case for this: A federation mode, two block pools in DN, ThrottledAsyncChecker schedules two same health checks for same volume.

2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current
2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current


completedChecks cleans up the entry for one successful check after called completedChecks#get. However, after this, another check we get the null. 
 ThrottledAsyncChecker throws NPE during block pool initialization ",yes
13188259,"The defaults for the following configs seems to be too aggressive. In java this can easily lead to several full GC pauses whose memory cannot be reclaimed.


HIVEMAPAGGRHASHMEMORY(""hive.map.aggr.hash.percentmemory"", (float) 0.99,
    ""Portion of total memory to be used by map-side group aggregation hash table""),
HIVEMAPAGGRMEMORYTHRESHOLD(""hive.map.aggr.hash.force.flush.memory.threshold"", (float) 0.9,
    ""The max memory to be used by map-side group aggregation hash table.\n"" +
    ""If the memory usage is higher than this number, force to flush data""),


We can be little bit conservative for these configs to avoid getting into GC pause. 
 pre-allocate LLAP cache at init time",13329416,"As now we support altering meta table, it will be better to also deal with changing meta replica number using altering meta, i.e, we could unify the logic in MasterMetaBootstrap to ModifyTableProcedure, and another benefit is that we do not need to restart master when changing the replica number for meta. 
 Change meta replica count by altering meta table descriptor",No
13222639,"We definitely need to capture any exception during CacheUpdateMasterWork.update(), otherwise, Java would refuse to schedule future update(). 
 Metastore cache update shall capture exception",13147870,"Command does not output anything, let's add a formatter.row call.
We should include the possible outputs in the help text.
Further improvement possibility.This command can be used for checking if the compaction is done but very impractical if one wants to wait until it is done. Wish there would be a flag in the shell that enforces synchronous compactions, that is, every time you issue a compact or major_compact in the shell while this flag is set, you won't get back the prompt until it finishes. 
 Improving shell command compaction_state",No
13256532,"You can't run s3guard prune with role DTs as we don't create it with permissons to do so.
I think it may actually be useful to have an option where we don't restrict the role. This doesn't just help with debugging, it would let things like SQS integration pick up the creds from S3A. 
 IAM role created by S3A DT doesn't include DynamoDB scan",13155536,"Hive can use statistics to answer queries like count. This can be problematic on external tables where another tool might add files that Hive doesn’t know about. In that case Hive will return incorrect results. 
 Disable compute.query.using.stats for external table",No
13171244,"After Kerberos ticket expires, RegistryDNS throws NPE error:


2018-07-06 01:26:25,025 ERROR yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(68)) - Thread Thread[TGT Renewer for rm/host1.example.com@EXAMPLE.COM,5,main] threw an Exception.

java.lang.NullPointerException

    at javax.security.auth.kerberos.KerberosTicket.getEndTime(KerberosTicket.java:482)

    at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:894)

    at java.lang.Thread.run(Thread.java:745)
 
 YARN RegistryDNS throws NPE when Kerberos tgt expires",13170998,"Found the following NPE thrown in UGI tgt renewer. The NPEwas thrown within an exception handler so the original exception was hidden, though it's likely caused by expired tgt.

18/07/02 10:30:57 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[TGT Renewer for foo@EXAMPLE.COM,5,main]
java.lang.NullPointerException
        at javax.security.auth.kerberos.KerberosTicket.getEndTime(KerberosTicket.java:482)
        at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:894)
        at java.lang.Thread.run(Thread.java:748)

Suspect it's related to https://bugs.openjdk.java.net/browse/JDK-8154889.
The relevant code was added in HADOOP-13590. File this jira to handle the exception better. 
 UserGroupInformation TGT renewer throws NPE",yes
13198321,"set a wrong AbsoluteCapacity when call ParentQueue#setAbsoluteCapacity
private void deriveCapacityFromAbsoluteConfigurations(String label,
 Resource clusterResource, ResourceCalculator rc, CSQueue childQueue) {
// 3. Update absolute capacity as a float based on parent's minResource and
 // cluster resource.
 childQueue.getQueueCapacities().setAbsoluteCapacity(label,
 (float) childQueue.getQueueCapacities().getCapacity()
 / getQueueCapacities().getAbsoluteCapacity(label));

should bechildQueue.getQueueCapacities().setAbsoluteCapacity(label,
 (float) childQueue.getQueueCapacities().getCapacity(label)
 / getQueueCapacities().getAbsoluteCapacity(label)); 
  AbsoluteCapacity is wrongly set when call  ParentQueue#setAbsoluteCapacity",13192789,"Absolute capacity should beequal to multiply capacity by parent-queue's absolute-capacity,
but currently it's calculated asdividing capacity byparent-queue's absolute-capacity.
Calculation for absolute-maximum-capacity has the same problem.
For example:
root.a capacity=0.4 maximum-capacity=0.8
root.a.a1 capacity=0.5maximum-capacity=0.6
Absolute capacity of root.a.a1 should be 0.2 but is wrongly calculated as 1.25
Absolute maximum capacity of root.a.a1 should be 0.48 but is wrongly calculated as 0.75
Moreover:
childQueue.getQueueCapacities().getCapacity() should be changedto childQueue.getQueueCapacities().getCapacity(label) to avoid getting wrong capacity from default partition when calculating for anon-default partition. 
 Absolute (maximum) capacity of level3+ queues is wrongly calculated for absolute resource",yes
13259503,"Optional should be used as a return type only. It's a neat solution for handling data that might not be present. We should avoid using Optional Anti-Patterns i.e. using it as a field or parameter type due to these reasons:
1. Using Optional parameters causing conditional logic inside the methods is not productive.
2. Packing an argument in an Optional is suboptimal for the compiler and does an unnecessary wrapping.
3. Optional field is not serializable. 
 Avoid Optional Anti-Pattern where possible",13245922,"I have managed to adapt SLS wrapper into FIFO scheduler. It is currently half-working (container allocations are not traced, but all else works). Going to upload patch shortly. 
 Add SLS support for FIFO Scheduler",No
13242897,"BlockManager#scanAndCompactStorages is called every 600sec by default.
In big cluster with thousands of datanodes, it will print out 10 thousands of informations every sec when scanAndCompactStorages() is running,which may make much noise in namenode logs. And, currently these INFO logs at namenode side and does not provide much information.
 
 Improve information on scanAndCompactStorages  in BlockManager and lower log level",13167320,"StorageInfoDefragmenter floods log when compacting StorageInfo TreeSet. In StorageInfoDefragmenter#scanAndCompactStorages, it prints for all the StorageInfo under each DN. If there are 1k nodes in cluster, and each node has 10 data dir configured, it will print 10k lines every compact interval time (10 mins). The log looks large, we could switch log level from INFO to DEBUGin StorageInfoDefragmenter#scanAndCompactStorages.

2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-329bd988-a558-43a6-b31c-9142548b0179 : 0.8222276264591439
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-b5505847-1389-4a80-b9d8-876172a83897 : 0.933351976137211
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-ca164b2f-0a2c-4b26-8e99-f0ece0909997 : 0.9330040998881849
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-89b912ba-339b-45e3-b981-541b22690ccb : 0.9314626719970249
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-89c0377b-a49c-4288-9304-e104d98de5bd : 0.9309580852251582
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-5ffad4d2-168a-446d-a92e-ef46a82f26f8 : 0.8938870614035088
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-eecbbd34-10f4-4647-8710-0f5963da3aaa : 0.8963103205353998
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-7aafa122-433f-49c8-bf00-11bcdd8ce048 : 0.8950508004926109
2018-06-19 10:18:48,849 INFO [StorageInfoMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: StorageInfo TreeSet fill ratio DS-eb9ba675-9c23-40a1-9241-c314dc0e2867 : 0.8947356866877415

 
 StorageInfoDefragmenter floods log when compacting StorageInfo TreeSet",yes
13168603,"

create or replace view v1 as select 1 as q 


results in an error:


Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Table already exists: default.v1 (state=08S01,code=1)


however the following works (thank you Miklos Gergely)


create or replace view v1 as select 1 as q union all select 1 as qq where false

 
 Fix create view over literals",13157703,"

CREATE VIEW view_s1 AS select 1;

-- FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table

 
 Create View - Table not found _dummy_table",yes
13193178,"Currently if distributed node attributes exist, even though there is no change, updating for distributed node attributes will happen in every heartbeat between NM and RM. Updating process will hold NodeAttributesManagerImpl#writeLock and may have some influence in a large cluster. We have found nodes UI of a large cluster is opened slowly and most time it's waiting for the lock in NodeAttributesManagerImpl. I think this updating should be called only when necessary to enhance the performance of related process. 
 Updating distributed node attributes only when necessary",13135691,"When NM reports its distributed attributes to RM, RM needs to do proper validation of the received attributes, if attributes were not valid or failed to update, RM needs to notify NM about such failures. Such validation needs to be also done in NM registration as well.
This JIRA tracks how NM re-act against RM response, probably we can display some info through UI/metrics/log etc. 
 Node attributes error handling based on RM validation response",yes
13249240,"Originally reported by Christopher Tubbs: https://lists.apache.org/thread.html/db2f5d5d8600c405293ebfb3bfc415e200e59f72605c5a920a461c09@%3Cgeneral.hadoop.apache.org%3E
None of the artifacts seem to have valid detached checksum files that are in compliance withhttps://www.apache.org/dev/release-distribution There should be some "".shaXXX"" files in there, and not just the (optional) "".mds"" files. 
 Add SHA-256 or SHA-512 checksum to release artifacts to comply with the release distribution policy",13189292,"Please update committer list:
Name: Janaki Lahorani
Apache ID: janaki
Organization: Cloudera 
 Update Committer List",No
13130340,"When attempting operations against a non-existent table, some of the errors that we get are very unintuitive.

$ bin/hbase shell -n <<< ""count 'no_such_table'""
2018-01-11 16:22:54,499 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Took 0.4709 seconds                                                                                                                                                                                                                           java exception
ERROR Java::OrgApacheHadoopHbase::TableNotFoundException: Region of 'hbase:namespace,,1515709347596.af9aaacccc132e8baa3dcf2874ce6d93.' is expected in the table of 'no_such_table', but hbase:meta says it is in the table of 'hbase:namespace'. hbase:meta might be damaged.


Similar errors from scan, and from interactive shell instead on noninteractive mode. 
 shell gives unfriendly errors for nonexistant table",13130326,"HBase shell doesn't have a notion of UncheckedIOException, so it may not handle it correctly. For an example, if we scan not existing table the error look weird:

hbase(main):001:0> scan 'a'
ROW                                                                                         COLUMN+CELL

ERROR: a

 
 hbase shell doesn't handle the exceptions that are wrapped in java.io.UncheckedIOException",yes
13257036,"Created a table with 2 versions , after couple of delete operations Scan return stale data 
Steps followed 
1. create 'Ashok', 
{NAME => 'cf', VERSIONS => 2}

2. put 'Ashok','r1','cf:cq','500'
3. flush 'Ashok'
4.  put 'Ashok','r1','cf:cq','600'
     put 'Ashok','r1','cf:cq','700'
     put 'Ashok','r1','cf:cq','800'
     put 'Ashok','r1','cf:cq','900'
5. delete 'Ashok','r1','cf:cq',1568694586309
At step5 we deleted value 700 by specifying Timestamp
6. flush 'Ashok'
7. delete 'Ashok','r1','cf:cq',1568694588889
At step7 we deleted value 800 by specifying Timestamp
8. flush 'Ashok'
9. scan 'Ashok',
{ VERSIONS => 10 }

At step 9  scan retuned wrong data value 900 & 500 
Expected output 900 & 600 
 
 Scan returns wrong data when version is used in CF",13204208,"Originally tested with HBase Shell delete command, but it's also reproducible with Client API Delete operation.
The problem is that the memstore scan filter logic for versions only counts the amount of cells it has read so far, then once the VERSIONS limit has been reached, it just skips the remaining cells.If a delete marker is inserted on a given cell version, that cell will not be accounted, then oldest versions that should had disappeared will now pop up on the scan results.
Example, for a cell from a CF with max versions of 3, that has 4 versions T1, T2, T3 and T4, scan correctly shows T4, T3 and T2. If a delete is triggered for any for these 3 versions, say T3, scan now will show: T4, T2 and T1, but T1 was supposed to be gone by the time T4 was added. 
 Delete for a specific cell version can bring back versions above VERSIONS limit",yes
13198047,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13198048,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
   this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
   if (completedChecks.containsKey(target))
{ 

   // here may be happen garbage collection，and result may be null.

   final LastCheckResult<V> result = completedChecks.get(target);     

   final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;  

   。。。。

}

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.
the solution is：
this.completedChecks = new ReferenceMap(1, 1);
or
 this.completedChecks = new HashMap<>();
 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",yes
13203306,"in S3AFileSystem.initialize(), we check for the bucket existing with verifyBucketExists(), which calls s3.doesBucketExist(). But that doesn't check for auth issues. 
s3. doesBucketExistV2() does at least validate credentials, and should be switched to. This will help things fail faster 
See SPARK-24000
(this is a dupe of HADOOP-15409; moving off git PRs so we can get yetus to test everything) 
 S3AFileSystem.verifyBucketExists to move to s3.doesBucketExistV2",13154831,"in S3AFileSystem.initialize(), we check for the bucket existing with verifyBucketExists(), which calls s3.doesBucketExist(). But that doesn't check for auth issues. 
s3. doesBucketExistV2() does at least validate credentials, and should be switched to. This will help things fail faster 
See SPARK-24000 
 S3AFileSystem.verifyBucketExists to move to s3.doesBucketExistV2",yes
13183916,"TestHFileArchiving#testCleaningRace creates HFileCleaner instance within the test.
When SnapshotHFileCleaner.init() is called, there is no master parameter passed in params.
When the chore runs the cleaner during the test, NPE comes out of this line in getDeletableFiles():


      return cache.getUnreferencedFiles(files, master.getSnapshotManager());


since master is null.
We should either check for the null master or, pass master instance properly when constructing the cleaner instance. 
 Partially initialized SnapshotHFileCleaner leads to NPE during TestHFileArchiving",13206348,"In FSImageFormatProtobuf.SectionName#fromString(), as follows:


public static SectionName fromString(String name) {
  for (SectionName n : values) {
    if (n.name.equals(name))
      return n;
  }
  return null;
}


When the code meets an unknown section from the fsimage, the function will return null. Callers always operates the return value with a ""switch"" clause, like FSImageFormatProtobuf.Loader#loadInternal(), as:


switch (SectionName.fromString(n))


NPE will be thrown here. 
 how to ask a question about hdfs？ I don't find the page",No
13167213,"HADOOP-13660 upgraded from commons-configuration 1.x to 2.x. commons-configuration is used when parsing the metrics configuration properties file. The new builder API used in the new version apparently makes use of a bunch of very bloated reflection and classloading nonsense to achieve the same goal, and this results in a regression of >100ms of CPU time as measured by a program which simply initializes DefaultMetricsSystem.
This isn't a big deal for long-running daemons, but for MR tasks which might only run a few seconds on poorly-tuned jobs, this can be noticeable. 
 Upgrade to commons-configuration 2.1 regresses task CPU consumption",13212603,"BackportHADOOP-15549 to branch-3.1 to fix IllegalArgumentException:

02:44:34.707 ERROR org.apache.hadoop.hive.ql.exec.Task: Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.
	at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:116)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:109)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:102)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:475)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:454)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:369)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:151)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:730)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:221)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:220)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:266)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:719)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:709)
	at org.apache.spark.sql.hive.StatisticsSuite.createNonPartitionedTable(StatisticsSuite.scala:719)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$2(StatisticsSuite.scala:822)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:284)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:283)
	at org.apache.spark.sql.StatisticsCollectionTestBase.withTable(StatisticsCollectionTestBase.scala:40)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1(StatisticsSuite.scala:821)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1$adapted(StatisticsSuite.scala:820)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.hive.StatisticsSuite.testAlterTableProperties(StatisticsSuite.scala:820)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$70(StatisticsSuite.scala:851)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:104)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:53)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:53)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1340)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1031)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	Suppressed: java.io.IOException: Failed to use org.apache.hadoop.mapred.LocalClientProtocolProvider due to error: 
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:148)
		... 78 more
	Caused by: org.apache.commons.configuration2.ex.ConfigurationRuntimeException: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:463)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:479)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:492)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResultInstance(BasicConfigurationBuilder.java:447)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResult(BasicConfigurationBuilder.java:417)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.getConfiguration(BasicConfigurationBuilder.java:285)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:119)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.create(MetricsConfig.java:98)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:478)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:188)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:163)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:62)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:58)
		at org.apache.hadoop.mapred.LocalJobRunnerMetrics.create(LocalJobRunnerMetrics.java:45)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:771)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:764)
		at org.apache.hadoop.mapred.LocalClientProtocolProvider.create(LocalClientProtocolProvider.java:42)
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:130)
		... 78 more
	Caused by: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2195)
		at org.apache.commons.beanutils.PropertyUtilsBean.setSimpleProperty(PropertyUtilsBean.java:2108)
		at org.apache.commons.beanutils.PropertyUtilsBean.setNestedProperty(PropertyUtilsBean.java:1914)
		at org.apache.commons.beanutils.PropertyUtilsBean.setProperty(PropertyUtilsBean.java:2021)
		at org.apache.commons.beanutils.BeanUtilsBean.setProperty(BeanUtilsBean.java:1018)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initProperty(BeanHelper.java:365)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBeanProperties(BeanHelper.java:273)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBean(BeanHelper.java:192)
		at org.apache.commons.configuration2.beanutils.BeanHelper$BeanCreationContextImpl.initBean(BeanHelper.java:669)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.initBeanInstance(DefaultBeanFactory.java:162)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.createBean(DefaultBeanFactory.java:116)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:459)
		... 95 more
	Caused by: java.lang.IllegalArgumentException: argument type mismatch
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2127)
		... 106 more


 
 Backport HADOOP-15549 to branch-3.0",yes
13160843,"There is hadoop version mismatch between hive and storage-api and hence different transitive dependency versions gets pulled. 
 sync up hadoop version used by storage-api with hive",13167128,"In failure testing, we stopped the KMS and then tried to run some encryption related commands.
hdfs crypto -createZonewill complain with a short ""RemoteException: Connection refused."" This message could be improved to explain that we cannot connect to the KMSClientProvier.
For example,hadoop key listwhile KMS is down will error:


 -bash-4.1$ hadoop key list
 Cannot list keys for KeyProvider: KMSClientProvider[http://hdfs-cdh5-vanilla-1.vpc.cloudera.com:16000/kms/v1/]: Connection refusedjava.net.ConnectException: Connection refused
 at java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
 at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
 at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
 at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at java.net.Socket.connect(Socket.java:579)
 at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
 at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
 at sun.net.www.http.HttpClient.New(HttpClient.java:308)
 at sun.net.www.http.HttpClient.New(HttpClient.java:326)
 at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
 at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
 at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:186)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:125)
 at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:312)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:397)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:392)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:392)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.getKeys(KMSClientProvider.java:479)
 at org.apache.hadoop.crypto.key.KeyShell$ListCommand.execute(KeyShell.java:286)
 at org.apache.hadoop.crypto.key.KeyShell.run(KeyShell.java:79)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
 at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:513)

 
 Improve error message when creating encryption zone while KMS is unreachable",No
13314885,"Currently the AcidUtils.parseBaseOrDeltaBucketFilename considers files loaded to the table as original base files. We should fix that.
Also by checking the code for AcidUtils.parseBaseOrDeltaBucketFilename, I have found 2 things:

The attribute copyNumber is not used anymore, so we should remove it
The version of the parsedDelta we use here tries to check if the files are in raw format, or not. We do not need this information here, so we can use a different implementation of parseDelta, and avoid a remote call and file read.

 
 Fix AcidUtils.parseBaseOrDeltaBucketFilename handling of data loaded by LOAD DATA",13176303,"There is a typo in the existing YarnConfiguration which uses theDEFAULT_NM_LOCALIZER_PORT as the default for NM Collector Service port. This Jira aims to fix the typo. 
 Fix NM Collector Service Port issue in YarnConfiguration",No
13135930,"In https://issues.apache.org/jira/browse/MAPREDUCE-6415, we have created a tool to combine aggregated logs into HAR files which currently only work for TFileLogAggregationFileController. We should make it support IndexedLogAggregtionController as well. 
 Make HAR tool support IndexedLogAggregtionController",13135926,"In https://issues.apache.org/jira/browse/MAPREDUCE-6415, we have created a tool to combine aggregated logs into HAR files which currently only work for TFileLogAggregationFileController. We should make it support  IndexedLogAggregtionController as well. 
 Make HAR tool support IndexedLogAggregtionController",yes
13132036,"Queries such as following can produce wrong result


select  
   count(ws_order_number)
from
   web_sales ws1
where
and exists (select *
            from web_sales ws2
            where ws1.ws_order_number = ws2.ws_order_number
              and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)
and not exists(select *
               from web_returns wr1
               where ws1.ws_order_number = wr1.wr_order_number);


This query is simplified version of tpcds query 94. Such queries are rewritten into LEFT SEMI JOIN and LEFT OUTER JOIN with residual predicate/filter (non-equi join key). Problem is that these joins are being merged, we shouldn't be merging since semi join has non-equi join filter.
Basically the underlying issue is that if a query has multiple join with LEFT SEMI JOIN with non-equi join key it is being merged with other joins. Merge logic should check such cases and avoid merging. 
 Query with EXISTS and NOT EXISTS with non-equi predicate can produce wrong result",13196326,"Currently, we instantiate /static with the default settings.
However, if this folder is behind a symbolic link, this won't load.l
This is exactly the same issue and solution as described in GEODE-5445. 
 Allow HttpServer2 to discover resources in /static when symlinks are used",No
13159283,"Without closing ctx in testMarkSuspectBlock, testIgnoreMisplacedBlock, testAppendWhileScanning, some tests fail on Windows:
[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestBlockScanner
[ERROR] Tests run: 14, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 113.398 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestBlockScanner
[ERROR] testScanAllBlocksWithRescan(org.apache.hadoop.hdfs.server.datanode.TestBlockScanner) Time elapsed: 0.031 s <<< ERROR!
java.io.IOException: Could not fully delete E:\OSS\hadoop-branch-2\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\name1
 at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1047)
 at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883)
 at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)
 at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473)
 at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner$TestContext.<init>(TestBlockScanner.java:102)
 at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testScanAllBlocksImpl(TestBlockScanner.java:366)
 at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testScanAllBlocksWithRescan(TestBlockScanner.java:435)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
...
[INFO]
[INFO] Results:
[INFO]
[ERROR] Errors:
[ERROR] TestBlockScanner.testAppendWhileScanning:899 ╗ IO Could not fully delete E:\OS...
[ERROR] TestBlockScanner.testCorruptBlockHandling:488 ╗ IO Could not fully delete E:\O...
[ERROR] TestBlockScanner.testDatanodeCursor:531 ╗ IO Could not fully delete E:\OSS\had...
[ERROR] TestBlockScanner.testMarkSuspectBlock:717 ╗ IO Could not fully delete E:\OSS\h...
[ERROR] TestBlockScanner.testScanAllBlocksWithRescan:435->testScanAllBlocksImpl:366 ╗ IO
[ERROR] TestBlockScanner.testScanRateLimit:450 ╗ IO Could not fully delete E:\OSS\hado...
[ERROR] TestBlockScanner.testVolumeIteratorWithCaching:261->testVolumeIteratorImpl:169 ╗ IO
[ERROR] TestBlockScanner.testVolumeIteratorWithoutCaching:256->testVolumeIteratorImpl:169 ╗ IO
[INFO]
[ERROR] Tests run: 14, Failures: 0, Errors: 8, Skipped: 0 
 Enable TestMiniLlapLocalCliDriver#special_character_in_tabnames_1.q",13318434,"
move similar queries to unit tests into the parser module and keep only one in the q test.
use explain instead of executing the queries if possible since we are focusing on parser testing

 
 Simplify special_character_in_tabnames_1.q",yes
13261032,"Tried to use the hbase-thirdparty lib. Ran into this issue https://github.com/eclipse/jetty.project/issues/3244 when I tried to update core to use the new hbase-thirdparty (A single failed unit test, TestProtobufUtil, complained of missing ByteBuffer position method when buffer was offheap). Pushed a recompiled artifact, one that uses the 3.1.0 tag., built with jdk8 and that works if local repo but can't override published artifaict.  Let me make a new one.
Need an issue else the RC scripts fail. Let this be it. Resovling against hbase-thirdparty-3.1.1. 
 Squash hbase-thirdparty-3.1.0; was compiled w/ jdk10 so ""NoSuchMethodError: java.nio.ByteBuffer.*""",13291672,"See this discussion on mailing list
https://lists.apache.org/thread.html/rbab2e79f25b38c85fe205390b3fdbc6711984881a994a499c54aee97%40%3Cdev.hbase.apache.org%3E 
 NettyRpcClientConfigHelper will not share event loop by default which is incorrect",No
13155468,"System monitoring software usually send a tcp packet to test if port is alive.  This can cause RegistryDNS to throw BufferUnderflowException.


2018-04-26 17:07:55,846 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Execution exception when running task in RegistryDNS 3
2018-04-26 17:07:55,847 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread RegistryDNS 3:
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:151)
        at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:771)
        at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)
        at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)


This is perfectly normal, but it would be nice to hide this error message to reduce verbose logging on port ping. 
 Reduce RegistryDNS port ping logging",13155293,"Yarn registry dns server is constantly getting BufferUnderflowException. 


2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(50)) - Execution exception when running task in RegistryDNS 76

2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread RegistryDNS 76:

java.nio.BufferUnderflowException

    at java.nio.Buffer.nextGetIndex(Buffer.java:500)

    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)

    at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)

    at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)

    at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)

    at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)

    at java.util.concurrent.FutureTask.run(FutureTask.java:266)

    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

    at java.lang.Thread.run(Thread.java:748)

 
 Yarn registry dns log finds BufferUnderflowException on port ping",yes
13348170,"TestDistributedShell has grown so large over time. It has 29 tests.
 This is running the risk of exceeding 30 minutes limit for a single unit class.

The implementation has lots of code redundancy.
The Jira splitsTestDistributedShell into three different unitTest for each TimeLineVersion: V1.0, 1.5, and 2.0
Fixes the broken test testDSShellWithEnforceExecutionType

 
 Refactor TestDistributedShell",13275126,"
org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithOpportunisticContainers
org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShellWithEnforceExecutionType

Please see the Apache Jenkins Test result:
https://builds.apache.org/job/hadoop-multibranch/job/PR-1767/1/testReport/

These 2 tests are failed on both X86 and ARM platform. 
 DistributedShell test failure on X86 and ARM",yes
13253419,"capacity-scheduler


...
yarn.scheduler.capacity.root.dev.maximum-application-lifetime=-1
yarn.scheduler.capacity.root.dev.default-application-lifetime=604800


refreshQueue was failed as follows


2019-08-28 15:21:57,423 WARN  resourcemanager.AdminService (AdminService.java:logAndWrapException(910)) - Exception refresh queues.
java.io.IOException: Failed to re-init queues : Default lifetime604800 can't exceed maximum lifetime -1
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:477)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:423)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:394)
        at org.apache.hadoop.yarn.server.api.impl.pb.service.ResourceManagerAdministrationProtocolPBServiceImpl.refreshQueues(ResourceManagerAdministrationProtocolPBServiceImpl.java:114)
        at org.apache.hadoop.yarn.proto.ResourceManagerAdministrationProtocol$ResourceManagerAdministrationProtocolService$2.callBlockingMethod(ResourceManagerAdministrationProtocol.java:271)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
Caused by: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Default lifetime604800 can't exceed maximum lifetime -1
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs(LeafQueue.java:268)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:162)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.<init>(LeafQueue.java:141)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.parseQueue(CapacitySchedulerQueueManager.java:259)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.parseQueue(CapacitySchedulerQueueManager.java:283)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager.reinitializeQueues(CapacitySchedulerQueueManager.java:171)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:726)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:472)
        ... 12 more

 
 Failed to set default-application-lifetime if maximum-application-lifetime is less than or equal to zero",13327596,"When we enable fallback, rename should success if the src.parent or dst.parent on inernalDir.

org.apache.hadoop.security.AccessControlException: InternalDir of ViewFileSystem is readonly, operation rename not permitted on path /newFileOnRoot.org.apache.hadoop.security.AccessControlException: InternalDir of ViewFileSystem is readonly, operation rename not permitted on path /newFileOnRoot.
 at org.apache.hadoop.fs.viewfs.ViewFileSystem.readOnlyMountTable(ViewFileSystem.java:95) at org.apache.hadoop.fs.viewfs.ViewFileSystem.readOnlyMountTable(ViewFileSystem.java:101) at org.apache.hadoop.fs.viewfs.ViewFileSystem.rename(ViewFileSystem.java:683) at org.apache.hadoop.hdfs.ViewDistributedFileSystem.rename(ViewDistributedFileSystem.java:533) at org.apache.hadoop.hdfs.TestViewDistributedFileSystemWithMountLinks.verifyRename(TestViewDistributedFileSystemWithMountLinks.java:114) at org.apache.hadoop.hdfs.TestViewDistributedFileSystemWithMountLinks.testRenameOnInternalDirWithFallback(TestViewDistributedFileSystemWithMountLinks.java:90) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
 
 Fix the rename issues with fallback fs enabled",No
13330075,"The comment says it is only for internal use but MetaTableAccessor is IA.Private, so it is OK to keep this method. 
 Remove the deprecated annotation for MetaTableAccessor.getScanForTableName",13161552,"For some uses cases it is necessary to know the output schema for a HiveQL before executing the query. But there is no existing client API that provides this information.
Hive JDBC doesn't provide the schema for parametric types in ResultSetMetaData.
GenericUDTFGetSplits bundles the proper schema metadata with the fragments for input splits. An option can be added to return only the schema metadata from compilation, and the generation of input splits can be skipped.
 
 Provide option for GenericUDTFGetSplits to return only schema metadata",No
13128365,"NameNode crashes repeatedly with NPE at the startup when trying to find the total number of under construction blocks. This crash happens after an open file, which was also part of a snapshot gets deleted along with the snapshot.

Failed to start namenode.
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.LeaseManager.getNumUnderConstructionBlocks(LeaseManager.java:146)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCompleteBlocksTotal(FSNamesystem.java:6537)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startCommonServices(FSNamesystem.java:1232)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:706)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:692)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)


 
 NameNode crashes during restart after an OpenForWrite file present in the Snapshot got deleted",13129716,"This is probably caused by some other issue as it implies a 0-sized range being in the list. However, it's good to handle this correctly for now. 
 LLAP IO: InputStream may return 0 bytes",No
13264443,"Replace ""/"" with constant NodeBase.PATH_SEPARATOR_STR 
 Refactor DFSNetworkTopology#isNodeInScope",13167128,"In failure testing, we stopped the KMS and then tried to run some encryption related commands.
hdfs crypto -createZonewill complain with a short ""RemoteException: Connection refused."" This message could be improved to explain that we cannot connect to the KMSClientProvier.
For example,hadoop key listwhile KMS is down will error:


 -bash-4.1$ hadoop key list
 Cannot list keys for KeyProvider: KMSClientProvider[http://hdfs-cdh5-vanilla-1.vpc.cloudera.com:16000/kms/v1/]: Connection refusedjava.net.ConnectException: Connection refused
 at java.net.PlainSocketImpl.socketConnect(Native Method)
 at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
 at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
 at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
 at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
 at java.net.Socket.connect(Socket.java:579)
 at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
 at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
 at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
 at sun.net.www.http.HttpClient.New(HttpClient.java:308)
 at sun.net.www.http.HttpClient.New(HttpClient.java:326)
 at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
 at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
 at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:186)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:125)
 at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
 at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:312)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:397)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:392)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:415)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:392)
 at org.apache.hadoop.crypto.key.kms.KMSClientProvider.getKeys(KMSClientProvider.java:479)
 at org.apache.hadoop.crypto.key.KeyShell$ListCommand.execute(KeyShell.java:286)
 at org.apache.hadoop.crypto.key.KeyShell.run(KeyShell.java:79)
 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
 at org.apache.hadoop.crypto.key.KeyShell.main(KeyShell.java:513)

 
 Improve error message when creating encryption zone while KMS is unreachable",No
13302944,"[ERROR] Errors: 
[ERROR] TestJMXListener.setupBeforeClass:61 » IO Shutting down 
 TestJMXListener.setupBeforeClass can fail due to not getting a random port.",13302942,"The TestJMXListener can fail because the random port it wants to put the jmx listener on is occupied when it goes to run. Handle this case in test startup. 
 [Flakey Test] TestJMXListener rmi port clash",yes
13180212,"Not sure which commit broke it but it is on the top of the flaky list. We use a method in TestEndToEndSplitTransition, but the method will use its own HBaseTestingUtility which is not initialized so NPE. Need to rewrite. 
 NPE in TestTableResource",13179984,"The following can be observed in master branch:


java.lang.NullPointerException
	at org.apache.hadoop.hbase.rest.TestTableResource.setUpBeforeClass(TestTableResource.java:134)


The NPE comes from the following in TestEndToEndSplitTransaction :


        compactAndBlockUntilDone(TEST_UTIL.getAdmin(),
          TEST_UTIL.getMiniHBaseCluster().getRegionServer(0), daughterA.getRegionName());


Initial check of the code shows that TestEndToEndSplitTransaction uses TEST_UTIL instance which is created within TestEndToEndSplitTransaction. However, TestTableResource creates its own instance of HBaseTestingUtility.
Meaning TEST_UTIL.getMiniHBaseCluster() would return null, since the instance created by TestEndToEndSplitTransaction has hbaseCluster as null. 
 TestTableResource fails with NPE",yes
13215668,"Hadoop release Infoshould be updated to ""Hadoop 2019"" in Namenode Web UI
Steps :-
-Install the cluster with Release 3.1.2

Check the Namenode Web-UI
Hadoop Info is still displaying as ""Hadoop 2018""

Actual Result :- 
 Hadoop Info is still displaying as ""Hadoop 2018""
Expected Result :-
 Hadoop Info needs to be updated as ""Hadoop 2019""
 
 Hadoop release Info Needs to be updated to ""Hadoop 2019"" in Namenode Web UI",13207113,"Update the year to 2019 from 2018. 
 Update the year to 2019",yes
13198035,"A vendor might need a customized scheduling policy for their devices. It could be scheduled based on topology, resource utilization, virtualization, device attribute and so on.
We'll provide another optional interface ""DevicePluginScheduler"" for the vendor device plugin to implement. Once it's implemented, the framework will prefer it to the default scheduler.
Thiswould bring more flexibility to the framework's scheduling mechanism. 
 DataNode runs async disk checks  maybe  throws NullPointerException In ThrottledAsyncChecker.java  ",13234493,"ThrottledAsyncChecker throws NPE during block pool initialization. The error leads the block pool registration failure.
The exception

2019-05-20 01:02:36,003 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool <registering> (Datanode Uuid xxxxx) service to xx.xx.xx.xx/xx.xx.xx.xx
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult.access$000(ThrottledAsyncChecker.java:211)
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:129)
        at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:209)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3387)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1508)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:319)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:272)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:768)
        at java.lang.Thread.run(Thread.java:745)


Looks like this error due to WeakHashMap type map completedChecks has removed the target entry while we still get that entry. Although we have done a check before we get it, there is still a chance the entry is got as null. 
We met a corner case for this: A federation mode, two block pools in DN, ThrottledAsyncChecker schedules two same health checks for same volume.

2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current
2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current


completedChecks cleans up the entry for one successful check after called completedChecks#get. However, after this, another check we get the null. 
 ThrottledAsyncChecker throws NPE during block pool initialization ",yes
13160335,"Add some useful stuff and some refinement to PE tool
1. Add multiPut support
Though we have BufferedMutator, sometimes we need to benchmark batch put in a certain number.
Set --multiPut=number to enable batchput(meanwhile, --autoflush need be set to false)
2. Add Connection Number support
Before, there is only on parameter to control the connection used by threads. oneCon=true means all threads use one connection, false means each thread has it own connection.
When thread number is high and oneCon=false, we noticed high context switch frequency in the machine which PE run on, disturbing the benchmark results(each connection has its own netty worker threads, 2*CPU IIRC).  
So, added a new parameter connCount to PE. set --connCount=2 means all threads will share 2 connections.
3. Add avg RT and avg TPS/QPS statstic for all threads
Useful when we want to meansure the total throughtput of the cluster
4. Delete some redundant code
Now RandomWriteTest is inherited from SequentialWrite. 
 Add multiPut support and other miscellaneous to PE",13263492,"This is a really simple UI change proposal:
 The icon of ""Decommissioned & dead"" datanode could be improved. It can be changed from   to   so that,

icon ""   "" can be used for all status starts with ""decommission"" on dfshealth.html,
icon ""   "" can be differentiated with icon ""   "" onfederationhealth.html




DataNode Infomation Legend(now)
 dfshealth.html#tab-datanode



DataNode Infomation Legend(proposed)
 dfshealth.html#tab-datanode



NameService Legend
 federationhealth.htm#tab-namenode




 
 Change the ICON of ""Decommissioned & dead"" datanode on ""dfshealth.html""",No
13129081,"Build is failing on me (Trying to cut beta-1 RC on branch-2). It is first time we go to use the jars made by hbase-checkstyle in the hbase-error-prone module under 'build support' module when running the 'site' target. It is trying to make the checkstyle report.
I see that we find the right jar to read:
[DEBUG] The resource 'hbase/checkstyle-suppressions.xml' was found as jar:file:/home/stack/rc/hbase-2.0.0-beta-1.20180107T061305Z/repository/org/apache/hbase/hbase-checkstyle/2.0.0-beta-1/hbase-checkstyle-2.0.0-beta-1.jar!/hbase/checkstyle-suppressions.xml.
But then it thinks the jar corrupt 'ZipException: invalid distance too far back'.
Here is mvn output:
12667058 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (checkstyle) on project hbase-error-prone: Failed during checkstyle executi         on: Unable to process suppressions file location: hbase/checkstyle-suppressions.xml: Cannot create file-based resource:invalid distance too far back -> [Help 1]
12667059 org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (checkstyle) on project hba         se-error-prone: Failed during checkstyle execution
I'm running this command:
mvn -X install -DskipTests site assembly:single -Papache-release -Prelease -Dmaven.repo.local=//home/stack/rc/hbase-2.0.0-beta-1.20180107T061305Z/repository
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T08:41:47-08:00) 
Java version: 1.8.0_151, vendor: Oracle Corporation 
 Build fails, unable to read hbase/checkstyle-suppressions.xml ""invalid distance too far back""",13301696,"HADOOP-16621 removed two public API methods:
1. o.a.h.security.token.Token(TokenProto tokenPB) --> replaced by o.a.h.security.ipc.ProtobufHelper.tokenFromProto()
2. o.a.h.security.token.Token.toTokenProto() --> replaced by o.a.h.security.ipc.ProtobufHelper.protoFromToken()
Protobuf is declared private. Should we make it public now? 
 Declare ProtobufHelper a public API",No
13278403,"PUT requests in HttpFS with path as ""/"" were not supported .
 
 HttpFS: put requests are not supported for path ""/""",13151016,"Hi,
I compiled a draft document for the HBase incompatibilities from the raw source content that was available in HBase Beta 1 site. Can someone please review and provide a feedbackorshare your comments on this document?
Appreciate your support and time.

Best Regards,
Triguna 
 Document incompatibilities between HBase 1.x and HBase 2.0",No
13251225,"Currently, all map containers are requests as soon as Application master comes up and then all reducer containers are requested. This doesn't get flexibility to simulate behavior of DAG where various number of containers would be requested at different time. 
 Add support for arbitrary DAG AM Simulator.",13342498,"https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2488/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn.txt

[ERROR] Failures: 
[ERROR]   TestRouterWebServicesREST.testAppAttemptXML:720->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppPriorityXML:796->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppQueueXML:846->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppStateXML:744->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppTimeoutXML:920->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppTimeoutsXML:896->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppXML:696->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testUpdateAppPriorityXML:832 expected:<200> but was:<500>
[ERROR]   TestRouterWebServicesREST.testUpdateAppQueueXML:882 expected:<200> but was:<500>
[ERROR]   TestRouterWebServicesREST.testUpdateAppStateXML:782 expected:<202> but was:<500>
[ERROR] Errors: 
[ERROR]   TestRouterWebServicesREST.testGetAppAttemptXML:1292->getAppAttempt:1464 Â» ClientHandler
[ERROR]   TestRouterWebServicesREST.testGetAppsMultiThread:1337->testGetContainersXML:1317->getAppAttempt:1464 Â» ClientHandler
[ERROR]   TestRouterWebServicesREST.testGetContainersXML:1317->getAppAttempt:1464 Â» ClientHandler 
 
 TestRouterWebServicesREST fails",No
13198046,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13234493,"ThrottledAsyncChecker throws NPE during block pool initialization. The error leads the block pool registration failure.
The exception

2019-05-20 01:02:36,003 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool <registering> (Datanode Uuid xxxxx) service to xx.xx.xx.xx/xx.xx.xx.xx
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$LastCheckResult.access$000(ThrottledAsyncChecker.java:211)
        at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker.schedule(ThrottledAsyncChecker.java:129)
        at org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.checkAllVolumes(DatasetVolumeChecker.java:209)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3387)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1508)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:319)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:272)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:768)
        at java.lang.Thread.run(Thread.java:745)


Looks like this error due to WeakHashMap type map completedChecks has removed the target entry while we still get that entry. Although we have done a check before we get it, there is still a chance the entry is got as null. 
We met a corner case for this: A federation mode, two block pools in DN, ThrottledAsyncChecker schedules two same health checks for same volume.

2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current
2019-05-20 01:02:36,000 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/2/hdfs/data/current


completedChecks cleans up the entry for one successful check after called completedChecks#get. However, after this, another check we get the null. 
 ThrottledAsyncChecker throws NPE during block pool initialization ",yes
13198046,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
  this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
  if (completedChecks.containsKey(target)) 
{
    // here may be happen garbage collection，and result may be null.
    final LastCheckResult<V> result = completedChecks.get(target);
    final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;
  }

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.

 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",13198048,"InThrottledAsyncChecker class，it members of thecompletedChecks is WeakHashMap, its definition is as follows：
   this.completedChecks =new WeakHashMap<>();
and one of its uses is as follows inschedule method:
   if (completedChecks.containsKey(target))
{ 

   // here may be happen garbage collection，and result may be null.

   final LastCheckResult<V> result = completedChecks.get(target);     

   final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;  

   。。。。

}

after""completedChecks.containsKey(target)""， may be happen garbage collection， and result may be null.
the solution is：
this.completedChecks = new ReferenceMap(1, 1);
or
 this.completedChecks = new HashMap<>();
 
 DataNode runs async disk checks  maybe  throws NullPointerException, and DataNode failed to register to NameSpace.",yes
13159430,"One possible scenario for HashTable/SyncTable is for synchronize different clusters, for instance, when replication has been enabled but data existed already, or due replication issues that may had caused long lags in the replication.
For secured clusters under different kerberos realms (with cross-realm properly set), though, current SyncTable version would fail to authenticate with the remote cluster when trying to read HashTable outputs (when sourcehashdiris remote) and also when trying to read table data on the remote cluster (whensourcezkclusteris remote).
The hdfs error would look like this:

INFO mapreduce.Job: Task Id : attempt_1524358175778_105392_m_000000_0, Status : FAILED

Error: java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: ""local-host/1.1.1.1""; destination host is: ""remote-nn"":8020;
    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
    at org.apache.hadoop.ipc.Client.call(Client.java:1506)
    at org.apache.hadoop.ipc.Client.call(Client.java:1439)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
    at com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:256)
...
    at org.apache.hadoop.hbase.mapreduce.HashTable$TableHash.readPropertiesFile(HashTable.java:144)
    at org.apache.hadoop.hbase.mapreduce.HashTable$TableHash.read(HashTable.java:105)
    at org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper.setup(SyncTable.java:188)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)
...
Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]

The above can be sorted if the SyncTable job acquires a DT for the remote NN. Once hdfs related authentication is done, it's also necessary to authenticate against remote HBase, as the below error would arise:

INFO mapreduce.Job: Task Id : attempt_1524358175778_172414_m_000000_0, Status : FAILED
Error: org.apache.hadoop.hbase.client.RetriesExhaustedException: Can't get the location
at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:326)
...
at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867)
at org.apache.hadoop.hbase.mapreduce.SyncTable$SyncMapper.syncRange(SyncTable.java:331)
...
Caused by: java.io.IOException: Could not set up IO Streams to remote-rs-host/1.1.1.2:60020
at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:786)
...
Caused by: java.lang.RuntimeException: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.
...
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
...

The above would need additional authentication logic against the remote hbase cluster. 
 SyncTable tool: Add support for cross-realm remote clusters",13140108,"Left & Right outer joins without keys are valid in SQL and they have different semantics from cross-products


create temporary table foo(x int) stored as orc;
insert into foo values(1),(2);
create temporary table bar(y int) stored as orc;
select count(*) from bar right outer join foo; -- = 2
select count(*) from bar, foo; -- = 0 


canSpecializeMapJoin should bail on these cases. 
 Vectorization: Disable vectorization of key-less outer joins",No
13178910,"

select current_date + 5;

FAILED: SemanticException [Error 10014]: Line 1:7 Wrong arguments '5': No matching method for class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDTIPlus with (date, int)


This works in Postgres 9.6 - http://sqlfiddle.com/#!17/9eecb/19253/0 
 Date: date + int fails to add days",13151249,"Missed in HIVE-18859 
 Update golden files for negative tests",No
13314885,"Currently the AcidUtils.parseBaseOrDeltaBucketFilename considers files loaded to the table as original base files. We should fix that.
Also by checking the code for AcidUtils.parseBaseOrDeltaBucketFilename, I have found 2 things:

The attribute copyNumber is not used anymore, so we should remove it
The version of the parsedDelta we use here tries to check if the files are in raw format, or not. We do not need this information here, so we can use a different implementation of parseDelta, and avoid a remote call and file read.

 
 Fix AcidUtils.parseBaseOrDeltaBucketFilename handling of data loaded by LOAD DATA",13137348,"See https://builds.apache.org/job/HBase%20Nightly/job/branch-2/284/testReport/junit/org.apache.hadoop.hbase.client/TestAsyncRegionAdminApi/testMergeRegions_0_/

java.lang.AssertionError: expected:<2> but was:<3> at org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi.testMergeRegions(TestAsyncRegionAdminApi.java:359)

Merge regions not work. The table still have 3 regions after the MergeRegionsProcedure finished.
The master start balance region 9e2773ba1efba79a2defa276e9a26ed4. But because the MergeRegionsProcedure pid=138 start work first, so the balance need wait for the lock. But after merge regions finished, the MoveRegionProcedure pid=139 start work and assign 9e2773ba1efba79a2defa276e9a26ed4 to a new region server. This is not right. The MoveRegionProcedure should skip to assign a region which was marked as offline. Or we should clear the merged regions' procedure whenMergeRegionsProcedure finished.

Logs:
2018-02-08 16:24:44,608 INFO [master/cd4730e3eae2:0.Chore.1] master.HMaster(1454): balance hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318
2018-02-08 16:24:44,608 DEBUG [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=37885] procedure2.ProcedureExecutor(868): Stored pid=138, state=RUNNABLE:MERGE_TABLE_REGIONS_PREPARE; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false
......
2018-02-08 16:24:50,111 INFO [PEWorker-13] procedure2.ProcedureExecutor(1249): Finished pid=138, state=SUCCESS; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false in 5.5710sec
2018-02-08 16:24:50,113 INFO [PEWorker-13] procedure.MasterProcedureScheduler(813): pid=139, state=RUNNABLE:MOVE_REGION_UNASSIGN; MoveRegionProcedure hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318 testMergeRegions testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4.
 
 Fix flaky TestAsyncRegionAdminApi",No
13241903,"In Safemode, when  namenoe process block reports, the BlockManagerSafeMode check the live  datanode thresholds.In our cluster, this threshold use default zero value.  According to previous version, it is not essential to get num of live datanodes. And it is very time-consuming when processing every reported block. 
  BlockManagerSafeMode should avoid to check datanode thresholds with default zero value.",13206252,"Stack:


Thread 456 (Edit log tailer):
State: RUNNABLE
Blocked count: 1139
Waited count: 12
Stack:
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getNumLiveDataNodes(DatanodeManager.java:1259)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.areThresholdsMet(BlockManagerSafeMode.java:570)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.checkSafeMode(BlockManagerSafeMode.java:213)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode.adjustBlockTotals(BlockManagerSafeMode.java:265)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.completeBlock(BlockManager.java:1087)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.forceCompleteBlock(BlockManager.java:1118)
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.updateBlocks(FSEditLogLoader.java:1126)
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:468)
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:258)
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:161)
org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:892)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:321)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:460)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:410)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:427)
org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:414)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:423)
Thread 455 (pool-16-thread-1):




code:


private boolean areThresholdsMet() {
  assert namesystem.hasWriteLock();
  int datanodeNum = blockManager.getDatanodeManager().getNumLiveDataNodes();
  synchronized (this) {
    return blockSafe >= blockThreshold && datanodeNum >= datanodeThreshold;
  }
}


According to the code, each time the method areThresholdsMet() is called, the value of datanodeNum is need to be calculated. However, in the scenario of datanodeThreshold is equal to 0（0 is the default value of the configuration）, This expression datanodeNum >= datanodeThreshold always returns true.
Calling the method getNumLiveDataNodes() is time consuming at a scale of 10,000 datanode clusters. Therefore, we add the judgment condition, and only when the datanodeThreshold is greater than 0, the datanodeNum is calculated, which improves the perfomance greatly.
The Call Treegraphis shown in the attached file.
 
 Performance improvement in Tailing EditLog",yes
13284779,"hdfs-2.10.0-webapps-secondary-status.html miss moment.js
 
 hdfs-2.10.0-webapps-secondary-status.html miss moment.js",13130005,"Let's upgrade to version 2.9.4 
 Upgrade version of com.fasterxml.jackson",No
13288866,"Currently when a table is dropped we are deriving if it was used by any materialized views from the error message got from the Metastore. Instead of this we should check it in advance, from theHiveMaterializedViewsRegistry. 
 Fix checking if a table is used by a materialized view before dropping",13330352,"I have discovered that it's possible to drop a table used by a materialized view. When I drop this table, the result is OK while I think this action should be refused. When I check in the metastore database, I can see that the table has been partially deleted (ie : the reference of the table still exists inTBLS and in MV_TABLES_USED). This introduces an inconsistency in the metastore.
Steps to reproduced :



jdbc:hive2://localhost.> use use ptest2_db_dev;
No rows affected (0.067 seconds)
0: jdbc:hive2://localhost.> create table table_blocked (id string);
No rows affected (0.97 seconds)
0: jdbc:hive2://localhost.> desc table_blocked;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+
1 row selected (0.171 seconds)
0: jdbc:hive2://localhost.> create materialized view table_blocked_mv as select * from table_blocked;
No rows affected (18.055 seconds)
0: jdbc:hive2://localhost.> desc table_blocked_mv;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+
1 row selected (0.316 seconds)
0: jdbc:hive2://localhost.> drop table table_blocked;
No rows affected (10.803 seconds)
0: jdbc:hive2://localhost.> desc table_blocked_mv;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+
1 row selected (0.222 seconds)
0: jdbc:hive2://localhost.> desc table_blocked;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table table_blocked. null (state=42000,code=40000)
0: jdbc:hive2://localhost.> select * from table_blocked_mv;
Error: Error while compiling statement: FAILED: SemanticException Table ptest2_db_dev.table_blocked not found when trying to obtain it to check masking/filtering policies (state=42000,code=40000)

 
 Drop table used by a materialized view",yes
13155194,"Allows external clients to consume output from LLAP daemons in Arrow stream format. 
 Arrow format for LlapOutputFormatService (umbrella)",13155202,"Support pushing arrow batches through org.apache.arrow.vector.ipc.ArrowOutputStream in LllapOutputFormatService. 
 Support ArrowOutputStream in LlapOutputFormatService",yes
13173387,"This is an effort of enriching UT for placement constraints, by adding following E2E tests

Inter-app constraints with AND, OR composite types
Placement constraints with multiple allocation tags
Inter-app constraints with all forms of namespaces

 
 Add more E2E tests for placement constraints",13143805,"AppPlacementAllocator currently only supports intra-app anti-affinity placement constraints, once YARN-8002 and YARN-8013 are resolved, it needs to support inter-app constraints too. Also, this may require some refactoring on the existing code logic. Use this JIRA to track. 
 Support all types of placement constraint support for Capacity Scheduler",yes
13294545,"The Operation class declares its logger this way:
Operation.java

public abstract class Operation {
  public static final Logger LOG = LoggerFactory.getLogger(Operation.class.getName());
  ...
}


Notice that this is an abstract class, but the Logger is tied to the Operation.class.getName().  This means that logging cannot be controlled for each subclass of Operation independently since they all use the same static Logger instance.
Make the LOG a protected instance variable that inherits the name of the child class. 
 Improve Logger for Operation Child Classes",13137288,"Deprecated HostAndPort#getHostText method was deleted in Guava 22.0and newHostAndPort#getHost method is not available before Guava 20.0.
This patch implements getHost(HostAndPort) method that extracts host from HostAndPort#toStringvalue.
This is a little hacky,that's whyI'm not sure if it worth to merge thispatch, but it could be niceif Hadoop will be Guava-neutral.
With this patch Hadoop can be built against latest Guava v24.0. 
 Make Hadoop compatible with Guava 22.0+",No
13259478,"As part of %secondary_group mapping, we ensure o/p of %secondary_group while processing the queue mapping is available using CSQueueManager. Similarly, we will need to same for%primary_group. 
 Validate %primary_group queue in CS queue manager",13201551,"See this in the outout and then the test hang

2018-11-29 20:47:50,061 WARN  [MockRSProcedureDispatcher-pool5-t10] assignment.AssignmentManager(894): The region server localhost,102,1 is already dead, skip reportRegionStateTransition call

 
 TestAssignmentManager is flakey",No
13292252,"Hello,
While using webhdfs, I encountered a strange bug where I just cannot rename a file if it has a space in the filename.
It seems strange to me, is there anything I am missing ?

Edit: After some debugging, it seems to be linked with the way spaces are encoded the webhdfs url: the JDK's URLEncoder uses '+' to encode spaces, whereas a CURL command where the filename is encoded with '%20' for spaces works just fine. 
 Cannot rename file with space in name",13227568,"The following commands with percent (%) no longer work starting with version 3.1:


$ hadoop/bin/hdfs dfs -touchz webhdfs://localhost/%
$ hadoop/bin/hdfs dfs -cat webhdfs://localhost/%
cat: URLDecoder: Incomplete trailing escape (%) pattern


Also, plus (+ ) characters get turned into spaces when doing DN operations:


$ hadoop/bin/hdfs dfs -touchz webhdfs://localhost/a+b
$ hadoop/bin/hdfs dfs -mkdir webhdfs://localhost/c+d
$ hadoop/bin/hdfs dfs -ls /
Found 4 items
-rw-r--r--   1 jing supergroup          0 2019-04-12 11:20 /a b
drwxr-xr-x   - jing supergroup          0 2019-04-12 11:21 /c+d


I can confirm that these commands work correctly on 2.9 and 3.0. Also, the usual hdfs:// client works as expected.
I suspect a relation with HDFS-13176 or HDFS-13582, but I'm not sure what the right fix is. Note that Hive uses % to escape special characters in partition values, so banning % might not be a good option. For example, Hive will create a paths like table_name/partition_key=%2F when partition_key='/'. 
 Percent (%) and plus (+) characters no longer work in WebHDFS",yes
13154181,"Hive is case insensitive wrt db/table names.  These gets normalized to lower case for SQL processing.
When HiveEndPoint is created it uses db.table strings as is, and they end up propagated this way to transaction metadata tables in the metastore via lock acquisition.  This makes them look like different tables in Cleaner and lock manager. 
 Streaming Ingest API doesn't normalize db.table names",13140575,"http://hbase.apache.org/book.html#tricks.pre-split


hbase>create 't1','f',SPLITS => ['10','20',30']


Missing a quote before the 30./ 
 Clean up inputs in JDBC PreparedStatement",No
13228962,"There doesn't seem to be a way to get the INodeId of a file using the Hadoop shell e.g. stat.
It can be obtained via a webhdfs LISTSTATUS request. 
 Provide a way to query INodeId (fileId) via Hadoop shell",13212415,"When debugging the FSImage corruption issue, I often need to know a file's or directory's inode id. At this moment, the only way to do that is to use OIV tool to dump the FSImage and look up the filename, which is very inefficient.
Here I propose adding option ""-i"" in FsShell that prints files' or directories' inode id.
Implementation
For hdfs:// (HDFS)
fileId exists in HdfsLocatedFileStatus, which is already returned to hdfs-client. We just need to print it in Ls#processPath().
For file:// (Local FS)
Linux
Use java.nio.
Windows
Windows has the concept of ""File ID"" which is similar to inode id. It is unique in NTFS and ReFS.
For other FS
The fileId entry will be ""0"" in FileStatus if it is not set. We could either ignore or throw an exception. 
 FsShell ls: Add option -i to print inode id",yes
13216222,"Create new synonym for the existing function

Mid for substr
postiion for locate 
 Create Synonym mid for  substr, position for  locate",13146946,"This git diff shows the conflicting results


diff --git a/ql/src/test/results/clientpositive/druid/druidmini_dynamic_partition.q.out b/ql/src/test/results/clientpositive/druid/druidmini_dynamic_partition.q.out
index 714778ebfc..cea9b7535c 100644
--- a/ql/src/test/results/clientpositive/druid/druidmini_dynamic_partition.q.out
+++ b/ql/src/test/results/clientpositive/druid/druidmini_dynamic_partition.q.out
@@ -243,7 +243,7 @@ POSTHOOK: query: SELECT  sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@druid_partitioned_table
 POSTHOOK: Output: hdfs://### HDFS PATH ###
-1408069801800  4139540644      10992545287     165393120
+1408069801800  3272553822      10992545287     -648527473
 PREHOOK: query: SELECT  sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM druid_partitioned_table_0
 PREHOOK: type: QUERY
 PREHOOK: Input: default@druid_partitioned_table_0
@@ -429,7 +429,7 @@ POSTHOOK: query: SELECT sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM d
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@druid_partitioned_table
 POSTHOOK: Output: hdfs://### HDFS PATH ###
-2857395071862  4139540644      -1661313883124  885815256
+2857395071862  3728054572      -1661313883124  71894663
 PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE druid_partitioned_table
   SELECT cast (`ctimestamp1` as timestamp with local time zone) as `__time`,
     cstring1,
@@ -566,7 +566,7 @@ POSTHOOK: query: SELECT sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM d
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@druid_partitioned_table
 POSTHOOK: Output: hdfs://### HDFS PATH ###
-1408069801800  7115092987      10992545287     1232243564
+1408069801800  4584782821      10992545287     -1808876374
 PREHOOK: query: SELECT  sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM druid_partitioned_table_0
 PREHOOK: type: QUERY
 PREHOOK: Input: default@druid_partitioned_table_0
@@ -659,7 +659,7 @@ POSTHOOK: query: SELECT sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM d
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@druid_partitioned_table
 POSTHOOK: Output: hdfs://### HDFS PATH ###
-1408069801800  7115092987      10992545287     1232243564
+1408069801800  4584782821      10992545287     -1808876374
 PREHOOK: query: EXPLAIN SELECT  sum(cint), max(cbigint),  sum(cbigint), max(cint)  FROM druid_max_size_partition
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT  sum(cint), max(cbigint),  sum(cbigint), max(cint)  FROM druid_max_size_partition
@@ -758,7 +758,7 @@ POSTHOOK: query: SELECT sum(cint), max(cbigint),  sum(cbigint), max(cint) FROM d
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@druid_partitioned_table
 POSTHOOK: Output: hdfs://### HDFS PATH ###
-1408069801800  7115092987      10992545287     1232243564
+1408069801800  4584782821      10992545287     -1808876374
 PREHOOK: query: DROP TABLE druid_partitioned_table_0
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@druid_partitioned_table_0


 
 Druid Storage Handler returns conflicting results for Qtest druidmini_dynamic_partition.q",No
13201019,"In https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node, there are two configuration tags in mapred-site.xml. 
 mapred-site.xml is misformatted in single node setup document",13256676,"官网说明错误


正确的应该是下面的
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/</value>
    </property>
</configuration>
 
 官网代码片段错误",yes
13156157,"Mxnet could be run on YARN. This jira will help to add examples, yarnfile-, docker files which are needed to run Mxnet on YARN.-

Please go toYARN-8135Submarine for deep learning framework support on YARN. 
 mxnet yarn spec file to add to native service examples",13156158,"Umbrella jira to track various deep learning frameworks which can run on yarn native services.

Please go toYARN-8135Submarine for deep learning framework support on YARN. 
 [Umbrella] YARN deep learning framework examples to run on native service",yes
13293403,"LLAP external client (via hive-warehouse-connector) somehow seems to be sending duplicate submissions for the same fragment/attempt. When the 2nd request is sent this results in the following error:

2020-03-17T06:49:11,239 WARN  [IPC Server handler 2 on 15001 ()] org.apache.hadoop.ipc.Server: IPC Server handler 2 on 15001, call Call#75 Retry#0 org.apache.hadoop.hive.llap.protocol.LlapProtocolBlockingPB.submitWork from 19.40.252.114:33906
java.lang.IllegalStateException: Only a single registration allowed per entity. Duplicate for TaskWrapper{task=attempt_1854104024183112753_6052_0_00_000128_1, inWaitQueue=true, inPreemptionQueue=false, registeredForNotifications=true, canFinish=true, canFinish(in queue)=true, isGuaranteed=false, firstAttemptStartTime=1584442003327, dagStartTime=1584442003327, withinDagPriority=0, vertexParallelism= 2132, selfAndUpstreamParallelism= 2132, selfAndUpstreamComplete= 0}
    at org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.registerForUpdates(QueryInfo.java:233) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.registerForFinishableStateUpdates(QueryInfo.java:205) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.registerForFinishableStateUpdates(QueryFragmentInfo.java:160) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeRegisterForFinishedStateNotifications(TaskExecutorService.java:1167) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.schedule(TaskExecutorService.java:564) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.schedule(TaskExecutorService.java:93) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.submitWork(ContainerRunnerImpl.java:292) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.submitWork(LlapDaemon.java:610) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.submitWork(LlapProtocolServerImpl.java:122) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(LlapDaemonProtocolProtos.java:22695) ~[hive-exec-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.32-1]
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) ~[hadoop-common-3.1.1.3.1.4.26-3.jar:?]
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) ~[hadoop-common-3.1.1.3.1.4.26-3.jar:?]
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) ~[hadoop-common-3.1.1.3.1.4.26-3.jar:?]
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) ~[hadoop-common-3.1.1.3.1.4.26-3.jar:?]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_191]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_191]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.1.4.26-3.jar:?]
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) ~[hadoop-common-3.1.1.3.1.4.26-3.jar:?]


I think the issue here is that this error occurred too late - based on the stack trace, LLAP has already accepted/registered the fragment. The subsequent cleanup of this fragment/attempt also affects the first request. Which results in the LLAP crash described in HIVE-23061:

2020-03-17T06:49:11,304 ERROR [ExecutionCompletionThread #0 ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Thread Thread[ExecutionCompletionThread #0,5,main] threw an Exception. Shutting down now...
java.lang.IllegalStateException: Cannot invoke unregister on an entity which has not been registered
    at com.google.common.base.Preconditions.checkState(Preconditions.java:508) ~[hive-exec-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.32-1]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.unregisterForUpdates(QueryInfo.java:256) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.unregisterFinishableStateUpdate(QueryInfo.java:209) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.unregisterForFinishableStateUpdates(QueryFragmentInfo.java:166) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeUnregisterForFinishedStateNotifications(TaskExecutorService.java:1177) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(TaskExecutorService.java:980) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(TaskExecutorService.java:944) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1021) ~[hive-exec-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.32-1]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_191]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_191]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]

 
 Error when submitting fragment to LLAP via external client: IllegalStateException: Only a single registration allowed per entity",13292992,"The following exception goes uncaught and causes the entire LLAP daemon to shut down:

2020-03-17T06:49:11,304 ERROR [ExecutionCompletionThread #0 ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Thread Thread[ExecutionCompletionThread #0,5,main] threw an Exception. Shutting down now...
java.lang.IllegalStateException: Cannot invoke unregister on an entity which has not been registered
    at com.google.common.base.Preconditions.checkState(Preconditions.java:508) ~[hive-exec-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.32-1]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.unregisterForUpdates(QueryInfo.java:256) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.unregisterFinishableStateUpdate(QueryInfo.java:209) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.unregisterForFinishableStateUpdates(QueryFragmentInfo.java:166) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeUnregisterForFinishedStateNotifications(TaskExecutorService.java:1177) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(TaskExecutorService.java:980) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(TaskExecutorService.java:944) ~[hive-llap-server-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.26-3]
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1021) ~[hive-exec-3.1.0.3.1.4.26-3.jar:3.1.0.3.1.4.32-1]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_191]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_191]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]

 
 LLAP crash due to unhandled exception: Cannot invoke unregister on an entity which has not been registered",yes
13152903,"DefaultSpeculator speculates a task one time. By default, the number of speculators is max(max(10, 0.01 * tasks.size), 0.1 * running tasks).
I set mapreduce.job.reduce.slowstart.completedmaps = 1 to start reduce after all the map tasks are finished. The cluster has 1000 vcores, and the Job has 5000 reduce jobs. At first, 1000 reduces tasks can run simultaneously, number of speculators can speculator at most is 0.1 * 1000 = 100 tasks. Reduce tasks with less data can over shortly, and speculator will speculator a task per second by default. The task be speculated execution may be because the more data to be processed. It will speculator 100 tasks within 100 seconds. When 4900 reduces is over, If a reduce is executed with a lot of data be processed and is put on a slow machine. The speculate opportunity is running out, it will not be speculated. It can increase the execution time of job significantly.
In short, it may waste the speculate opportunity at first only because the execution time of reduce with less data to be processed as average time. At end of job, there is no speculate opportunity available, especially last several running tasks, judged the number of the running tasks .
In my opinion, the number of running tasks should not determine the number of speculate opportunity .The number of tasks be speculated can be judged by square of finished task percent. Take an example, if ninety percent of the task is finished, only 0.9*0.9 = 0.81 speculate opportunity can be used. It will leave enough opportunity for latter tasks. 
 Default speculator won't speculate the last several submitted reduced task if the total task num is large",13152886,"DefaultSpeculator speculates a task one time.
By default, the number of speculators is max(max(10, 0.01 * tasks.size), 0.1 * running tasks)
I set mapreduce.job.reduce.slowstart.completedmaps = 1 to start reduce after all the map tasks are finished.
The cluster has 1000 vcores, and the Job has 5000 reduce jobs.
At first, 1000 reduces tasks can run simultaneously, number of speculators can speculator at most is 0.1 * 1000 = 100 tasks. Reduce tasks with less data can over shortly, and speculator will speculator a task per second by default. The task be speculated execution may be because the more data to be processed. It will speculator 100 tasks within 100 seconds.
When 4900 reduces is over, If a reduce is executed with a lot of data be processed and is put on a slow machine. The speculate opportunity is running out, it will not be speculated. It can increase the execution time of job significantly.
In short, it may waste the speculate opportunity at first only because the execution time of reduce with less data to be processed as average time. At end of job, there is no speculate opportunity available, especially last several running tasks, judged the number of the running tasks .

In my opinion, the number of tasks be speculated can be judged by square of finished task percent. Take an example, if ninety percent of the task is finished, only 0.9*0.9 = 0.81 speculate opportunity can be used. It will leave enough opportunity for latter tasks.
 
 Default speculator won't sepculate the last several submitted reduced task if the total task num is large",yes
13155200,"Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches. 
 Arrow batch serializer",13155204,"This is a sub-class of LlapBaseRecordReader that wraps the socket inputStream and produces Arrow batches for an external client. 
 Provide an Arrow stream reader for external LLAP clients ",yes
13181876,"I've created a script that will put continuous put one record (size 2.5KB) and flush immediately – in middle am doing compaction at regular intervals. Rate of flushes are around 20flushes/sec. After some time, my RS aborted and never came up back
 with the following error


2018-08-29 11:34:34,183 DEBUG [flush-table-TestTable_client_1258196-thread-1] regionserver.RSRpcServices: Closing region operation on TestTable_client_1,32816,1535513244999.762a3e633b03e5f847f357aca28768d0.
2018-08-29 11:34:34,183 INFO  [RpcServer.FifoWFPBQ.default.handler=49,queue=4,port=16040] regionserver.RSRpcServices: flush table task succeed 1, failed 10.
2018-08-29 11:34:34,280 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=230, memsize=4.2 K, hasBloomFilter=false, into tmp file hdfs://hacluster/hbase/data/hbase/meta/1588230740/.tmp/1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,290 INFO  [MemStoreFlusher.0] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,291 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://hacluster/hbase/data/hbase/meta/1588230740/.tmp/1cf1deee293848b0bea08940696dbd2a as hdfs://hacluster/hbase/data/hbase/meta/1588230740/info/1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,304 INFO  [MemStoreFlusher.0] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,304 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://hacluster/hbase/data/hbase/meta/1588230740/info/1cf1deee293848b0bea08940696dbd2a, entries=13, sequenceid=230, filesize=6.6 K
2018-08-29 11:34:34,307 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: ABORTING region server host-xxxx,16040,1535454741321: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: hbase:meta,,1
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2578)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2255)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2217)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2108)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:2034)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:505)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:475)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:263)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:826)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:1117)
        at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1090)
        at org.apache.hadoop.hbase.regionserver.HStore.access$700(HStore.java:120)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2450)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2533)
        ... 9 more
2018-08-29 11:34:34,307 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.AccessController, org.apache.hadoop.hbase.

 
 NullPointerException in StoreScanner",13243811,"

2019-07-09 08:02:14,262 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server hostname,16020,1562233574704: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: namespace:table,963,1562296120996.b8e2f19748d374d192b93f106a0f73b3.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2646)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2322)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2284)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2170)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:2095)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:508)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:478)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:76)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:264)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:863)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:1172)
        at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1145)
        at org.apache.hadoop.hbase.regionserver.HStore.access$900(HStore.java:122)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2505)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2600)
        ... 9 more

 
 [Flush] NPE when region flushs",yes
13318439,"HIVE-21784 uses a new WriterOptions instead of the field in OrcRecordUpdater:
https://github.com/apache/hive/commit/f62379ba279f41b843fcd5f3d4a107b6fcd04dec#diff-bb969e858664d98848960a801fd58b5cR580-R583
so in this scenario, the overwrite creates an empty bucket file, which is fine as that was the intention of that patch, but it creates that with invalid schema:


CREATE TABLE test_table (
   cda_id             int,
   cda_run_id         varchar(255),
   cda_load_ts        timestamp,
   global_party_id    string)
PARTITIONED BY (
   cda_date           int,
   cda_job_name       varchar(12))
CLUSTERED BY (cda_id) 
INTO 2 BUCKETS
STORED AS ORC;


INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id
UNION ALL
SELECT 2 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id;

ALTER TABLE test_table ADD COLUMNS (group_id string) CASCADE ;

INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id, 'group_id' as group_id;


because of HIVE-21784, the new empty bucket_00000 shows this schema in orc dump:


Type: struct<_col0:int,_col1:varchar(255),_col2:timestamp,_col3:string,_col4:string>


instead of:


Type: struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<cda_id:int,cda_run_id:varchar(255),cda_load_ts:timestamp,global_party_id:string,group_id:string>>


and this could lead to problems later, when hive tries to look into the file during split generation 
 Empty bucket files are inserted with invalid schema after HIVE-21784",13130540,"
$ bin/stop-hbase.sh
stopping hbasecat: /tmp/hbase-mdrob-master.pid: No such file or directory

 
 stop-hbase gives unfriendly message when local hbase isn't running",No
13317204,"When Node Recovery is Enabled, Stopping a NM won't unregister to RM. So RM Active Nodes will be still having those stopped nodes until NM Liveliness Monitor Expires after configured timeout (yarn.nm.liveness-monitor.expiry-interval-ms = 10 mins). During this 10mins, Multi Node Placement assigns the containers on those nodes. They need to exclude the nodes which has not heartbeated for configured heartbeat interval (yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000ms) similar to Asynchronous Capacity Scheduler Threads. (CapacityScheduler#shouldSkipNodeSchedule)
Repro:
1. Enable Multi Node Placement (yarn.scheduler.capacity.multi-node-placement-enabled) + Node Recovery Enabled  (yarn.node.recovery.enabled)
2. Have only one NM running say worker0
3. Stop worker0 and start any other NM say worker1
4. Submit a sleep job. The containers will timeout as assigned to stopped NM worker0.
 
 Skip schedule on not heartbeated nodes in Multi Node Placement",13173400,"Currently only HB-lagged is handled, with hard-coded 2 times of HB lag which we should make it configurable. And more over, we need to exclude unhealthy and decommissioned nodes too. 
 Exclude lagged/unhealthy/decommissioned nodes in async allocating thread",yes
13320121,"We are using placement constaints anti-affinity in an application along with node label. The application requests two containers with anti affinity on the node label containing only two nodes.
So two containers will be allocated in the two nodes, one on each node satisfying anti-affinity.
When one nodemanager goes down for some time, the node is marked as lost by RM and then it will kill all containers in that node.
The AM will now have one pending container request, since the previous container got killed.
When the Nodemanager becomes up after some time, the pending container is not getting allocated in that node again and the application has to wait forever for that container.
If the ResourceManager is restarted, this issue disappears and the container gets allocated on the NodeManager which came back up recently.
This seems to be an issue with the allocation tags not removed.
The allocation tag is added for the container container_e68_1595886973474_0005_01_000003 .


2020-07-28 17:02:04,091 DEBUG constraint.AllocationTagsManager (AllocationTagsManager.java:addContainer(355)) - Added container=container_e68_1595886973474_0005_01_000003 with tags=[hbase]\


However, the allocation tag is not removed when the container container_e68_1595886973474_0005_01_000003 is released. There is no equivalent DEBUG message seen for removing tags. This means that the tags are not getting removed. If the tag is not removed, then scheduler will not allocate in the same node due to anti-affinity resulting in the issue observed.


2020-07-28 17:19:34,353 DEBUG scheduler.AbstractYarnScheduler (AbstractYarnScheduler.java:updateCompletedContainers(1038)) - Container FINISHED: container_e68_1595886973474_0005_01_000003
2020-07-28 17:19:34,353 INFO  scheduler.AbstractYarnScheduler (AbstractYarnScheduler.java:completedContainer(669)) - Container container_e68_1595886973474_0005_01_000003 completed with event FINISHED, but corresponding RMContainer doesn't exist.


This seems to be due to changes done in YARN-8511 . Change here was made to remove the tags only after NM confirms container is released. However, in our scenario this is not happening. So the tag will never get removed until RM restart.
Reverting YARN-8511 fixes this particular issue and tags are getting removed. But this is not a valid solution since the problem thatYARN-8511solves is also valid. We need to find a solution which does not break YARN-8511 and also fixes this issue. 
 When NM goes down and comes back up, PC allocation tags are not removed for completed containers",13274718,"When a node is decommissioned, allocation tags that are attached to the node are not removed.
I could see that allocation tags are revived when recommissioning the node.
RM removes allocation tags only if NM confirms the container releases by YARN-8511. but, decommissioned NM does not connect to RM anymore.
Once a node is decommissioned, allocation tags that attached to the node should be removed immediately. 
 Allocation tags are not removed when node decommission",yes
13248267,"The command hadoop fs -test support seven options : -d / -e / -f / -s / -w / -r / -z.
However when see the usage of this command, only see five options. w and r are missing.
 
 Wrong usage hint for hadoop fs command test",13141728,"Entering the search term in anyof the three configuration tables (core, yarn, mapred) will result in filtering across all tables. Pagination also behaves the same way as clicking on one table changes the page in all the tables. This results in weird behavior since each table can have different number of pages present. 
 Yarn ui2 configuration tables have the same search and pagination instances",No
13240995,"Whilecolocating the recovered edits directory with hbase.wal.dir, BASE_NAMESPACE_DIR got missed. This results in recovered edits being put in a separate directory rather than the default region directory even if the hbase.wal.dir is not overridden. Eg. if data is stored in /hbase/data/namespace/table1, recovered edits are put in /hbase/namespace/table1. This also messes up the regular cleaner chores which never operate on this new directory and these directories will never be deleted, even for split parents or dropped tables. We should change the default back to have the base namespace directory in path. 
 Recovered WAL directories not getting cleaned up",13241533,"If you call s3guard init s3a://name/ then the custom bucket options of fs.s3a.bucket.name are not picked up, instead the global value is used.
Fix: take the name of the bucket and use that to eval properties and patch the config used for the init command. 
 Port HBASE-22617 (Recovered WAL directories not getting cleaned up) to branch-1",yes
13244776,"As discussed in the following thread, we can deprecate / remove OfflineMetaRepair.
https://lists.apache.org/thread.html/f122efdc79be541d678e22cf8cf573352ab468159596d820c38bf84b@%3Cdev.hbase.apache.org%3E
Maybe we can deprecate in 2.x and remove in 3.0. 
 Deprecate / Remove OfflineMetaRepair in hbase-2+",13263492,"This is a really simple UI change proposal:
 The icon of ""Decommissioned & dead"" datanode could be improved. It can be changed from   to   so that,

icon ""   "" can be used for all status starts with ""decommission"" on dfshealth.html,
icon ""   "" can be differentiated with icon ""   "" onfederationhealth.html




DataNode Infomation Legend(now)
 dfshealth.html#tab-datanode



DataNode Infomation Legend(proposed)
 dfshealth.html#tab-datanode



NameService Legend
 federationhealth.htm#tab-namenode




 
 Change the ICON of ""Decommissioned & dead"" datanode on ""dfshealth.html""",No
13338470,"When the cluster performs a failover from NN1 to NN2, NN2 is asking all the other NNs to cede active state and transit to StandBy including the Observer NameNodes.
Seems we should block Observer from becoming StandBy and participating in Failover. Of course, since we can transit StandBy NameNode to Observer, we can separately support promote Observer NameNode to StandBy NameNode. 
 Prevent Observer NameNode from becoming StandBy NameNode",13266704,"HDFS-14130 made ZKFC aware of the Observer Namenode and hence allows ZKFC running along with the observer NOde.
The Observer namenode isn't suppose to be part of ZKFC election process.
But if the  Namenode was part of election, before turning into Observer by transitionToObserver Command. The ZKFC still sends instruction to the Namenode as a result of previous participation and sometimes tend to change the state of Observer to Standby.
This is also the reason for  failure in TestDFSZKFailoverController.
TestDFSZKFailoverController has been consistently failing with a time out waiting in testManualFailoverWithDFSHAAdmin(). In particular waitForHAState(1, HAServiceState.OBSERVER);. 
 [SBN read] Prevent ZKFC changing Observer Namenode state",yes
13134637,"Some class has missed the ASF header 
 Some class has missed the ASF header",13176837,"Reproducer


> set hive.optimize.remove.sq_count_check=true;
> create table tempty(i int);
> create table t(c0 int);
> explain select * from t where c0 > (select count(*) from tempty group by 1);

 
 Scalar subquery throws error when hive.optimize.remove.sq_count_check is on",No
13146665,"ALTER TABLE ADD CONSTRAINT should be able to add CHECK constraint (table level) 
 ALTER TABLE ADD CONSTRAINT support for CHECK constraint",13342382,"Apache Spark 3.0 has been relesead in June 2020.
This addresses the changes needed to run the connector with Spark 3.0 and also to be able to compile the connector using Spark 3.0 as a dependency. 
 Hbase connector with apache spark",No
13214438,"Consecutive calls to StringBuffer/StringBuilder .append should be chained, reusing the target object. This can improve the performance by producing a smaller bytecode, reducing overhead and improving inlining. 
 Consecutive StringBuilder append should be reused",13189791,"Currently the Acid compactor is implemented as generated MR job (CompactorMR.java).
It could also be expressed as a Hive query that reads from a given partition and writes data back to the same partition.  This will merge the deltas and 'apply' the delete events.  The simplest would be to just use Insert Overwrite but that will change all ROW__IDs which we don't want.
Need to implement this in a way that preserves ROW__IDs and creates a new base_x directory to handle Major compaction.
Minor compaction will be investigated separately.
 
 Query based compactor for full CRUD Acid tables",No
13261798,"Capacity Scheduler does not support two percentage values for leaf queue capacity and maximum-capacity settings. So, you can't do something like this:
yarn.scheduler.capacity.root.users.john.leaf-queue-template.capacity=memory-mb=50.0%, vcores=50.0%
Only a single percentage value is accepted.
This makes it nearly impossible to properly convert a similar setting from Fair Scheduler, where such a configuration is valid and accepted (<maxChildResources>).
Note: using absolute resources (yarn.scheduler.capacity.root.users.john.leaf-queue-template.capacity=memory-mb=16384, vcores=8) is addressed in YARN-10154.
 
 Capacity scheduler: enhance leaf-queue-template capacity / maximum-capacity setting",13261783,"Capacity Scheduler does not support two percentage values for capacity and maximum-capacity settings. So, you can't do something like this:
yarn.scheduler.capacity.root.users.john.maximum-capacity=memory-mb=50.0%, vcores=50.0%
It's possible to use absolute resources, but not two separate percentages (which expresses capacity as a percentage of the overall cluster resource). Such a configuration is accepted in Fair Scheduler. 
 Capacity scheduler: enhance capacity / maximum-capacity setting",yes
13214022,"
hbase(main):001:0> move ""hbase:meta,,1.1588230740"", ""server1""

ERROR: Unknown region hbase:meta,,1.1588230740!

For usage try 'help ""move""'

Took 1.1780 seconds
hbase(main):003:0> move ""1588230740"", ""server1""
Took 84.3050 seconds

 
 shell commands don't recognize meta region's full name",13156715,"As mentioned in this comment, the MRAppMaster fails for docker containers if there is no additional user lookup strategy (e.g. bind-mounting /var/run/nscd or /etc/passwd). We need a better solution so that users can still run even if they are not known inside of the container by name 
 MRAppMaster fails when using UID:GID pair within docker container",No
13272180,"There is a scenario when different SplitGenerator instances try to cover the delta-only buckets (having no base file) more than once, so there could be multiple OrcSplit instances generated for the same delta file, causing more tasks to read the same delta file more than once, causing duplicate records in a simple select star query.
File structure for a 256 bucket table


drwxrwxrwx   - hive hadoop          0 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/base_0000013
-rw-r--r--   3 hive hadoop        353 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/base_0000013/bucket_00012
-rw-r--r--   3 hive hadoop       1642 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/base_0000013/bucket_00140
drwxrwxrwx   - hive hadoop          0 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/delta_0000014_0000014_0000
-rwxrwxrwx   3 hive hadoop        348 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/delta_0000014_0000014_0000/bucket_00012
-rwxrwxrwx   3 hive hadoop       1635 2019-11-29 15:55 /apps/hive/warehouse/naresh.db/test1/delta_0000014_0000014_0000/bucket_00140
drwxrwxrwx   - hive hadoop          0 2019-11-29 16:04 /apps/hive/warehouse/naresh.db/test1/delta_0000015_0000015_0000
-rwxrwxrwx   3 hive hadoop        348 2019-11-29 16:04 /apps/hive/warehouse/naresh.db/test1/delta_0000015_0000015_0000/bucket_00012
-rwxrwxrwx   3 hive hadoop       1808 2019-11-29 16:04 /apps/hive/warehouse/naresh.db/test1/delta_0000015_0000015_0000/bucket_00140
drwxrwxrwx   - hive hadoop          0 2019-11-29 16:06 /apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000
-rwxrwxrwx   3 hive hadoop        348 2019-11-29 16:06 /apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00043
-rwxrwxrwx   3 hive hadoop       1633 2019-11-29 16:06 /apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00171


in this case, when bucket_00171 file has a record, and there is no base file for that, a select  with ETL split strategy can generate 2 splits for the same delta bucket...
the scenario of the issue:
1. ETLSplitStrategy contains a covered[] array which is shared between the SplitInfo instances to be created
2. a SplitInfo instance is created for every base file (2 in this case)
3. for every SplitInfo, a SplitGenerator is created, and in the constructor, parent's getSplit is called, which tries to take care of the deltas
I'm not sure at the moment what's the intention of this, but this way, duplicated delta split can be generated, which can cause duplicated read later (note that both tasks read the same delta file: bucket_00171)


2019-12-01T16:24:53,669  INFO [TezTR-127843_16_30_0_171_0 (1575040127843_0016_30_00_000171_0)] orc.ReaderImpl: Reading ORC rows from hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00171 with {include: [true, true, true, true, true, true, true, true, true, true, true, true], offset: 0, length: 9223372036854775807, schema: struct<idp_warehouse_id:bigint,idp_audit_id:bigint,batch_id:decimal(9,0),source_system_cd:varchar(500),insert_time:timestamp,process_status_cd:varchar(20),business_date:date,last_update_time:timestamp,report_date:date,etl_run_time:timestamp,etl_run_nbr:bigint>}
2019-12-01T16:24:53,672  INFO [TezTR-127843_16_30_0_171_0 (1575040127843_0016_30_00_000171_0)] lib.MRReaderMapred: Processing split: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat:OrcSplit [hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1, start=171, length=0, isOriginal=false, fileLength=9223372036854775807, hasFooter=false, hasBase=false, deltas=[{ minTxnId: 14 maxTxnId: 14 stmtIds: [0] }, { minTxnId: 15 maxTxnId: 15 stmtIds: [0] }, { minTxnId: 16 maxTxnId: 16 stmtIds: [0] }]]
2019-12-01T16:24:55,807  INFO [TezTR-127843_16_30_0_425_0 (1575040127843_0016_30_00_000425_0)] orc.ReaderImpl: Reading ORC rows from hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1/delta_0000016_0000016_0000/bucket_00171 with {include: [true, true, true, true, true, true, true, true, true, true, true, true], offset: 0, length: 9223372036854775807, schema: struct<idp_warehouse_id:bigint,idp_audit_id:bigint,batch_id:decimal(9,0),source_system_cd:varchar(500),insert_time:timestamp,process_status_cd:varchar(20),business_date:date,last_update_time:timestamp,report_date:date,etl_run_time:timestamp,etl_run_nbr:bigint>}
2019-12-01T16:24:55,813  INFO [TezTR-127843_16_30_0_425_0 (1575040127843_0016_30_00_000425_0)] lib.MRReaderMapred: Processing split: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat:OrcSplit [hdfs://c3351-node2.squadron.support.hortonworks.com:8020/apps/hive/warehouse/naresh.db/test1, start=171, length=0, isOriginal=false, fileLength=9223372036854775807, hasFooter=false, hasBase=false, deltas=[{ minTxnId: 14 maxTxnId: 14 stmtIds: [0] }, { minTxnId: 15 maxTxnId: 15 stmtIds: [0] }, { minTxnId: 16 maxTxnId: 16 stmtIds: [0] }]]


seems like this issue doesn't affect AcidV2, as getSplits() returns an empty collection or throws an exception in case of unexpected deltas (which was the case here, where deltas was not unexpected):
https://github.com/apache/hive/blob/8ee3497f87f81fa84ee1023e891dc54087c2cd5e/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java#L1178-L1197 
 ACID v1: covered delta-only splits (without base) should be marked as covered (branch-2)",13140575,"http://hbase.apache.org/book.html#tricks.pre-split


hbase>create 't1','f',SPLITS => ['10','20',30']


Missing a quote before the 30./ 
 Clean up inputs in JDBC PreparedStatement",No
13231874,"We refactored access checking so other components can reuse permissions checks formerly encapsulated by the AccessController coprocessor. The new API is AccessChecker, committed as far back as branch-1.4. This should be backported to branch-1.3 as well so any potential user of AccessChecker can address changes and fixes for HBase versions 1.3 and up.  
 Backport AccessChecker refactor to branch-1.3 ",13138410,"(col1) IN (col2) can be transformed to (col1) = (col2), to avoid the hash-set implementation. 
 Optimize: Transform IN clauses to = when there's only one element",No
13165018,"In RM-HA env, kill ZK leader and then perform RM failover.
Sometimes, active RM gets NPE and fail to come up successfully



2018-06-08 10:31:03,007 INFO client.ZooKeeperSaslClient (ZooKeeperSaslClient.java:run(289)) - Client will use GSSAPI as SASL mechanism.

2018-06-08 10:31:03,008 INFO zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server xxx/xxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'

2018-06-08 10:31:03,009 WARN zookeeper.ClientCnxn (ClientCnxn.java:run(1146)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect

java.net.ConnectException: Connection refused

at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)

at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)

at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)

at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)

2018-06-08 10:31:03,344 INFO service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService failed in state INITED

java.lang.NullPointerException

at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)

at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)

at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)

at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)

at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)

at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)

at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)

at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)

at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)

at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)

at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)

at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)

2018-06-08 10:31:03,345 INFO ha.ActiveStandbyElector (ActiveStandbyElector.java:quitElection(409)) - Yielding from election
 
 ActiveStandbyElectorBasedElectorService is failing with NPE",13315903,"LLAP IO supports caching but currently this is only done via LlapRecordReader / using splits, aka good old mapreduce way.
At certain times it would worth to leverage the caching of files on certain paths, that are not necessarily associated with a record reader directly. An example of this could be the caching of ACID delete delta files, as they are currently being read without caching.
With this patch we'd extend the LLAP API and offer another entry point for retrieving metadata of ORC files. 
 LLAP - add API to look up ORC metadata for certain Path",No
13297804,"Why we writing '/tmp' dir?


Error Message

org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist

Stacktrace

org.apache.hadoop.yarn.exceptions.YarnRuntimeException: org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist
	at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)
Caused by: org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist
	at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)
Caused by: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist
	at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)


 
 Remove snakeyaml lib from Hive distribution",13128460,"Within a database, to filter for tables with the string 'abc' in its name, I can use something like:


hive> use my_database;
hive> show tables '*abc*';


It would be great if I can do something similar to search within the list of columns in a table.
I have a table with around 3200 columns. Searching for the column of interest is an onerous task after doing a describe on it. 
 Make it easier to search for column name in a table",No
13324869,"As of now the default for github is set only to worklog, To enable link and label, We need to add this. 
 Add .asf.yaml to allow github and jira integration",13251158,"Reproducer


set hive.query.results.cache.enabled=false;
set hive.optimize.ppd.storage=true;
set hive.optimize.index.filter=true;

set hive.tez.bucket.pruning=true; 


CREATE TABLE `test_table`(                 
   `col_1` int,                                     
   `col_2` string,                                  
   `col_3` string)                                  
 CLUSTERED BY (                                     
   col_1)                                           
 INTO 4 BUCKETS;                                     

insert into test_table values(1, 'one', 'ONE'), (2, 'two', 'TWO'), (3,'three','THREE'),(4,'four','FOUR');

select * from test_table;

explain select col_1, col_2, col_3 from test_table where col_1 <> 2 order by col_2;
select col_1, col_2, col_3 from test_table where col_1 <> 2 order by col_2;



Above sql query produce zero rows. 
 Turning on hive.tez.bucket.pruning produce wrong results",No
13255898,"https://orc.apache.org/docs/releases.html
 
 Update ORC to version 1.6",13255405,"DataNode use threadGroup.activeCount() as the number of DataXceiver, it's not accurate since threadGroup includes not only DataXceiver thread and DataXceiverServer thread, PacketResponder thread and BlockRecoveryWorker thread is also in the same threadGroup.
In the worst case, the reported DataXceiver count maybe double of actual count(e.g. all DataXceiver process write block operation, they create same number of PacketResponder thread at the same time). 
 The calculation of DataXceiver count is not accurate",No
13205263,"Backport the change from HBASE-19994 to branch-1 but make the new behavior configurable. Still changes the interfaces. Could be ok for a minor release (1.5.0) 
 Port HBASE-19994 (Create a new class for RPC throttling exception, make it retryable) to branch-1",13155273,"Port HBASE-19994 (Create a new class for RPC throttling exception, make it retryable). Need to preserve the current behavior where the client gets a non-retryable ThrottlingException and only optionally throw back the retryable RpcThrottlingException if explicitly allowed by configuration. 
 Port HBASE-19994 (Create a new class for RPC throttling exception, make it retryable) to branch-1",yes
13247345,"Yarn Daemon Logs displays the URL instead of log name.
 
 [UI2] Yarn Daemon Logs displays the URL instead of log name",13208073,"hdfs ec -verifyClusterSetup command verifies if there are enough data nodes and racks for the enabled erasure coding policies
I think it would be beneficial if it could accept an erasure coding policy as a parameter optionally. For example the following commandwould run the verify for only the RS-6-3-1024k policy.


hdfs ec -verifyClusterSetup -policy RS-6-3-1024k

 
 Port YARN-7033 NM recovery of assigned resources to branch-3.0/branch-2",No
13324237,"When the current SqlOperator is SqlCastFunction, FunctionRegistry.getFunctionInfo would return null, 
but when hive.allow.udf.load.on.demand is enabled, HiveServer2 will refer to metastore for the function definition, an exception stack trace can be seen here in HiveServer2 log:
INFO exec.FunctionRegistry: Unable to look up default.cast in metastore
org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Function @hive#default.cast does not exist)
 at org.apache.hadoop.hive.ql.metadata.Hive.getFunction(Hive.java:5495) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Registry.getFunctionInfoFromMetastoreNoLock(Registry.java:788) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Registry.getQualifiedFunctionInfo(Registry.java:657) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Registry.getFunctionInfo(Registry.java:351) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:597) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.getHiveUDF(SqlFunctionConverter.java:158) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:112) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:68) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:68) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:68) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.calcite.rex.RexCall.accept(RexCall.java:191) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate.visitCall(PartitionPrune.java:134) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]

So it's may be better to handle explicit cast before geting the FunctionInfo from Registry. Even if there is no cast in the query, the method handleExplicitCast returns null quickly when op.kind is not aSqlKind.CAST.
 
 SqlFunctionConverter#getHiveUDF handles cast before geting FunctionInfo",13297804,"Why we writing '/tmp' dir?


Error Message

org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist

Stacktrace

org.apache.hadoop.yarn.exceptions.YarnRuntimeException: org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist
	at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)
Caused by: org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist
	at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)
Caused by: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist
	at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)


 
 Remove snakeyaml lib from Hive distribution",No
13161649,"Not able to do insert into table belonging to a non default namespace in HDFS federated cluster.
Steps to reproduce :
1) Create a HDFS federated cluster with 2 namespaces
2) Create an external table belonging to non-default namespace.


CREATE EXTERNAL TABLE test_ext_tbl2 (id int, name string, dept string) PARTITIONED BY (year int) location 'hdfs://ns2/tmp/test_ext_tbl2'


3) Try to insert a row into the newly created table.


INSERT INTO test_ext_tbl2 PARTITION (year=2016) VALUES (8,'Henry','CSE');


The query is hung and after some time its failing with below error :


ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1527031638037_0017_1_00, diagnostics=[Task failed, taskId=task_1527031638037_0017_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1527031638037_0017_1_00_000000_0:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable (null)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable (null)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:101)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:76)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:419)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267)
	... 16 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing writable (null)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:563)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:92)
	... 19 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: DestHost:destPort ctr-e138-1518143905142-326063-01-000006.hwx.site:8020 , LocalHost:localPort ctr-e138-1518143905142-326063-01-000006.hwx.site/172.27.67.65:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:708)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:863)
	at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:985)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:931)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:918)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:985)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:931)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:918)
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.forwardUDTFOutput(UDTFOperator.java:133)
	at org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.collect(UDTFCollector.java:45)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.forward(GenericUDTF.java:110)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFInline.process(GenericUDTFInline.java:64)
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.process(UDTFOperator.java:116)
	at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:985)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:931)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:918)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:985)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:931)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:125)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:550)
	... 20 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: DestHost:destPort ctr-e138-1518143905142-326063-01-000006.hwx.site:8020 , LocalHost:localPort ctr-e138-1518143905142-326063-01-000006.hwx.site/172.27.67.65:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FileSinkOperator.java:766)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:697)
	... 42 more
Caused by: java.io.IOException: DestHost:destPort ctr-e138-1518143905142-326063-01-000006.hwx.site:8020 , LocalHost:localPort ctr-e138-1518143905142-326063-01-000006.hwx.site/172.27.67.65:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1503)
	at org.apache.hadoop.ipc.Client.call(Client.java:1445)
	at org.apache.hadoop.ipc.Client.call(Client.java:1355)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy22.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1577)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1574)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1589)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734)
	at org.apache.hadoop.fs.FileSystem.deleteOnExit(FileSystem.java:1677)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketForFileIdx(FileSinkOperator.java:729)
	... 43 more
Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:756)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:719)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:812)
	at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1560)
	at org.apache.hadoop.ipc.Client.call(Client.java:1391)
	... 66 more
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
	at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
	at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:410)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:799)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:795)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:795)
	... 69 more

 
 multi-insert query with multiple GBY, and distinct in only some branches can produce incorrect results",13159711,"Repro steps:


drop database if exists ax1 cascade;
create database ax1;
use ax1;

CREATE TABLE 
	tmp1 ( 
		v1 string , v2 string , v3 string ) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t' 
LINES TERMINATED BY '\n' 
;

INSERT INTO tmp1
VALUES 
('a', 'b', 'c1') 
, ('a', 'b', 'c2') 
, ('d', 'e', 'f') 
, ('g', 'h', 'i') 
;

CREATE TABLE 
tmp_grouped_by_one_col  ( v1 string , cnt__v2 int , cnt__v3 int ) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t' 
LINES TERMINATED BY '\n' 
;

CREATE TABLE 
tmp_grouped_by_two_col ( v1 string , v2 string , cnt__v3 int ) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t' 
LINES TERMINATED BY '\n' 
;

CREATE TABLE 
tmp_grouped_by_all_col ( v1 string , v2 string , v3 string ) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t' 
LINES TERMINATED BY '\n' 
;

FROM tmp1
INSERT INTO tmp_grouped_by_one_col 
SELECT v1, count(distinct v2), count(distinct v3) 
GROUP BY v1
INSERT INTO tmp_grouped_by_all_col 
SELECT v1, v2, v3
GROUP BY v1, v2, v3
;

select 'tmp_grouped_by_one_col',count(*) from tmp_grouped_by_one_col
union all
select 'tmp_grouped_by_two_col',count(*) from tmp_grouped_by_two_col
union all
select 'tmp_grouped_by_all_col',count(*) from tmp_grouped_by_all_col;

select * from tmp_grouped_by_all_col;


tmp_grouped_by_all_col table should have 4 reocrds but it loads 7 records into the table.


+----------------------------+----------------------------+----------------------------+--+
| tmp_grouped_by_all_col.v1  | tmp_grouped_by_all_col.v2  | tmp_grouped_by_all_col.v3  |
+----------------------------+----------------------------+----------------------------+--+
| a                          | b                          | b                          |
| a                          | c1                         | c1                         |
| a                          | c2                         | c2                         |
| d                          | e                          | e                          |
| d                          | f                          | f                          |
| g                          | h                          | h                          |
| g                          | i                          | i                          |
+----------------------------+----------------------------+----------------------------+--+


 
 Multiple inserts using ""Group by"" and ""Distinct""  generates incorrect results",yes
13274887,"Looks like it was copied from a UT class and forgot to change it. 
 MetricsRegionServerWrapperImpl.getL1CacheHitCount always returns 200",13137288,"Deprecated HostAndPort#getHostText method was deleted in Guava 22.0and newHostAndPort#getHost method is not available before Guava 20.0.
This patch implements getHost(HostAndPort) method that extracts host from HostAndPort#toStringvalue.
This is a little hacky,that's whyI'm not sure if it worth to merge thispatch, but it could be niceif Hadoop will be Guava-neutral.
With this patch Hadoop can be built against latest Guava v24.0. 
 Make Hadoop compatible with Guava 22.0+",No
13216563,"If external tables are enabled for replication on an existing repl policy, then bootstrapping of external tables are combined with incremental dump.
If incremental bootstrap load fails with non-retryable error for which user will have to manually drop all the external tables before trying with another bootstrap dump. For full bootstrap, to retry with different dump, we suggested user to drop the DB but in this case they need to manually drop all the external tables which is not so user friendly. So, need to handle it in Hive side as follows.
REPL LOAD takes additional config (passed by user in WITH clause) that says, drop all the tables which are bootstrapped from previous dump. 
hive.repl.clean.tables.from.bootstrap=<previous_bootstrap_dump_dir>
Hive will use this config only if the current dump is combined bootstrap in incremental dump.
Caution to be taken by user that this config should not be passed if previous REPL LOAD (with bootstrap) was successful or any successful incremental dump+load happened after ""previous_bootstrap_dump_dir"". 
 Hive should support clean-up of previously bootstrapped tables when retry from different dump.",13227064,"In current AbfsOutPutStream, new temporary buffers always get allocated when uploading the data to server. These big chunk creates huge pressure on the GC, and in some extreme cases that GC doesn't happen in time(observed in some customer's env), it also lead to OutOfMemory issue easily.
Hence we need to replace the current implementation with a bufferpool. 
 ABFS: add bufferpool to AbfsOutputStream",No
13141728,"Entering the search term in anyof the three configuration tables (core, yarn, mapred) will result in filtering across all tables. Pagination also behaves the same way as clicking on one table changes the page in all the tables. This results in weird behavior since each table can have different number of pages present. 
 Yarn ui2 configuration tables have the same search and pagination instances",13130375,"Currently, YARN RM reject requested resource if memory or vcores are less than 0 or greater than maximum allocation. We should run the check for customized resource types as well. 
 DefaultAMSProcessor should properly check customized resource types against minimum/maximum allocation",No
13220405,"InTimelineServiceV2.md 255，The step to createthe timeline service schema does not work


Finally, run the schema creator tool to create the necessary tables:

    bin/hadoop org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator -create


should be


The schema creation can be run on the hbase cluster which is going to store the timeline
service tables. The schema creator tool requires both the timelineservice-hbase as well
as the hbase-server jars. Hence, during schema creation, you need to ensure that the
hbase classpath contains the yarn-timelineservice-hbase jar.

On the hbase cluster, you can get it from hdfs since we placed it there for the
coprocessor in the step above.

```
   hadoop fs -get /hbase/coprocessor/hadoop-yarn-server-timelineservice-hbase-client-${project.version}.jar
   hadoop fs -get /hbase/coprocessor/hadoop-yarn-server-timelineservice-${project.version}.jar
   hadoop fs -get /hbase/coprocessor/hadoop-yarn-server-timelineservice-hbase-common-${project.version}.jar
         <local-dir>/.
```

Next, add it to the hbase classpath as follows:

```
   export HBASE_CLASSPATH=$HBASE_CLASSPATH:/home/yarn/hadoop-current/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-${project.version}.jar
   export HBASE_CLASSPATH=$HBASE_CLASSPATH:/home/yarn/hadoop-current/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-${project.version}.jar
   export HBASE_CLASSPATH=$HBASE_CLASSPATH:/home/yarn/hadoop-current/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-${project.version}.jar
```

Finally, run the schema creator tool to create the necessary tables:

```
    bin/hbase org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator -create
```

 
 fix wrong command in TimelineServiceV2.md ",13177636,"

nodemanager/bin> ./hadoop org.apache.yarn.timelineservice.storage.TimelineSchemaCreator -create
Error: Could not find or load main class org.apache.yarn.timelineservice.storage.TimelineSchemaCreator



share/hadoop/yarn/timelineservice/ is not part of class path 
 TimelineSchemaCreator command not working",yes
13194259,"The jetty version 9.3.20.v20170531 being used currently in master has several CVE associated with it.
Version 9.3.25.v20180904 has those issues resolved. 
 Update jetty dependency to 9.3.25.v20180904",13187544,"Current version is9.3.20.v20170531 
 Upgrade version of Jetty to 9.3.25.v20180904",yes
13198473,"Currently the retry policy parameter is hard coded, should make it configurable for user. 
 ABFS: make retry policy configurable",13153015,"Consider we have two clusters in A and S state, and then we transit A to DA. And later we want to transit DA to A, since the remote cluster is in S, we should be able to do it. But there are some remote wals on the HDFS for the cluster in S state, so we need to remove them first before transiting the cluster in DA state to A. 
 Also remove remote wals when peer is in DA state",No
13161552,"For some uses cases it is necessary to know the output schema for a HiveQL before executing the query. But there is no existing client API that provides this information.
Hive JDBC doesn't provide the schema for parametric types in ResultSetMetaData.
GenericUDTFGetSplits bundles the proper schema metadata with the fragments for input splits. An option can be added to return only the schema metadata from compilation, and the generation of input splits can be skipped.
 
 Provide option for GenericUDTFGetSplits to return only schema metadata",13329636,"Use the shaded version of guava in hadoop-thirdparty 
 Use shaded guava from thirdparty",No
13342375,"Currently hbase-spark connector only works with Spark 2.x.
Apache Spark 3.0 has been relesead in June 2020.
This addresses the changes needed to run the connector with Spark 3.0 and also to be able to compile the connector using Spark 3.0 as a dependency. 
 Allow hbase-connector to be used with Apache Spark 3.0 ",13320811,"when  maven  command-line
mvn -Dspark.version=2.2.2 -Dscala.version=2.11.7 -Dscala.binary.version=2.11 -Dcheckstyle.skip=true -Dmaven.test.skip=true clean install
will return error
[ERROR] [Error] F:\hbase-connectors\spark\hbase-spark\src\main\scala\org\apache\hadoop\hbase\spark\datasources\HBaseTableScanRDD.scala:216: overloaded method value addTaskCompletionListener with alternatives:
  (f: org.apache.spark.TaskContext => Unit)org.apache.spark.TaskContext <and>
  (listener: org.apache.spark.util.TaskCompletionListener)org.apache.spark.TaskContext
 does not take type parameters
[ERROR] one error found
but use the spark.version=2.4.0 is ok
mvn -Dspark.version=2.4.0 -Dscala.version=2.11.7 -Dscala.binary.version=2.11 -Dcheckstyle.skip=true -Dmaven.test.skip=true clean install
other try
mvn -Dspark.version=3.0.0 -Dscala.version=2.12.12 -Dscala.binary.version=2.12  -Dcheckstyle.skip=true -Dmaven.test.skip=true clean install
return error 
[ERROR] [Error] F:\hbase-connectors\spark\hbase-spark\src\main\scala\org\apache\hadoop\hbase\spark\HBaseContext.scala:439: object SparkHadoopUtil in package deploy cannot be accessed in package org.apache.spark.deploy
[ERROR] [Error] F:\hbase-connectors\spark\hbase-spark\src\main\scala\org\apache\hadoop\hbase\spark\HBaseContext.scala:487: not found: value SparkHadoopUtil
[ERROR] two errors found
go to the spark @github
define SparkHadoopUtil  to private[spark] 


private[spark] class SparkHadoopUtil extends Logging {}


 
 hbase-connectors mvn install error",yes
13315903,"LLAP IO supports caching but currently this is only done via LlapRecordReader / using splits, aka good old mapreduce way.
At certain times it would worth to leverage the caching of files on certain paths, that are not necessarily associated with a record reader directly. An example of this could be the caching of ACID delete delta files, as they are currently being read without caching.
With this patch we'd extend the LLAP API and offer another entry point for retrieving metadata of ORC files. 
 LLAP - add API to look up ORC metadata for certain Path",13176609,"Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService. 
 VectorFileSinkArrowOperator",No
13224985,"hive-service added these dependency. hive-service-rpc do not need these dependency. 
 Remove tomcat:jasper-* from hive-service-rpc",13164216,"jasper dependency version looks old and unwanted. There is a comment which says it is required by thrift but I don't see jasper as thrift dependency. Try removing it to see if its safe (after precommit test run). 
 remove jasper dependency",yes
13267101,"

create table delta_result (a int) stored as orc tblproperties('transactional'='false');
insert into delta_result select 1;
select * from delta_result;


The above query will result in the following exception:
2019-11-08T13:49:05,780 WARN [HiveServer2-Handler-Pool: Thread-7906] thrift.ThriftCLIService: Error fetching results:2019-11-08T13:49:05,780 WARN [HiveServer2-Handler-Pool: Thread-7906] thrift.ThriftCLIService: Error fetching results:org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.RuntimeException: ORC split generation failed with exception: java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:481) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:331) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:946) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:567) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:801) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1837) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1822) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_211] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_211] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_211]Caused by: java.io.IOException: java.lang.RuntimeException: ORC split generation failed with exception: java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:638) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:545) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2142) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:241) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:476) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] ... 13 moreCaused by: java.lang.RuntimeException: ORC split generation failed with exception: java.lang.StringIndexOutOfBoundsException: String index out of range: -1 at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1929) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:2016) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchOperator.generateWrappedSplits(FetchOperator.java:461) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:430) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:336) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:576) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:545) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2142) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:241) ~[hive-exec-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:476) ~[hive-service-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT] 
 Avoid reading table as ACID when table name is starting with ""delta"", but table is not transactional",13325609,"Currently, WALPrettyPrinter provides an option to ignore the values in the output, but it prints the whole cell and has no option to ignore some information from the cell. Sometimes, the user may only need the row keys from WAL and it may reduce the size of output from WALPrettyPrinter significantly. 
We should provide flexibility to output only rowkey from the cell. 
In addition we should increase the flexibility for providing multiple tables in the table filter.  
 Provide a flexibility to print only row key and filter for multiple tables in the WALPrettyPrinter",No
13131136,"while using list command in hbase shell, most of all works well except which one contains 'd' char, as well as hbase program api prefix regex.
eg. list 'd.*' wont work, but '^d[\da-f]{31}' works well. and 'd.*' performs just listing all of the tables.

 
 hbase shell api 'list' or program api prefix has regex problem",13131135,"while using list command in hbase shell, most of all works well except which one contains 'd' char, as well as hbase program api prefix regex.
eg. list 'd.*' wont work, but '^d[\da-f]{31}' works well. and 'd.*' performs just listing all of the tables.

 
 hbase shell api 'list' or program api prefix has regex problem",yes
13325501,"
add or improve some logs for adding local & global deadnodes
logic improve
fix typo

 
 Tiny Improve for DeadNode detector",13186838,"This is caused by HADOOP-15642 but I'd missed it because I'd been playing with assumed roles locally (restricting their rights) and mistook the failures for ""steve's misconfigured the test role"", not ""the SDK
Some of the failures are actually due to changed messages from the AWS endpoints, so are independent of the SDK —and mean that existing builds will fail with false positives. The branch-3.1 patch fixes those tests only and needs to be applied to any branch with the ITestAssumeRole tests 
 S3A assumed role tests failing due to changed error text in AWS exceptions",No
13180840,"In the RpcServer#logResponse method the logic
stringifiedParam = stringifiedParam.subSequence(
0, LOG.isTraceEnabled() ? 1000 : 150) + "" <TRUNCATED>"";
If the trace is on and the length of stringifiedParam is 150< stringifiedParam.length() < 1000  will get the following exception.
java.lang.StringIndexOutOfBoundsException: String index out of range: 1000 
 RpcServer when trace enabled might get the following exception. java.lang.StringIndexOutOfBoundsException: ",13174559,"Two things:

We truncate RpcServer output to 1000 characters for trace logging. Would be better if that value was configurable.
There is the chance for an ArrayIndexOutOfBounds when truncating the TRACE log message.

Esteban mentioned this to me earlier, so I'm crediting him as the reporter.
cc: Josh Elser 
 Improve RpcServer TRACE logging",yes
13155057,"I have been trying to configure the Hadoop kms to use hdfs as the key provider but it seems that this functionality is failing.
I followed the Hadoop docs for that matter, and I added the following field to my kms-site.xml:


<property> 
   <name>hadoop.kms.key.provider.uri</name>
   <value>jceks://hdfs@nn1.example.com/kms/test.jceks</value> 
   <description> 
      URI of the backing KeyProvider for the KMS. 
   </description> 
</property>

That route exists in hdfs, and I expect the kms to create the file test.jceks for its keystore. However, the kms failed to start due to this error:


ERROR: Hadoop KMS could not be started REASON: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""hdfs"" Stacktrace: --------------------------------------------------- org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""hdfs"" at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3220) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3240) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3291) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3259) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:470) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) at org.apache.hadoop.crypto.key.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:132) at org.apache.hadoop.crypto.key.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88) at org.apache.hadoop.crypto.key.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:660) at org.apache.hadoop.crypto.key.KeyProviderFactory.get(KeyProviderFactory.java:96) at org.apache.hadoop.crypto.key.kms.server.KMSWebApp.contextInitialized(KMSWebApp.java:187) at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4276) at org.apache.catalina.core.StandardContext.start(StandardContext.java:4779) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:803) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:780) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583) at org.apache.catalina.startup.HostConfig.deployDirectory(HostConfig.java:1080) at org.apache.catalina.startup.HostConfig.deployDirectories(HostConfig.java:1003) at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:507) at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1322) at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:325) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1069) at org.apache.catalina.core.StandardHost.start(StandardHost.java:822) at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1061) at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463) at org.apache.catalina.core.StandardService.start(StandardService.java:525) at org.apache.catalina.core.StandardServer.start(StandardServer.java:761) at org.apache.catalina.startup.Catalina.start(Catalina.java:595) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)


For what I could manage to understand, it seems that this error is because there is no FileSystem implemented for HDFS. I have looked up this error but it always refers to a lack of jars for the hdfs-client when upgrading, which I have not done (it is a fresh installation). Ihave tested it using Hadoop 2.7.2 and 2.9.0
Thank you in advance. 
 Hadoop KMS with HDFS keystore: No FileSystem for scheme ""hdfs""",13137348,"See https://builds.apache.org/job/HBase%20Nightly/job/branch-2/284/testReport/junit/org.apache.hadoop.hbase.client/TestAsyncRegionAdminApi/testMergeRegions_0_/

java.lang.AssertionError: expected:<2> but was:<3> at org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi.testMergeRegions(TestAsyncRegionAdminApi.java:359)

Merge regions not work. The table still have 3 regions after the MergeRegionsProcedure finished.
The master start balance region 9e2773ba1efba79a2defa276e9a26ed4. But because the MergeRegionsProcedure pid=138 start work first, so the balance need wait for the lock. But after merge regions finished, the MoveRegionProcedure pid=139 start work and assign 9e2773ba1efba79a2defa276e9a26ed4 to a new region server. This is not right. The MoveRegionProcedure should skip to assign a region which was marked as offline. Or we should clear the merged regions' procedure whenMergeRegionsProcedure finished.

Logs:
2018-02-08 16:24:44,608 INFO [master/cd4730e3eae2:0.Chore.1] master.HMaster(1454): balance hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318
2018-02-08 16:24:44,608 DEBUG [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=37885] procedure2.ProcedureExecutor(868): Stored pid=138, state=RUNNABLE:MERGE_TABLE_REGIONS_PREPARE; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false
......
2018-02-08 16:24:50,111 INFO [PEWorker-13] procedure2.ProcedureExecutor(1249): Finished pid=138, state=SUCCESS; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false in 5.5710sec
2018-02-08 16:24:50,113 INFO [PEWorker-13] procedure.MasterProcedureScheduler(813): pid=139, state=RUNNABLE:MOVE_REGION_UNASSIGN; MoveRegionProcedure hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318 testMergeRegions testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4.
 
 Fix flaky TestAsyncRegionAdminApi",No
13313644,"We recently introduced new set of HMS APIs that take ValidWriteIDList in the request, as part of HIVE-22017.
We should switch to these new APIs, wherever required and start sending ValidWriteIDList in request for all the new HMS get_* APIs that are in request/response form. 
 Send ValidWriteIDList in request for all the new HMS get_* APIs that are in request/response form",13313642,"We recently introduced new set of HMS APIs that take ValidWriteIDList in the request, as part ofHIVE-22017.
We should switch to these new APIs, wherever required and start sending ValidWriteIDList in request for all the new HMS get_* APIs that are in request/response form. 
 Send ValidWriteIDList in request for all the new HMS get_* APIs that are in request/response form",yes
13134919,"Currently, we have only two scopes defined: NODE and RACK against which we check the cardinality of the placement.
This idea should be extended to support node-attribute scopes. For eg: Placement of containers across upgrade domains and failure domains.  
 Support special Node Attribute scopes in addition to NODE and RACK",13159147,"Modifications required in Distributed shell to support NodeAttributes 
 Modify distributedshell to support Node Attributes",yes
13155485,"there are some minor issues which were found during testing:

0 sized map is persisted
if reoptimization occurs write happens twice
move entry limit to only apply to the cache
ensure that executor not get blocked

 
 RuntimeStats fixes",13334534,"The shutdown of Datanode is a very long latency. A block scanner waits for 5 minutes to join on each VolumeScanner thread.
Since the scanners are daemon threads and do not alter the block content, it is safe to ignore such conditions on shutdown of Datanode. 
 Improve datanode shutdown latency",No
13250103,"HI,
I think current formal way to make multiple prefix filters is to create a FilterList and add PrefixFilter instances to the list:


FilterList allFilters = new FilterList(FilterList.Operator.MUST_PASS_ONE);
allFilters.addFilter(new PrefixFilter(Bytes.toBytes(""123"")));
allFilters.addFilter(new PrefixFilter(Bytes.toBytes(""456"")));
allFilters.addFilter(new PrefixFilter(Bytes.toBytes(""678"")));
scan.setFilter(allFilters);


(c.f., https://stackoverflow.com/questions/41074213/hbase-how-to-specify-multiple-prefix-filters-in-a-single-scan-operation )
However, in the case of creating a single prefix filter, HBase provides scan.setRowPrefixFilter method.
This method creates a range filter by setting a start row and a stop row.
The value of a stop row is decided by calling calculateTheClosestNextRowKeyForPrefix ( c.f., https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java#L574-L597 )
MultiRowRangeFilter could leverage a list of start row and stop row pairs and calculateTheClosestNextRowKeyForPrefix could compute the stop row value corresponding to given start row (i.e., a prefix).
I think this kind of filter (a filter which is functionally equivalent to multiple prefix filters) should be creatable by MultiRowRangeFilter and it's better than the current formal way.
Cheers, 
 MultiRowRangeFilter should provide a method for creating a filter which is functionally equivalent to multiple prefix filters",13315740,"See parent issue. There is a CLASSPATH issue when running against hadoop3 that needs resolving. Meantime, the canary fails to run with a cryptic message. This will surprise operators. Let me make it so you ask for the canary webui; by default, it does not come up. 
 [HS2] Send tableId in request for get_table_request API",No
13204706,"From the user mailing thread: – From tzq <admin@tinohean.com>
I might find a spelling error in ""http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html""
hadoop daemonlog -getlevel <host:port> <classname> [-protocol (http|https)]
hdoop daemonlog -setlevel <host:port> <classname> <level> [-protocol (http|https)]
The second hadoop has a typo. 
 Typo in daemonlog docs",13204607,"http://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-common/CommandsManual.html

hdoop daemonlog -setlevel <host:port> <classname> <level> [-protocol (http|https)]


hdoop should be hadoop.
This issue was reported from user mailing list:  https://lists.apache.org/thread.html/0d57c60d3242e4bd8f0401669957c251e687077bb7b7fb2725837ba4@%3Cuser.hadoop.apache.org%3E 
 Fix typo in CommandsManual.md",yes
13168309,"Intermediately Resource manager going down with following exceptions

2018-06-25 15:24:30,572 FATAL event.EventDispatcher (EventDispatcher.java:run(75)) - Error in handling event type NODE_UPDATE to the Event Dispatcher
java.lang.NullPointerException
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.getLocalityWaitFactor(RegularContainerAllocator.java:268)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.canAssign(RegularContainerAllocator.java:315)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignOffSwitchContainers(RegularContainerAllocator.java:388)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainersOnNode(RegularContainerAllocator.java:469)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.tryAllocateOnNode(RegularContainerAllocator.java:250)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:819)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:857)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocator.assignContainers(ContainerAllocator.java:55)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:868)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1121)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:734)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:558)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:734)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:558)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:734)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:558)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1338)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1333)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1422)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1197)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1059)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1464)
    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:150)
    at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)
    at java.lang.Thread.run(Thread.java:745)
2018-06-25 15:24:30,573 INFO event.EventDispatcher (EventDispatcher.java:run(79)) - Exiting, bbye..
2018-06-25 15:24:30,579 ERROR delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:run(690)) - ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted

Before the build we applied the patches available for this version where we found the same kind of exception in one of the Jira
https://issues.apache.org/jira/browse/YARN-6629
but on different class file. 
 Resource Manager shutdown with FATAL Exception",13168792,"At some point RM just hangs and stops allocating resources.At the point RM get hangs, YARN throwsNullPointerExceptionat RegularContainerAllocator#allocate, andRegularContainerAllocator#preCheckForPlacementSet, andRegularContainerAllocator#getLocalityWaitFactor. 
 YARN RM hangs and stops allocating resources when applications successively running",yes
13311745,"Problems
Example:


drop table if exists tbl2;
create transactional table tbl2 (a int, b int) clustered by (a) into 4 buckets stored as ORC TBLPROPERTIES('transactional'='true','transactional_properties'='default');
insert into tbl2 values(1,2),(1,3),(1,4),(2,2),(2,3),(2,4);
insert into tbl2 values(3,2),(3,3),(3,4),(4,2),(4,3),(4,4);
insert into tbl2 values(5,2),(5,3),(5,4),(6,2),(6,3),(6,4);

E.g. in the example above, bucketId=0 when a=2 and a=6.
1. Data loss
 In non-acid tables, an operator's temp files are named with their task id. Because of this snippet, temp files in the FileSinkOperator for compaction tables are identified by their bucket_id.


if (conf.isCompactionTable()) {
 fsp.initializeBucketPaths(filesIdx, AcidUtils.BUCKET_PREFIX + String.format(AcidUtils.BUCKET_DIGITS, bucketId),
 isNativeTable(), isSkewedStoredAsSubDirectories);
 } else {
 fsp.initializeBucketPaths(filesIdx, taskId, isNativeTable(), isSkewedStoredAsSubDirectories);
 }


So 2 temp files containing data with a=2 and a=6 will be named bucket_0 and not 000000_0 and 000000_1 as they would normally.
 In FileSinkOperator.commit, when data with a=2, filename: bucket_0 is moved from _task_tmp.-ext-10002 to _tmp.-ext-10002, it overwrites the files already there with a=6 data, because it too is named bucket_0. You can see in the logs:


 WARN [LocalJobRunner Map Task Executor #0] exec.FileSinkOperator: Target path file:.../hive/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnNoBuckets-1591107230237/warehouse/testmajorcompaction/base_0000002_v0000013/.hive-staging_hive_2020-06-02_07-15-21_771_8551447285061957908-1/_tmp.-ext-10002/bucket_00000 with a size 610 exists. Trying to delete it.


2. Results in one original file
 OrcFileMergeOperator merges the results of the FSOp into 1 file named 000000_0.
Fix
1. FSOp will store data as: taskid/bucketId. e.g. 0_0/bucket_0
2. OrcMergeFileOp, instead of merging a bunch of files into 1 file named 000000_0, will merge all files named bucket_0 into one file named bucket_0, and so on.
3. MoveTask will get rid of the taskId directories if present and only move the bucket files in them, in case OrcMergeFileOp is not run. 
 Major QB compaction with multiple FileSinkOperators results in data loss and one original file",13267117,"

set hive.execution.engine=mr;
drop table if exists tbl2;
create table tbl2 (a int, b int) clustered by (a) into 2 buckets stored as ORC TBLPROPERTIES('bucketing_version'='2', 'transactional'='true', 'compactorthreshold.hive.compactor.delta.num.threshold'='3');
insert into tbl2 values(1,2),(1,3),(1,4),(2,2),(2,3),(2,4);
insert into tbl2 values(3,2),(3,3),(3,4),(4,2),(4,3),(4,4);
delete from tbl2 where b = 2;
insert into tbl2 values(5,2),(5,3),(5,4),(6,2),(6,3),(6,4);
delete from tbl2 where a = 1;


Having the above use case, at the end of the major compaction the base directory contains only one bucket file, although the table is bucketed in 2 buckets. Before running the compaction, the delta directories contains the right amount of bucket files, and the data is split accordingly.
 
 Query based major compaction always creates only one bucket file",yes
13142151,"Currently Distcp uploads a file by two strategies

append parts
copy to temp then rename

option 2 executes the following sequence in promoteTmpToTarget


    if ((fs.exists(target) && !fs.delete(target, false))
        || (!fs.exists(target.getParent()) && !fs.mkdirs(target.getParent()))
        || !fs.rename(tmpTarget, target)) {
      throw new IOException(""Failed to promote tmp-file:"" + tmpTarget
                              + "" to: "" + target);
    }


For any object store, that's a lot of HTTP requests; for S3A you are looking at 12+ requests and an O(data) copy call. 
This is not a good upload strategy for any store which manifests its output atomically at the end of the write().
Proposed: add a switch to write directly to the dest path, which can be supplied as either a conf option (distcp.direct.write = true) or a CLI option (-direct).
 
 Distcp to add no-rename copy option",13209442,"When writing to an S3-based target, the temp file and rename logic in RetriableFileCopyCommand adds some unnecessary cost to the job, as the rename operation does a server-side copy + delete in S3 [1]. The renames are parallelized across all of the DistCp map tasks, so the severity is mitigated to some extent. However a configuration property to conditionally allow distributed copies to avoid that expense and write directly to the target path would improve performance considerably.
[1] https://github.com/apache/hadoop/blob/release-3.2.0-RC1/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md#object-stores-vs-filesystems 
 Avoid expensive rename when DistCp is writing to S3",yes
13288733,"The tests inorg.apache.hadoop.metrics2.impl.TestMetricsSystemImpl#testInitFirstVerifyCallBacks andorg.apache.hadoop.metrics2.impl.TestMetricsSystemImpl#testInitFirstVerifyStopInvokedImmediately can fail.
java.lang.AssertionError:
Element 0 for metrics expected:<MetricCounterLong{info=MetricsInfoImpl

{name=C1, description=C1 desc}

, value=1}>
but was:<MetricGaugeLong{info=MetricsInfoImpl

{name=G1, description=G1 desc}

, value=2}>
at org.junit.Assert.fail(Assert.java:88)
at org.junit.Assert.failNotEquals(Assert.java:834)
at org.junit.Assert.assertEquals(Assert.java:118)
at org.apache.hadoop.test.MoreAsserts.assertEquals(MoreAsserts.java:60)
at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.checkMetricsRecords(TestMetricsSystemImpl.java:439)
at org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testInitFirstVerifyCallBacks(TestMetricsSystemImpl.java:178)

The root cause of this failure can be analyzed in the following stack trace:
java.lang.Class.getDeclaredFields(Class.java:1916)
org.apache.hadoop.util.ReflectionUtils.getDeclaredFieldsIncludingInherited(ReflectionUtils.java:353)
org.apache.hadoop.metrics2.lib.MetricsSourceBuilder.<init>(MetricsSourceBuilder.java:68)
org.apache.hadoop.metrics2.lib.MetricsAnnotations.newSourceBuilder(MetricsAnnotations.java:43)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:223)
org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl.testInitFirstVerifyCallBacks(TestMetricsSystemImpl.java:156)
The specification about getDeclaredFields() says that ""the elements in the returned array are not sorted and are not in any particular order"". The documentation is here for your reference: https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html#getDeclaredFields--
 And the behaviour might be different for different JVM versions or vendors

The fix is to sort the fields returned by getDeclaredFields() so that the non-deterministic behaviour can be eliminated completely. In this way, the test becomes more stable and it will not suffer from the failure above any more.



 
 Sort fields in ReflectionUtils.java",13184112,"We have seen this issue in multiple runs:
https://builds.apache.org/job/PreCommit-HADOOP-Build/15146/testReport/org.apache.hadoop.hdfs.server.datanode/TestDataNodeMultipleRegistrations/testClusterIdMismatchAtStartupWithHA/
https://builds.apache.org/job/PreCommit-HADOOP-Build/15116/testReport/org.apache.hadoop.hdfs.server.datanode/TestDataNodeMultipleRegistrations/testDNWithInvalidStorageWithHA/ 
 TestDataNodeMultipleRegistrations is flaky",No
13171862,"
Remove code
Remove exception handling
Remove printStackTrace call

 
 Simplify StringSubstrColStart Initialization",13321632,"This was a bug we had seen multiple times on Hadoop 2.6.2. And the following analysis is based on the core dump, logs, and code in 2017 with Hadoop 2.6.2. We hadn't seen it after 2.9 in our env. However, it was because of the RPC retry policy change and other changes. There's still a possibility even with the current code if I didn't miss anything.
High-level description:
 We had seen a starving mapper issue several times. The MR job stuck in a live lock state and couldn't make any progress. The queue is full so the pending mapper can’t get any resource to continue, and the application master failed to preempt the reducer, thus causing the job to be stuck. The reason why the application master didn’t preempt the reducer was that there was a leaked container in assigned mappers. The node manager failed to report the completed container to the resource manager.
Detailed steps:


Container_1501226097332_249991_01_000199 was assigned to attempt_1501226097332_249991_m_000095_0 on 2017-08-08 16:00:00,417.


appmaster.log:6464:2017-08-08 16:00:00,417 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1501226097332_249991_01_000199 to attempt_1501226097332_249991_m_000095_0


The container finished on 2017-08-08 16:02:53,313.


yarn-mapred-nodemanager-.log.1:2017-08-08 16:02:53,313 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1501226097332_249991_01_000199 transitioned from RUNNING to EXITED_WITH_SUCCESS
yarn-mapred-nodemanager-.log.1:2017-08-08 16:02:53,313 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1501226097332_249991_01_000199


The NodeStatusUpdater go an exception in the heartbeat on 2017-08-08 16:07:04,238. In fact, the heartbeat request is actually handled by resource manager, however, the node manager failed to receive the response. Let’s assume the heartBeatResponseId=$hid in node manager. According to our current configuration, next heartbeat will be 10s later.


2017-08-08 16:07:04,238 ERROR org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Caught exception in status-updater
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ; destination host is: XXXXXXX
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
        at org.apache.hadoop.ipc.Client.call(Client.java:1472)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy33.nodeHeartbeat(Unknown Source)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.nodeHeartbeat(ResourceTrackerPBClientImpl.java:80)
        at sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy34.nodeHeartbeat(Unknown Source)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:597)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:513)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)


NodeStatusUpdaterImpl try to send another heart beart 10s after last exception. Container_1501226097332_249991_01_000199 was added to the heartbeat request as a completed container for the first time. We can confirm this point from the timestamp in recentlyStoppedContainers@NodeStatusUpdaterImpl.


Java.util.LinkedHashMap$Entry@0x7889caca0
http://localhost:7000/object/0x7889cacc8
Value 1502209034239 
Time add to recently stopped 
1502208434239 = 1502209034239 - 600000
$date -d@1502208434
Tue Aug  8 16:07:14 UTC 2017


RM thought the request as duplication heartbeat as the $hid is the same as the heartBeatId it received last time. So it returned the last response generated from previous heartbeat and didn’t handle the request, in other words, it didn't add container_1501226097332_249991_01_000199 as a completed container to its own data structure.


// 3. Check if it's a 'fresh' heartbeat i.e. not duplicate heartbeat
NodeHeartbeatResponse lastNodeHeartbeatResponse = rmNode.getLastNodeHeartBeatResponse();
if (remoteNodeStatus.getResponseId() + 1 == lastNodeHeartbeatResponse
   .getResponseId()) {
 LOG.info(""Received duplicate heartbeat from node ""
     + rmNode.getNodeAddress()+ "" responseId="" + remoteNodeStatus.getResponseId());
 return lastNodeHeartbeatResponse;


Node manager received the response and clear container_1501226097332_249991_01_000199 from pendingCompletedContainers@nodeStatusUpdaterImpl. However, container_1501226097332_249991_01_000199 is still in recentlyStoppedContainers@NodeStautsUpdaterImp. So the container won’t report container_1501226097332_249991_01_000199 again as a complete container in the heartbeat request.


             if (containerStatus.getState() == ContainerState.COMPLETE) {
 if (isApplicationStopped(applicationId)) {
   if (LOG.isDebugEnabled()) {
     LOG.debug(applicationId + "" is completing, "" + "" remove ""
         + containerId + "" from NM context."");
   }
   context.getContainers().remove(containerId);
   pendingCompletedContainers.put(containerId, containerStatus);
 } else {
   if (!isContainerRecentlyStopped(containerId)) {
     pendingCompletedContainers.put(containerId, containerStatus);
     // Adding to finished containers cache. Cache will keep it around at
     // least for #durationToTrackStoppedContainers duration. In the
     // subsequent call to stop the container will get removed from cache.
     addCompletedContainer(containerId);
   }
 }
} else {
 containerStatuses.add(containerStatus);
}


The application master relies on getResourses() to get completed containers and remove them from assignedRequests. As the container will never be reported to the RM by nodemanger again, thus application master can’t remove the attempt associated with the container. And the preemption calculation returns false due to following codes, thus causing the starving mapper.


boolean preemptReducesIfNeeded() {
 if (reduceResourceRequest.equals(Resources.none())) {
   return false; // no reduces
 }
 if (assignedRequests.maps.size() > 0) {
   // there are assigned mappers
   return false;
 }



 
 Flaky test AsyncResponseHandlerTest",No
13324409,"If there are large number of events that haven't been cleaned up for some reason, then ObjectStore.cleanWriteNotificationEvents() can run out of memory while it loads all the events to be deleted.
 It should fetch events in batches.
Similar tohttps://issues.apache.org/jira/browse/HIVE-19430 
 ObjectStore.cleanWriteNotificationEvents OutOfMemory on large number of pending events",13287268,"Add support for SPS in httpfs 
 HttpFS : Add Support for Storage Policy Satisfier ",No
13325869,"In our cluster, the NameNode appears NPE when processing lifeline messages sent by the DataNode, which will cause an maxLoad exception calculated by NN.
because DataNode is identified as busy and unable to allocate available nodes in choose  DataNode, program loop execution results in high CPU and reduces the processing performance of the cluster.
NameNode the exception stack:


2020-08-25 00:59:02,977 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8022, call Call#20535 Retry#0 org.apache.hadoop.hdfs.server.protocol.DatanodeLifelineProtocol.sendLifeline from xxxxx:34766
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorageStats(DatanodeDescriptor.java:460)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateHeartbeatState(DatanodeDescriptor.java:390)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager.updateLifeline(HeartbeatManager.java:254)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.handleLifeline(DatanodeManager.java:1805)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleLifeline(FSNamesystem.java:4039)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendLifeline(NameNodeRpcServer.java:1761)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeLifelineProtocolServerSideTranslatorPB.sendLifeline(DatanodeLifelineProtocolServerSideTranslatorPB.java:62)
        at org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2.callBlockingMethod(DatanodeLifelineProtocolProtos.java:409)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:886)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:828)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1903)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2717)




// DatanodeDescriptor#updateStorageStats
...
for (StorageReport report : reports) {

      DatanodeStorageInfo storage = null;
      synchronized (storageMap) {
        storage =
            storageMap.get(report.getStorage().getStorageID());
      }
      if (checkFailedStorages) {
        failedStorageInfos.remove(storage);
      }

      storage.receivedHeartbeat(report);  //  NPE exception occurred here 
      // skip accounting for capacity of PROVIDED storages!
      if (StorageType.PROVIDED.equals(storage.getStorageType())) {
        continue;
      }
...

 
 Fix NPE in DatanodeDescriptor#updateStorageStats when handle DN Lifeline",13195506,"java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateStorageStats(DatanodeDescriptor.java:460)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.updateHeartbeatState(DatanodeDescriptor.java:390)
        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager.updateLifeline(HeartbeatManager.java:254)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.handleLifeline(DatanodeManager.java:1789)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleLifeline(FSNamesystem.java:3997)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendLifeline(NameNodeRpcServer.java:1666)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeLifelineProtocolServerSideTranslatorPB.sendLifeline(DatanodeLifelineProtocolServerSideTranslatorPB.java:62)
        at org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$DatanodeLifelineProtocolService$2.callBlockingMethod(DatanodeLifelineProtocolProtos.java:409)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:898)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:844)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2727) 
 Fix NPE when PROVIDED storage is missing",yes
13129521,"exposed by: HIVE-18359
in case of vectorization, the summary row object was left as is (presumed null earlier); which may cause it to be inconsistent isNull conditions in .VectorHashKeyWrapperBatch
issue happens only if:

vectorizable groupby
groupping set contains empty
non-trivial empty; mapper is run
groupping key is select ; with a type which is backed by a bytea; ex:string



set hive.vectorized.execution.enabled=true;
create table tx2 (a integer,b integer,c integer,d double,u string,bi binary) stored as orc;

insert into tx2 values
(1,2,3,1.1,'x','b'),
(3,2,3,1.1,'y','b');

select  sum(a),
        u,
        bi,
        'asd',
        grouping(bi),
        'NULL,1' as expected
from    tx2
where   a=2
group by a,u,bi grouping sets ( u, (), bi);


causes:


Caused by: java.lang.NullPointerException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:173)
        at org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.assignRowColumn(VectorHashKeyWrapperBatch.java:1065)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.writeSingleRow(VectorGroupByOperator.java:1134)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.access$800(VectorGroupByOperator.java:74)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeReduceMergePartial.close(VectorGroupByOperator.java:862)
        at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.closeOp(VectorGroupByOperator.java:1176)
        at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:705)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:383)
        ... 16 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1515531021543_0001_12_01 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
2018-01-09T12:50:30,611 DEBUG [01fdcefd-40b0-45a6-8e5b-b1cd14241088 main] ql.Driver: Shutting down query 

 
 upgrade to tez-0.9.1",13157886,"The runThriftServer() in TestThriftHttpServer,  we have the following :


private void runThriftServer(int customHeaderSize) throws Exception {
//......
    startHttpServerThread(args.toArray(new String[args.size()]));

    // wait up to 10s for the server to start
    for (int i = 0; i < 100
        && (thriftServer.serverRunner == null ||  thriftServer.serverRunner.httpServer ==
        null); i++) {
      Thread.sleep(100);
    }
//......
      checkHttpMethods(url);
//......
}


The port may still not open  even if  the thriftServer.serverRunner != null  and  thriftServer.serverRunner.httpServer != null,   so the checkHttpMethods will get a connection refused ... 
We should wait till the port is really listening.... 
 Fix the flaky TestThriftHttpServer",No
13289441,"Hive Metastore should accept requests for table creation where the id is set, ignoring it. 
 Allow table id to be set for table creation requests",13253841,"There's a hidden synchronization across threads when looking up isStateful and isDeterministic.
https://github.com/apache/hive/blob/master/common/src/java/org/apache/hive/common/util/AnnotationUtils.java#L27


  // to avoid https://bugs.openjdk.java.net/browse/JDK-7122142
  public static <T extends Annotation> T getAnnotation(Class<?> clazz, Class<T> annotationClass) {
    synchronized (annotationClass) {
      return clazz.getAnnotation(annotationClass);
    }
  }


This is serializing multiple threads initializing UDFs (or checking them during compilation) & also being locked across threads for each instance of GenericUDFOpEqual in the specific scenario.
https://bugs.openjdk.java.net/browse/JDK-7122142 is fixed in jdk8+ 
 UDF: FunctionRegistry synchronizes on org.apache.hadoop.hive.ql.udf.UDFType class",No
13287428,"https://github.com/apache/hive/pull/927 
 Schedule Repl Dump Task using Hive scheduler",13298387,"The following highlighted line seems to be incorrect in the test suite:
https://github.com/apache/hive/blob/master/ql/src/test/results/clientpositive/perf/tez/cbo_query95.q.out#L89
Note that the project takes all the columns from the table scan, yet it only needs a couple of them.
I did some very small debugging on this. When I removed the applyJoinOrderingTransform here:https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L1897
... the problem goes away. So presumably one of the rules in there is causing the problem.
Here is a slightly simplified version of the query which has the same problem (using the same tpc-ds database):
explain cbo with ws_wh as
(select ws1.ws_order_number
from web_sales ws1,web_returns wr2 
where ws1.ws_order_number = wr2.wr_order_number)
select
 ws_order_number
from
 web_sales ws1 
where
ws1.ws_order_number in (select wr_order_number
              from web_returns,ws_wh
              where wr_order_number = ws_wh.ws_order_number)
 
 Project not defined correctly after reordering a join",No
13301696,"HADOOP-16621 removed two public API methods:
1. o.a.h.security.token.Token(TokenProto tokenPB) --> replaced by o.a.h.security.ipc.ProtobufHelper.tokenFromProto()
2. o.a.h.security.token.Token.toTokenProto() --> replaced by o.a.h.security.ipc.ProtobufHelper.protoFromToken()
Protobuf is declared private. Should we make it public now? 
 Declare ProtobufHelper a public API",13206658,"Looking at the Hadoop source code, there are a few places where the code assumes the user name can be acquired from Java's system property user.name.
For example,
FileSystem

/** Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   */
  public Path getHomeDirectory() {
    return this.makeQualified(
        new Path(USER_HOME_PREFIX + ""/"" + System.getProperty(""user.name"")));
  }


This is incorrect, as in a Kerberized environment, a user may login as a user principal different from its system login account.
It would be better to use UserGroupInformation.getCurrentUser().getShortUserName(), similar to HDFS-12485.
Unfortunately, I am seeing this improper use in Yarn, HDFS federation SFTPFilesystem and Ozone code (tests are ignored)
The impact should be small, since it only affects the case where system is Kerberized and that the user principal is different from system login account. 
 Replace incorrect use of system property user.name",No
13151337,"HDFS-13310 introduces an API for DNA_BACKUP. Here, we implement DNA_BACKUP command in Datanode. 
These have been broken up to make reviewing it easier. 
 [PROVIDED Phase 2] Implement DNA_BACKUP command in Datanode",13183670,"The current hadoop.apache.org doesn't mention Ozone in the ""Modules"" section.
We can add something like this (or better):
Hadoop Ozone is an object store for Hadoop on top of the Hadoop HDDS which provides low-level binary storage layer.
We can also link to the http://ozone.hadoop.apache.org 
 Add Ozone submodule to the hadoop.apache.org",No
13319021,"Having this repo on github is great and will probably increase the number of contributors tremendously. With that there will of course be an influx in the number of PRs so it's going to be critical to make reviewing PRs as easy as possible.
Usinghttps://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATEas a template improve the template to make reviewing easier for contributors. 
 Improve Github PR Template",13176303,"There is a typo in the existing YarnConfiguration which uses theDEFAULT_NM_LOCALIZER_PORT as the default for NM Collector Service port. This Jira aims to fix the typo. 
 Fix NM Collector Service Port issue in YarnConfiguration",No
13320184,"As we only public src tarballs for hbase-thirdparty. 
 ABFS: configure output stream thread pool",13283135,"I see this test fail a lot in my environments. It also uses such a large array that it seems particularly memory wasteful and difficult to get good contention in the test as well. 
 TestSyncTimeRangeTracker fails quite easily and allocates a very expensive array.",No
13197004,"

        } else {
          assert (!rqst.isSetSrcTxnToWriteIdList());
          assert (rqst.isSetTxnIds());
          txnIds = rqst.getTxnIds();
        }

        Collections.sort(txnIds); //easier to read logs and for assumption done in replication flow


when the input comes from


  @Override
  public long allocateTableWriteId(long txnId, String dbName, String tableName) throws TException {
    return allocateTableWriteIdsBatch(Collections.singletonList(txnId), dbName, tableName).get(0).getWriteId();
  }




java.lang.UnsupportedOperationException: null
    at java.util.AbstractList.set(AbstractList.java:132) ~[?:1.8.0]
    at java.util.AbstractList$ListItr.set(AbstractList.java:426) ~[?:1.8.0]
    at java.util.Collections.sort(Collections.java:170) ~[?:1.8.0]
    at org.apache.hadoop.hive.metastore.txn.TxnHandler.allocateTableWriteIds(TxnHandler.java:1523) ~[hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.allocate_table_write_ids(HiveMetaStore.java:7349) ~[hive-standalone-metastore-server-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]


 
 TxnHandler: sort() called on immutable lists",13281346,"HIVE-19937 introduced some interning logic into mapwork deserialization process, but it's only related to spark, maybe we should skip this for tez, reducing the cpu pressure in tez tasks.
UPDATE: Hive on spark is not supported anymore, the  MapWorkSerializer can be completely removed. 
 Skip interning of MapWork fields during deserialization",No
13283721,"Currently CookieSigner throws an IllegalArgumentException if the cookie signature is invalid. 


if (!MessageDigest.isEqual(originalSignature.getBytes(), currentSignature.getBytes())) {
      throw new IllegalArgumentException(""Invalid sign, original = "" + originalSignature +
        "" current = "" + currentSignature);
    }


CookieSigner is only used in the ThriftHttpServlet#getClientNameFromCookie and doesn't handle the IllegalArgumentException. It is only checking if the value from the cookie is null or not.
https://github.com/apache/hive/blob/master/service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java#L295


      currValue = signer.verifyAndExtract(currValue);
      // Retrieve the user name, do the final validation step.
      if (currValue != null) {


This should be fixed to either:
a) Have CookieSigner not return an IllegalArgumentException
b) Improve ThriftHttpServlet to handle CookieSigner throwing an IllegalArgumentException 
 ThriftHttpServlet#getClientNameFromCookie should handle CookieSigner IllegalArgumentException on invalid cookie signature",13202274,"TestThriftHttpServer is the first on the flaky list for branch-1 and branch-1.4 with approximately 60% failure rate.
Thrift server is not yet accepting request at the time the test starts.
java.net.ConnectException: Connection refused (Connection refused) at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.checkHttpMethods(TestThriftHttpServer.java:275) at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.testThriftServerHttpOptionsForbiddenWhenOptionsDisabled(TestThriftHttpServer.java:176) 
 ConnectException in TestThriftHttpServer",No
13270074,"Noticed some unused methods on RequestConverter class, probably some leftovers from previous refactorings. Since this is targeted for private use, should be fine to just remove those extra unused on master branch. 
 Remove unused methods from RequestConverter",13179551,"Backport HBASE-17519 (Rollback the removed cells) to branch-1.3, which handles rollback of append/increment completely in case of failure 
 Backport HBASE-17519 (Rollback the removed cells) to branch-1.3",No
13203076,"Migrate the submarine installation script document from the hadoop-yarn project. 
 [Submarine] Update submarine installation script document",13131670,"Starting Beeline gives the following warnings:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in[jar:file:/opt/cloudera/parcels/CDH-6.x-1.cdh6.x.p0.215261/jars/log4j-slf4j-impl-2.8.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in[jar:file:/opt/cloudera/parcels/CDH-6.x-1.cdh6.x.p0.215261/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Seehttp://www.slf4j.org/codes.html#multiple_bindingsfor an explanation.
SLF4J: Actual binding is of type[org.apache.logging.slf4j.Log4jLoggerFactory]
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property 'org.apache.logging.log4j.simplelog.StatusLogger.level' to TRACE to show Log4j2 internal initialization logging.
 
 Beeline gives log4j warnings",No
13167771,"YARN-8319 filter check for flows pages. The same behavior need to be added for all other REST API as long as ATS provides support for ACLs 
 Add basic ACL check for all ATS v2 REST APIs",13154307,"To let resource manager run long running applications you must set the property yarn.webapp.api-service.enable to true, as described in http://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/yarn-service/QuickStart.html.
By the way, on the documentation, it is indicated only in the REST API section.
I think it is useful to add this configuration also in the first section of the quick start guide 
 yarn.webapp.api-service.enable should be highlighted in the quickstart",No
13281721,"A converted configuration throws this error:

2020-01-27 03:35:35,007 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2020-01-27 03:35:35,008 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
java.lang.IllegalArgumentException: Illegal queue mapping u:%user:%user;u:%user:root.users.%user;u:%user:root.default
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getQueueMappings(CapacitySchedulerConfiguration.java:1113)
	at org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule.initialize(UserGroupMappingPlacementRule.java:244)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getUserGroupMappingPlacementRule(CapacityScheduler.java:671)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.updatePlacementRules(CapacityScheduler.java:712)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:753)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:361)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:426)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)


Mapping rules should be separated by a "","" character, not by a semicolon. 
 FS-CS converter: separator between mapping rules should be comma",13280990,"This ticket is intended to fix two issues:
1. Based on the latest documentation, there are two important properties that are ignored if we have placement rules:



Property
Explanation


yarn.scheduler.fair.allow-undeclared-pools
If this is true, new queues can be created at application submission time, whether because they are specified as the application’s queue by the submitter or because they are placed there by the user-as-default-queue property. If this is false, any time an app would be placed in a queue that is not specified in the allocations file, it is placed in the “default” queue instead. Defaults to true. If a queue placement policy is given in the allocations file, this property is ignored.


yarn.scheduler.fair.user-as-default-queue
Whether to use the username associated with the allocation as the default queue name, in the event that a queue name is not specified. If this is set to “false” or unset, all jobs have a shared default queue, named “default”. Defaults to true. If a queue placement policy is given in the allocations file, this property is ignored.



Right now these settings affects the conversion regardless of the placement rules. 
2. A converted configuration throws this error:

2020-01-27 03:35:35,007 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2020-01-27 03:35:35,008 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
java.lang.IllegalArgumentException: Illegal queue mapping u:%user:%user;u:%user:root.users.%user;u:%user:root.default
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getQueueMappings(CapacitySchedulerConfiguration.java:1113)
	at org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule.initialize(UserGroupMappingPlacementRule.java:244)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getUserGroupMappingPlacementRule(CapacityScheduler.java:671)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.updatePlacementRules(CapacityScheduler.java:712)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:753)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:361)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:426)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)


Mapping rules should be separated by a "","" character, not by a semicolon.
3. When initializing FS for conversion, we add the current yarn-site.xml as resource. This is not necessary. This can cause problems like:

[...]
20/01/29 02:45:38 ERROR config.RangerConfiguration: addResourceIfReadable(ranger-yarn-audit.xml): couldn't find resource file location
20/01/29 02:45:38 ERROR config.RangerConfiguration: addResourceIfReadable(ranger-yarn-security.xml): couldn't find resource file location
20/01/29 02:45:38 ERROR config.RangerConfiguration: addResourceIfReadable(ranger-yarn-policymgr-ssl.xml): couldn't find resource file location
20/01/29 02:45:38 ERROR conf.Configuration: error parsing conf file:/etc/hadoop/conf.cloudera.YARN-1/xasecure-audit.xml
java.io.FileNotFoundException: /etc/hadoop/conf.cloudera.YARN-1/xasecure-audit.xml (No such file or directory)
	at java.base/java.io.FileInputStream.open0(Native Method)
	at java.base/java.io.FileInputStream.open(FileInputStream.java:219)
	at java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)
	at java.base/java.io.FileInputStream.<init>(FileInputStream.java:112)
	at java.base/sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:86)
	at java.base/sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:184)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2966)
	at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3057)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3018)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2996)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2871)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:1223)
	at org.apache.ranger.authorization.hadoop.config.RangerPluginConfig.addSslConfigResource(RangerPluginConfig.java:234)
	at org.apache.ranger.authorization.hadoop.config.RangerPluginConfig.addResourcesForServiceType(RangerPluginConfig.java:161)
	at org.apache.ranger.authorization.hadoop.config.RangerPluginConfig.<init>(RangerPluginConfig.java:51)
	at org.apache.ranger.plugin.service.RangerBasePlugin.<init>(RangerBasePlugin.java:75)
	at org.apache.ranger.authorization.yarn.authorizer.RangerYarnPlugin.<init>(RangerYarnAuthorizer.java:246)
	at org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer.init(RangerYarnAuthorizer.java:81)
	at org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer.init(RangerYarnAuthorizer.java:94)
	at org.apache.hadoop.yarn.security.YarnAuthorizationProvider.getInstance(YarnAuthorizationProvider.java:57)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.initScheduler(FairScheduler.java:1375)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.serviceInit(FairScheduler.java:1484)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter.convert(FSConfigToCSConfigConverter.java:209)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter.convert(FSConfigToCSConfigConverter.java:102)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler.parseAndConvert(FSConfigToCSConfigArgumentHandler.java:137)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterMain.main(FSConfigToCSConfigConverterMain.java:40)
20/01/29 02:45:38 WARN config.RangerPluginConfig:  Unable to find SSL Configs
20/01/29 02:45:38 INFO config.RangerPluginConfig: PolicyEngineOptions: { evaluatorType: auto, evaluateDelegateAdminOnly: false, disableContextEnrichers: false, disableCustomConditions: false, disableTagPolicyEvaluation: false, enableTagEnricherWithLocalRefresher: false, disableTrieLookupPrefilter: false, optimizeTrieForRetrieval: false, cacheAuditResult: false }
20/01/29 02:45:38 INFO provider.AuditProviderFactory: AuditProviderFactory: creating..
20/01/29 02:45:38 INFO provider.AuditProviderFactory: AuditProviderFactory: initializing..
20/01/29 02:45:38 INFO provider.AuditProviderFactory: AuditProviderFactory: Audit not enabled..
20/01/29 02:45:38 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler failed in state INITED
java.lang.IllegalArgumentException: bound must be positive
	at java.base/java.util.Random.nextInt(Random.java:388)
	at org.apache.ranger.plugin.util.RangerRESTClient.<init>(RangerRESTClient.java:124)
	at org.apache.ranger.admin.client.RangerAdminRESTClient.init(RangerAdminRESTClient.java:765)
	at org.apache.ranger.admin.client.RangerAdminRESTClient.init(RangerAdminRESTClient.java:110)
	at org.apache.ranger.plugin.service.RangerBasePlugin.createAdminClient(RangerBasePlugin.java:584)
	at org.apache.ranger.plugin.util.PolicyRefresher.<init>(PolicyRefresher.java:95)
	at org.apache.ranger.plugin.service.RangerBasePlugin.init(RangerBasePlugin.java:176)
	at org.apache.ranger.authorization.yarn.authorizer.RangerYarnPlugin.init(RangerYarnAuthorizer.java:251)
	at org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer.init(RangerYarnAuthorizer.java:82)
	at org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer.init(RangerYarnAuthorizer.java:94)
	at org.apache.hadoop.yarn.security.YarnAuthorizationProvider.getInstance(YarnAuthorizationProvider.java:57)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.initScheduler(FairScheduler.java:1375)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.serviceInit(FairScheduler.java:1484)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter.convert(FSConfigToCSConfigConverter.java:209)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter.convert(FSConfigToCSConfigConverter.java:102)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler.parseAndConvert(FSConfigToCSConfigArgumentHandler.java:137)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterMain.main(FSConfigToCSConfigConverterMain.java:40)
[...]

 
 FS-CS converter: handle allow-undeclared-pools and user-as-default-queue properly and fix misc issues",yes
13253674,"Found intermittent failure of TestRMAppTransitions#testAppFinishedFinished in YARN-9664 jenkins report, the cause is that the assertion which will make sure dispatcher has handled APP_COMPLETED event but not wait, we need to add rmDispatcher.await() before that assertion like others in this class to fix this issue. 
 TestRMAppTransitions#testAppFinishedFinished fails intermittently",13232475,"Failed
org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppFinishedFinished[0]


Error Message
expected:<1> but was:<0>
Stacktrace
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.verifyAppCompletedEvent(TestRMAppTransitions.java:1307)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.verifyAppAfterFinishEvent(TestRMAppTransitions.java:1302)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testCreateAppFinished(TestRMAppTransitions.java:648)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions.testAppFinishedFinished(TestRMAppTransitions.java:1083)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


 
 TestRMAppTransitions fails intermittently",yes
13173993,"If a custom  uesr is used to run hive server 2, then repl subsystem should use the directories within the user's home directory for various configurations rather than use the default /user/hive/
 
 configure repl configuration directories based on user running hiveserver2",13283349,"See this discussion thread:
https://lists.apache.org/thread.html/abd60a8985a4898bae03b2c3c51d43a6b83d67c00caff82ba9ab2712%40%3Cdev.hbase.apache.org%3E 
 Remove hbase-prototcol module",No
13194000,"Came across this error when building hive using ""mvn clean install -DskipTests""



[INFO] Building tar: /Users/wei/apache/hive/standalone-metastore/target/apache-hive-standalone-metastore-4.0.0-SNAPSHOT-src.tar.gz
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Hive Storage API 2.7.0-SNAPSHOT .................... SUCCESS [ 5.656 s]
[INFO] Hive 4.0.0-SNAPSHOT ................................ SUCCESS [ 0.779 s]
[INFO] Hive Classifications ............................... SUCCESS [ 0.908 s]
[INFO] Hive Shims Common .................................. SUCCESS [ 3.217 s]
[INFO] Hive Shims 0.23 .................................... SUCCESS [ 7.102 s]
[INFO] Hive Shims Scheduler ............................... SUCCESS [ 2.069 s]
[INFO] Hive Shims ......................................... SUCCESS [ 1.905 s]
[INFO] Hive Common ........................................ SUCCESS [ 8.185 s]
[INFO] Hive Service RPC ................................... SUCCESS [ 3.603 s]
[INFO] Hive Serde ......................................... SUCCESS [ 7.438 s]
[INFO] Hive Standalone Metastore .......................... FAILURE [ 0.576 s]
[INFO] Hive Standalone Metastore Common Code .............. SKIPPED
[INFO] Hive Metastore ..................................... SKIPPED
[INFO] Hive Vector-Code-Gen Utilities ..................... SKIPPED
[INFO] Hive Llap Common ................................... SKIPPED
[INFO] Hive Llap Client ................................... SKIPPED
[INFO] Hive Llap Tez ...................................... SKIPPED
[INFO] Hive Spark Remote Client ........................... SKIPPED
[INFO] Hive Metastore Server .............................. SKIPPED
[INFO] Hive Query Language ................................ SKIPPED
[INFO] Hive Llap Server ................................... SKIPPED
[INFO] Hive Service ....................................... SKIPPED
[INFO] Hive Accumulo Handler .............................. SKIPPED
[INFO] Hive JDBC .......................................... SKIPPED
[INFO] Hive Beeline ....................................... SKIPPED
[INFO] Hive CLI ........................................... SKIPPED
[INFO] Hive Contrib ....................................... SKIPPED
[INFO] Hive Druid Handler ................................. SKIPPED
[INFO] Hive HBase Handler ................................. SKIPPED
[INFO] Hive JDBC Handler .................................. SKIPPED
[INFO] Hive HCatalog ...................................... SKIPPED
[INFO] Hive HCatalog Core ................................. SKIPPED
[INFO] Hive HCatalog Pig Adapter .......................... SKIPPED
[INFO] Hive HCatalog Server Extensions .................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client .................. SKIPPED
[INFO] Hive HCatalog Webhcat .............................. SKIPPED
[INFO] Hive HCatalog Streaming ............................ SKIPPED
[INFO] Hive HPL/SQL ....................................... SKIPPED
[INFO] Hive Streaming ..................................... SKIPPED
[INFO] Hive Llap External Client .......................... SKIPPED
[INFO] Hive Shims Aggregator .............................. SKIPPED
[INFO] Hive Kryo Registrator .............................. SKIPPED
[INFO] Hive TestUtils ..................................... SKIPPED
[INFO] Hive Kafka Storage Handler ......................... SKIPPED
[INFO] Hive Packaging ..................................... SKIPPED
[INFO] Hive Metastore Tools ............................... SKIPPED
[INFO] Hive Metastore Tools common libraries .............. SKIPPED
[INFO] Hive metastore benchmarks .......................... SKIPPED
[INFO] Hive Upgrade Acid .................................. SKIPPED
[INFO] Hive Pre Upgrade Acid 4.0.0-SNAPSHOT ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 42.026 s
[INFO] Finished at: 2018-10-24T15:34:40-07:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single (assemble) on project hive-standalone-metastore: Execution assemble of goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single failed: group id '74715970' is too big ( > 2097151 ). Use STAR or POSIX extensions to overcome this limit -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR] mvn <goals> -rf :hive-standalone-metastore


 
 Use ""posix"" for property tarLongFileMode for maven-assembly-plugin",13193184,"When executing


mvn clean install -DskipTests


Build Failed:


[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Hive Storage API 2.7.0-SNAPSHOT .................... SUCCESS [  5.299 s]
[INFO] Hive 4.0.0-SNAPSHOT ................................ SUCCESS [  0.750 s]
[INFO] Hive Classifications ............................... SUCCESS [  1.057 s]
[INFO] Hive Shims Common .................................. SUCCESS [  3.882 s]
[INFO] Hive Shims 0.23 .................................... SUCCESS [  5.020 s]
[INFO] Hive Shims Scheduler ............................... SUCCESS [  2.587 s]
[INFO] Hive Shims ......................................... SUCCESS [  2.038 s]
[INFO] Hive Common ........................................ SUCCESS [  6.921 s]
[INFO] Hive Service RPC ................................... SUCCESS [  3.503 s]
[INFO] Hive Serde ......................................... SUCCESS [  6.322 s]
[INFO] Hive Standalone Metastore .......................... FAILURE [  0.557 s]
[INFO] Hive Standalone Metastore Common Code .............. SKIPPED
[INFO] Hive Metastore ..................................... SKIPPED
[INFO] Hive Vector-Code-Gen Utilities ..................... SKIPPED
[INFO] Hive Llap Common ................................... SKIPPED
[INFO] Hive Llap Client ................................... SKIPPED
[INFO] Hive Llap Tez ...................................... SKIPPED
[INFO] Hive Spark Remote Client ........................... SKIPPED
[INFO] Hive Metastore Server .............................. SKIPPED
[INFO] Hive Query Language ................................ SKIPPED
[INFO] Hive Llap Server ................................... SKIPPED
[INFO] Hive Service ....................................... SKIPPED
[INFO] Hive Accumulo Handler .............................. SKIPPED
[INFO] Hive JDBC .......................................... SKIPPED
[INFO] Hive Beeline ....................................... SKIPPED
[INFO] Hive CLI ........................................... SKIPPED
[INFO] Hive Contrib ....................................... SKIPPED
[INFO] Hive Druid Handler ................................. SKIPPED
[INFO] Hive HBase Handler ................................. SKIPPED
[INFO] Hive JDBC Handler .................................. SKIPPED
[INFO] Hive HCatalog ...................................... SKIPPED
[INFO] Hive HCatalog Core ................................. SKIPPED
[INFO] Hive HCatalog Pig Adapter .......................... SKIPPED
[INFO] Hive HCatalog Server Extensions .................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client .................. SKIPPED
[INFO] Hive HCatalog Webhcat .............................. SKIPPED
[INFO] Hive HCatalog Streaming ............................ SKIPPED
[INFO] Hive HPL/SQL ....................................... SKIPPED
[INFO] Hive Streaming ..................................... SKIPPED
[INFO] Hive Llap External Client .......................... SKIPPED
[INFO] Hive Shims Aggregator .............................. SKIPPED
[INFO] Hive Kryo Registrator .............................. SKIPPED
[INFO] Hive TestUtils ..................................... SKIPPED
[INFO] Hive Kafka Storage Handler ......................... SKIPPED
[INFO] Hive Packaging ..................................... SKIPPED
[INFO] Hive Metastore Tools ............................... SKIPPED
[INFO] Hive Metastore Tools common libraries .............. SKIPPED
[INFO] Hive metastore benchmarks .......................... SKIPPED
[INFO] Hive Upgrade Acid .................................. SKIPPED
[INFO] Hive Pre Upgrade Acid 4.0.0-SNAPSHOT ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 38.602 s
[INFO] Finished at: 2018-10-22T15:14:24+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single (assemble) on project hive-standalone-metastore: Execution assemble of goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single failed: group id '25950964' is too big ( > 2097151 ). Use STAR or POSIX extensions to overcome this limit -> [Help 1]


I have tried to add tarLongFileMode=posix in standalone-metastore/pom.xml, but not success. What I added as below.


 <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-assembly-plugin</artifactId>
        <version>${maven.assembly.plugin.version}</version>
        <configuration>
            <tarLongFileMode>posix</tarLongFileMode>
        </configuration>
</plugin>


 
 Maven Build Failed with group id is too big ",yes
13137348,"See https://builds.apache.org/job/HBase%20Nightly/job/branch-2/284/testReport/junit/org.apache.hadoop.hbase.client/TestAsyncRegionAdminApi/testMergeRegions_0_/

java.lang.AssertionError: expected:<2> but was:<3> at org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi.testMergeRegions(TestAsyncRegionAdminApi.java:359)

Merge regions not work. The table still have 3 regions after the MergeRegionsProcedure finished.
The master start balance region 9e2773ba1efba79a2defa276e9a26ed4. But because the MergeRegionsProcedure pid=138 start work first, so the balance need wait for the lock. But after merge regions finished, the MoveRegionProcedure pid=139 start work and assign 9e2773ba1efba79a2defa276e9a26ed4 to a new region server. This is not right. The MoveRegionProcedure should skip to assign a region which was marked as offline. Or we should clear the merged regions' procedure whenMergeRegionsProcedure finished.

Logs:
2018-02-08 16:24:44,608 INFO [master/cd4730e3eae2:0.Chore.1] master.HMaster(1454): balance hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318
2018-02-08 16:24:44,608 DEBUG [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=37885] procedure2.ProcedureExecutor(868): Stored pid=138, state=RUNNABLE:MERGE_TABLE_REGIONS_PREPARE; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false
......
2018-02-08 16:24:50,111 INFO [PEWorker-13] procedure2.ProcedureExecutor(1249): Finished pid=138, state=SUCCESS; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false in 5.5710sec
2018-02-08 16:24:50,113 INFO [PEWorker-13] procedure.MasterProcedureScheduler(813): pid=139, state=RUNNABLE:MOVE_REGION_UNASSIGN; MoveRegionProcedure hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318 testMergeRegions testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4.
 
 Fix flaky TestAsyncRegionAdminApi",13169138,"Druid expressions do not support booleans yet. 
In druid expressions booleans are treated and parsed from longs, however when we store booleans from hive they are serialized as 'true' and 'false' string values. 
Need to make serialization consistent with deserialization and write long values when sending data to druid.  
 write booleans as long when serializing to druid",No
13308151,"if the job is waiting for a long to acquire the lock for a long time; it would be favourable to do a quick check after lock acqusition wether there are new triggers
https://github.com/apache/hive/pull/948#discussion_r432093713 
 special_character_in_tabnames_1.q is unstable",13318434,"
move similar queries to unit tests into the parser module and keep only one in the q test.
use explain instead of executing the queries if possible since we are focusing on parser testing

 
 Simplify special_character_in_tabnames_1.q",yes
13156992,"HIVE-19277 changes alone are not sufficient to support CORS. CrossOriginFilter has to be added to jetty which will serve appropriate response for OPTIONS pre-flight request. 
 
 Support CORS for all HS2 web endpoints",13273421,"There are 33 public methods in Driver, some of them either don't belong there, or should not be public. 
 Reduce the number of public methods in Driver",No
13146975,"It is currently possible for a component instance to have READY status before the AM retrieves an IP for the container. We should make sure the IP has been retrieved before marking the instance as READY.
This default probe could also have an option to check for a DNS entry for the instance's hostname if a DNS address is provided. 
 Create default readiness check for service components",13135693,"Scenario
 1) Launch Hbase on docker app
 2) Validate yarn service status using cli


{""name"":""hbase-app-with-docker"",""id"":""application_1517516543573_0012"",""artifact"":{""id"":""hbase-centos"",""type"":""DOCKER""},""lifetime"":3519,""components"":[{""name"":""hbasemaster"",""dependencies"":[],""artifact"":{""id"":""hbase-centos"",""type"":""DOCKER""},""resource"":{""cpus"":1,""memory"":""2048""},""state"":""STABLE"",""configuration"":{""properties"":{""docker.network"":""hadoop""},""env"":{""HBASE_MASTER_OPTS"":""-Xmx2048m -Xms1024m"",""HBASE_LOG_DIR"":""<LOG_DIR>""},""files"":[{""type"":""XML"",""properties"":{""hbase.zookeeper.quorum"":""${CLUSTER_ZK_QUORUM}"",""zookeeper.znode.parent"":""${SERVICE_ZK_PATH}"",""hbase.rootdir"":""${SERVICE_HDFS_DIR}/hbase"",""hbase.master.hostname"":""hbasemaster-0.${SERVICE_NAME}.${USER}.${DOMAIN}"",""hbase.master.info.port"":""16010"",""hbase.cluster.distributed"":""true""},""dest_file"":""/etc/hbase/conf/hbase-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/core-site.xml"",""src_file"":""core-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/hdfs-site.xml"",""src_file"":""hdfs-site.xml""}]},""quicklinks"":[],""containers"":[{""id"":""container_e02_1517516543573_0012_01_000002"",""ip"":""10.0.0.9"",""hostname"":""hbasemaster-0.hbase-app-with-docker.hrt-qa.test.com"",""state"":""READY"",""launch_time"":1517533029963,""bare_host"":""xxx"",""component_name"":""hbasemaster-0""}],""launch_command"":""sleep 15; /usr/hdp/current/hbase-master/bin/hbase master start"",""number_of_containers"":1,""run_privileged_container"":false},{""name"":""regionserver"",""dependencies"":[""hbasemaster""],""artifact"":{""id"":""hbase-centos"",""type"":""DOCKER""},""resource"":{""cpus"":1,""memory"":""2048""},""state"":""STABLE"",""configuration"":{""properties"":{""docker.network"":""hadoop""},""env"":{""HBASE_REGIONSERVER_OPTS"":""-XX:CMSInitiatingOccupancyFraction=70 -Xmx2048m -Xms1024m"",""HBASE_LOG_DIR"":""<LOG_DIR>""},""files"":[{""type"":""XML"",""properties"":{""hbase.regionserver.hostname"":""${COMPONENT_INSTANCE_NAME}.${SERVICE_NAME}.${USER}.${DOMAIN}"",""hbase.zookeeper.quorum"":""${CLUSTER_ZK_QUORUM}"",""zookeeper.znode.parent"":""${SERVICE_ZK_PATH}"",""hbase.rootdir"":""${SERVICE_HDFS_DIR}/hbase"",""hbase.master.hostname"":""hbasemaster-0.${SERVICE_NAME}.${USER}.${DOMAIN}"",""hbase.master.info.port"":""16010"",""hbase.cluster.distributed"":""true""},""dest_file"":""/etc/hbase/conf/hbase-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/core-site.xml"",""src_file"":""core-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/hdfs-site.xml"",""src_file"":""hdfs-site.xml""}]},""quicklinks"":[],""containers"":[{""id"":""container_e02_1517516543573_0012_01_000005"",""state"":""READY"",""launch_time"":1517533059022,""bare_host"":""xxx"",""component_name"":""regionserver-0""}],""launch_command"":""sleep 15; /usr/hdp/current/hbase-regionserver/bin/hbase regionserver start"",""number_of_containers"":1,""run_privileged_container"":false},{""name"":""hbaseclient"",""dependencies"":[],""artifact"":{""id"":""hbase-centos"",""type"":""DOCKER""},""resource"":{""cpus"":1,""memory"":""1024""},""state"":""STABLE"",""configuration"":{""properties"":{""docker.network"":""hadoop""},""env"":{""HBASE_LOG_DIR"":""<LOG_DIR>""},""files"":[{""type"":""XML"",""properties"":{""hbase.zookeeper.quorum"":""${CLUSTER_ZK_QUORUM}"",""zookeeper.znode.parent"":""${SERVICE_ZK_PATH}"",""hbase.rootdir"":""${SERVICE_HDFS_DIR}/hbase"",""hbase.master.hostname"":""hbasemaster-0.${SERVICE_NAME}.${USER}.${DOMAIN}"",""hbase.master.info.port"":""16010"",""hbase.cluster.distributed"":""true""},""dest_file"":""/etc/hbase/conf/hbase-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/core-site.xml"",""src_file"":""core-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/hdfs-site.xml"",""src_file"":""hdfs-site.xml""}]},""quicklinks"":[],""containers"":[{""id"":""container_e02_1517516543573_0012_01_000003"",""ip"":""10.0.0.8"",""hostname"":""hbaseclient-0.hbase-app-with-docker.hrt-qa.test.com"",""state"":""READY"",""launch_time"":1517533029964,""bare_host"":""xxx"",""component_name"":""hbaseclient-0""}],""launch_command"":""sleep infinity"",""number_of_containers"":1,""run_privileged_container"":false}],""configuration"":{""properties"":{""docker.network"":""hadoop""},""env"":{""HBASE_LOG_DIR"":""<LOG_DIR>""},""files"":[{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/core-site.xml"",""src_file"":""core-site.xml""},{""type"":""TEMPLATE"",""properties"":{},""dest_file"":""/etc/hadoop/conf/hdfs-site.xml"",""src_file"":""hdfs-site.xml""},{""type"":""XML"",""properties"":{""hbase.cluster.distributed"":""true"",""hbase.zookeeper.quorum"":""${CLUSTER_ZK_QUORUM}"",""hbase.rootdir"":""${SERVICE_HDFS_DIR}/hbase"",""zookeeper.znode.parent"":""${SERVICE_ZK_PATH}"",""hbase.master.hostname"":""hbasemaster-0.${SERVICE_NAME}.${USER}.${DOMAIN}"",""hbase.master.info.port"":""16010""},""dest_file"":""/etc/hbase/conf/hbase-site.xml""}]},""state"":""STABLE"",""quicklinks"":{""HBase Master Status UI"":""http://hbasemaster-0.hbase-app-with-docker.hrt-qa.test.com:16010/master-status""},""kerberos_principal"":{}}

Here, The service and all components are in STABLE state. However, ""IP"" detail of Regionserver is missing.
 Expected behavior:
 IP detail should be present for in component before moving to STABLE state
 Note: Region server gets assigned to IP after some delay. 
 Docker container IP detail missing when service is in STABLE state",yes
13213619,"This is a missing part in our ITBLL. We can config a dummy replication endpoint which replicates nothing but only reads and verifies that all the wal files are readable and the entries are all fine.
With this I think it will be much easier to catch the problem in the parent issue. 
 Create a special ReplicationEndpoint just for verifying the WAL entries are fine",13301793,"Currently there are multiple ways to clean up ACID related stuff:

AcidHouseKeeperService
AcidWriteSetService
AcidCompactionHistoryService
Initiator
Cleaner
etc.

We should consolidate them where possible and improve logging. 
 Consolidate acid-related cleanup tasks",No
13264443,"Replace ""/"" with constant NodeBase.PATH_SEPARATOR_STR 
 Refactor DFSNetworkTopology#isNodeInScope",13168212,"YARN-4599 adds elastic memory control that disables oom killer for the root container cgroup. Hence, all containers have their oom killer disabled because they inherit the setting from the root container cgroup. Hence, when strict memory control on individual containers is also enabled, the container will be frozen but not killed. We can let the container monitoring thread to take care of the frozen containers. 
 Support strict memory control on individual container with elastic control memory mechanism",No
13313479,"It will stop and then start a mini cluster every time after each test method, so let's just split them into individual test files. 
 Duplicate issue",13225364,"It appears that MultiRowRangeFilter was never written to function with reverse scans. There is too much logic that operates with the assumption that we are always moving ""forward"" through increasing ranges. It needs to be rewritten to ""traverse"" forward or backward, given how the context of the scan being used. 
 windows hbase-env causes hbase cli/etc to ignore HBASE_OPTS",No
13283349,"See this discussion thread:
https://lists.apache.org/thread.html/abd60a8985a4898bae03b2c3c51d43a6b83d67c00caff82ba9ab2712%40%3Cdev.hbase.apache.org%3E 
 Remove hbase-prototcol module",13279500,"CVE-2019-20330 is reported and fixed in jackson-databind 2.9.10.2.
https://nvd.nist.gov/vuln/detail/CVE-2019-20330 
 Upgrade jackson-databind to 2.9.10.2",No
13251043,"TestAppLogAggregatorImpl.verifyFilesToDelete fails


[ERROR] testDFSQuotaExceeded(org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl)  Time elapsed: 2.252 s  <<< FAILURE!
java.lang.AssertionError: The set of paths for deletion are not the same as expected: actual size: 0 vs expected size: 1
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl.verifyFilesToDelete(TestAppLogAggregatorImpl.java:344)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl.access$000(TestAppLogAggregatorImpl.java:82)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl$1.answer(TestAppLogAggregatorImpl.java:330)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl$1.answer(TestAppLogAggregatorImpl.java:319)
	at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39)
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96)
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29)
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35)
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:61)
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49)
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.DeletionService$MockitoMock$1136724178.delete(Unknown Source)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregationPostCleanUp(AppLogAggregatorImpl.java:556)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:476)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl.testDFSQuotaExceeded(TestAppLogAggregatorImpl.java:469)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

 
 TestAppLogAggregatorImpl.verifyFilesToDelete fails",13251033,"TestAppLogAggregatorImpl#testDFSQuotaExceeded currently fails on trunk. It was most likely introduced by YARN-9676 (resetting HEAD to the previous commit and then re-running the test passes).

[INFO] Running org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.781 s <<< FAILURE! - in org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl
[ERROR] testDFSQuotaExceeded(org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl)  Time elapsed: 2.361 s  <<< FAILURE!
java.lang.AssertionError: The set of paths for deletion are not the same as expected: actual size: 0 vs expected size: 1
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl.verifyFilesToDelete(TestAppLogAggregatorImpl.java:344)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl.access$000(TestAppLogAggregatorImpl.java:82)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl$1.answer(TestAppLogAggregatorImpl.java:330)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl$1.answer(TestAppLogAggregatorImpl.java:319)
	at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39)
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96)
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29)
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35)
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:61)
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49)
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.DeletionService$MockitoMock$1879282050.delete(Unknown Source)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregationPostCleanUp(AppLogAggregatorImpl.java:556)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:476)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestAppLogAggregatorImpl.testDFSQuotaExceeded(TestAppLogAggregatorImpl.java:469)
...

 
 TestAppLogAggregatorImpl#testDFSQuotaExceeded fails on trunk",yes
13318623,"Currently, the way to list snapshots is do a ls on <snapshotttableRootPath>/.snapshot directory. Since creation time is not recorded , there is no way to actually figure out the chronological order of snapshots. The idea here is to add a command to list snapshots for a snapshottable directory along with snapshot Ids which grow monotonically as snapshots are created in the system. With snapID, it will be helpful to figure out the chronology of snapshots in the system. 
 Add a command to list all snapshots for a snaphottable root with snapshot Ids",13134096,"HBASE-18142 made an intentionally incompatible change to delete.rb
Most notably from a user, the help message is now inaccurate: only two arguments must be provided (table name and row) whereas it was required to also provide a column to delete.
Chun-Hao Tang, Chia-Ping Tsai, might one of you folks be able to submit a patch to fix this up (maybe get those release notes added to HBASE-18142 too? ) 
 delete.rb should require user to provide the column",No
13200424,"Requirements:
Hive2 supports replication of managed tables. But in Hive3 with hive.strict.managed.tables=true, some of these managed tables are converted to ACID or MM tables. Also, some of them are converted to external tables based on below rules. 

Avro format with external schema, Storage handlers, List bucketed tabled are converted to external tables.
Location not owned by ""hive"" user are converted to external table.
Hive owned ORC format are converted to full ACID transactional table.
Hive owned Non-ORC format are converted to MM transactional table.

REPL LOAD should apply these rules during bootstrap and incremental phases and convert the tables accordingly. 
 Support bootstrap and incremental replication to a target with hive.strict.managed.tables enabled.",13196883,"Hive2 supports replication of managed tables. But in Hive3, some of these managed tables are converted to ACID or MM tables. Also, some of them are converted to external tables based on below rules.

Avro format with external schema, Storage handlers, List bucketed tabled are converted to external tables.
Location not owned by ""hive"" user are converted to external table.
Hive owned ORC format are converted to full ACID transactional table.
Hive owned Non-ORC format are converted to MM transactional table.

REPL LOAD should apply these rules during bootstrap and convert the tables accordingly. 
 Bootstrap of tables to target with hive.strict.managed.tables enabled.",yes
13161096,"S3Guard updates its table on a getFileStatus call, but not on a directory listing.
While this makes directory listings faster (no need to push out an update), it slows down subsequent queries of the files, such as a sequence of:


statuses = s3a.listFiles(dir)
for (status: statuses) {
  if (status.isFile) {
      try(is = s3a.open(status.getPath())) {
        ... do something
      }
}


this is because the open() is doing the getFileStatus check, even after the listing.
Updating the DDB tables after a listing would give those reads a speedup, albeit at the expense of initiating a (bulk) update in the list call. Of course, we could consider making that async, though that design (essentially a write-buffer) would require the buffer to be checked in the reads too.  
 S3Guard to self update on directory listings of S3",13199015,"Minor edit to see difference if any made by INFRA-17217 change. 
 [hbase-connectors] Edit on spark connector README",No
13303535,"hive.server2.authentication.kerberos.principal set in the form of hive/_HOST@REALM,
Tez task can start at the random NM host and unfold the value of _HOST with the value of fqdn where it is running. this leads to an authentication issue.
for LLAP there is fallback for LLAP daemon keytab/principal, Kafka 1.1 onwards support delegation token and we should take advantage of it for hive on tez. 
 Release 2.2.5",13154975,"There is momentarily issue between app ACCEPTED to RUNNING duration. If AM launching is delayed, then state is not updated in ATS. 
 Application State is not updated to ATS if AM launching is delayed.",No
13164156,"org.apache.hadoop.hbase.client.Scan#setStopRow javadoc, in ""deprecated"" paragraph, points out to incorrect value. It uses pointer to withStartRow, but should use withStopRow 
 org.apache.hadoop.hbase.client.Scan#setStopRow javadoc uses incorrect method",13176609,"Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService. 
 VectorFileSinkArrowOperator",No
13130281,"For the following snapshot:


 snapshot_table_tav6mrq397                                  NS592088:table_tav6mrq397 (2018-01-10 16:20:39 UTC)


namespace NS592088 is dropped.
restore_snapshot on the snapshot gave the following :


hbase(main):003:0> restore_snapshot 'snapshot_table_tav6mrq397'

ERROR: Unknown namespace snapshot_table_tav6mrq397!


The namespace in the message was incorrect. 
 restore_snapshot shell command gives wrong namespace if the namespace doesn't exist",13216222,"Create new synonym for the existing function

Mid for substr
postiion for locate 
 Create Synonym mid for  substr, position for  locate",No
13252784,"we've been regularly getting 4-5 concurrent builds of PRs. 
 [Backport] HBASE-22867 to branch-1 to avoid ForkJoinPool to spawn thousands of threads",13153077,"Commons-httpclient is not supported well anymore.  Remove dependency and move to Apache HTTP client. 
 Remove commons-httpclient 3.x usage",No
13212599,"BackportHADOOP-15549 to branch-3.1 to fix IllegalArgumentException:

02:44:34.707 ERROR org.apache.hadoop.hive.ql.exec.Task: Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.
	at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:116)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:109)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:102)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:475)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:454)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:369)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:151)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:730)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:221)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:220)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:266)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:719)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:709)
	at org.apache.spark.sql.hive.StatisticsSuite.createNonPartitionedTable(StatisticsSuite.scala:719)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$2(StatisticsSuite.scala:822)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:284)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:283)
	at org.apache.spark.sql.StatisticsCollectionTestBase.withTable(StatisticsCollectionTestBase.scala:40)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1(StatisticsSuite.scala:821)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1$adapted(StatisticsSuite.scala:820)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.hive.StatisticsSuite.testAlterTableProperties(StatisticsSuite.scala:820)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$70(StatisticsSuite.scala:851)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:104)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:53)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:53)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1340)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1031)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	Suppressed: java.io.IOException: Failed to use org.apache.hadoop.mapred.LocalClientProtocolProvider due to error: 
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:148)
		... 78 more
	Caused by: org.apache.commons.configuration2.ex.ConfigurationRuntimeException: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:463)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:479)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:492)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResultInstance(BasicConfigurationBuilder.java:447)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResult(BasicConfigurationBuilder.java:417)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.getConfiguration(BasicConfigurationBuilder.java:285)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:119)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.create(MetricsConfig.java:98)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:478)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:188)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:163)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:62)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:58)
		at org.apache.hadoop.mapred.LocalJobRunnerMetrics.create(LocalJobRunnerMetrics.java:45)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:771)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:764)
		at org.apache.hadoop.mapred.LocalClientProtocolProvider.create(LocalClientProtocolProvider.java:42)
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:130)
		... 78 more
	Caused by: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2195)
		at org.apache.commons.beanutils.PropertyUtilsBean.setSimpleProperty(PropertyUtilsBean.java:2108)
		at org.apache.commons.beanutils.PropertyUtilsBean.setNestedProperty(PropertyUtilsBean.java:1914)
		at org.apache.commons.beanutils.PropertyUtilsBean.setProperty(PropertyUtilsBean.java:2021)
		at org.apache.commons.beanutils.BeanUtilsBean.setProperty(BeanUtilsBean.java:1018)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initProperty(BeanHelper.java:365)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBeanProperties(BeanHelper.java:273)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBean(BeanHelper.java:192)
		at org.apache.commons.configuration2.beanutils.BeanHelper$BeanCreationContextImpl.initBean(BeanHelper.java:669)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.initBeanInstance(DefaultBeanFactory.java:162)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.createBean(DefaultBeanFactory.java:116)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:459)
		... 95 more
	Caused by: java.lang.IllegalArgumentException: argument type mismatch
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2127)
		... 106 more


 
 Backport HADOOP-15549 to branch-3.1",13212603,"BackportHADOOP-15549 to branch-3.1 to fix IllegalArgumentException:

02:44:34.707 ERROR org.apache.hadoop.hive.ql.exec.Task: Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.
	at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:116)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:109)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:102)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:475)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:454)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:369)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:151)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:730)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:221)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:220)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:266)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:719)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:709)
	at org.apache.spark.sql.hive.StatisticsSuite.createNonPartitionedTable(StatisticsSuite.scala:719)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$2(StatisticsSuite.scala:822)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:284)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:283)
	at org.apache.spark.sql.StatisticsCollectionTestBase.withTable(StatisticsCollectionTestBase.scala:40)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1(StatisticsSuite.scala:821)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1$adapted(StatisticsSuite.scala:820)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.hive.StatisticsSuite.testAlterTableProperties(StatisticsSuite.scala:820)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$70(StatisticsSuite.scala:851)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:104)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:53)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:53)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1340)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1031)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	Suppressed: java.io.IOException: Failed to use org.apache.hadoop.mapred.LocalClientProtocolProvider due to error: 
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:148)
		... 78 more
	Caused by: org.apache.commons.configuration2.ex.ConfigurationRuntimeException: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:463)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:479)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:492)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResultInstance(BasicConfigurationBuilder.java:447)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResult(BasicConfigurationBuilder.java:417)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.getConfiguration(BasicConfigurationBuilder.java:285)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:119)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.create(MetricsConfig.java:98)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:478)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:188)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:163)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:62)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:58)
		at org.apache.hadoop.mapred.LocalJobRunnerMetrics.create(LocalJobRunnerMetrics.java:45)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:771)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:764)
		at org.apache.hadoop.mapred.LocalClientProtocolProvider.create(LocalClientProtocolProvider.java:42)
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:130)
		... 78 more
	Caused by: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2195)
		at org.apache.commons.beanutils.PropertyUtilsBean.setSimpleProperty(PropertyUtilsBean.java:2108)
		at org.apache.commons.beanutils.PropertyUtilsBean.setNestedProperty(PropertyUtilsBean.java:1914)
		at org.apache.commons.beanutils.PropertyUtilsBean.setProperty(PropertyUtilsBean.java:2021)
		at org.apache.commons.beanutils.BeanUtilsBean.setProperty(BeanUtilsBean.java:1018)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initProperty(BeanHelper.java:365)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBeanProperties(BeanHelper.java:273)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBean(BeanHelper.java:192)
		at org.apache.commons.configuration2.beanutils.BeanHelper$BeanCreationContextImpl.initBean(BeanHelper.java:669)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.initBeanInstance(DefaultBeanFactory.java:162)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.createBean(DefaultBeanFactory.java:116)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:459)
		... 95 more
	Caused by: java.lang.IllegalArgumentException: argument type mismatch
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2127)
		... 106 more


 
 Backport HADOOP-15549 to branch-3.0",yes
13168436,"We don't log at INFO level the arrival on the Master of a create table request. Need to log for basic audit purposes the initiator and the pid created to run the create table.
Review other macro ops to make sure they log at INFO level. 
 Move precommit jobs to jdk 8",13168160,"

+ mvn clean package -B -DskipTests -Drat.numUnapprovedLicenses=1000 -Dmaven.repo.local=/home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/.m2/repository
[INFO] Scanning for projects...
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Building hive-ptest 3.0
[INFO] ------------------------------------------------------------------------
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-clean-plugin/2.5/maven-clean-plugin-2.5.pom
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 0.925 s
[INFO] Finished at: 2018-06-25T20:46:27Z
[INFO] Final Memory: 24M/1447M
[INFO] ------------------------------------------------------------------------
[ERROR] Plugin org.apache.maven.plugins:maven-clean-plugin:2.5 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-clean-plugin:jar:2.5: Could not transfer artifact org.apache.maven.plugins:maven-clean-plugin:pom:2.5 from/to central (https://repo.maven.apache.org/maven2): Received fatal alert: protocol_version -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginResolutionException
+ return 1
+ ret=1
+ unpack_test_results
+ '[' -z /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build ']'
+ cd /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target
jenkins-execute-build.sh: line 61: cd: /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target: No such file or directory
+ [[ -f test-results.tar.gz ]]
+ exit 1
+ rm -f /tmp/tmp.LFKzzyYwIt
Build step 'Execute shell' marked build as failure
Recording test results
ERROR: Step ?Publish JUnit test result report? failed: No test report files were found. Configuration error?
[description-setter] Description set: HIVE-19980  /   master-mr2
Finished: FAILURE

 
 Precommit jobs erroring out",yes
13238101,"Hi,
Is there any plan to update Guava library?
Hadoop using very old version 11.0.2.
Current new Version : 27.1-jre


 
 Guava library upgrade",13201362,"com.google.guava:guava should be upgraded to 27.0-jre due to new CVE's found CVE-2018-10237. 
 Update guava to 27.0-jre in hadoop-project",yes
13311956,"Listing api returns a Remote iterator. Currently it is blocking in the constructor Listing class. Also it is done in batches, so if a batch is exhausted , the next batch will be fetched from s3
thus causing small pauses on the client side every time a batch is fetched. We can add the following optimisations to fix these:

Make the constructor list call to s3 asynchronous.
Fetch the next batch asynchronously while the current batch is getting processed.

 
 Optimise s3a Listing to be fully asynchronous.",13129378,"DataNode Replica Trash will allow administrators to recover from a recent delete request that resulted in catastrophic loss of user data. This is achieved by placing all invalidated blocks in a replica trash on the datanode before completely purging them from the system. The design doc is attached here. 
 DataNode Replica Trash",No
13136865,"I'm using latest master branch code and mysql as metastore.
Creating table hits this error:

2018-02-07T22:04:55,438 ERROR [41f91bf4-bc49-4a73-baee-e2a1d79b8a4e main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MTable@28d16af8"" using statement ""INSERT INTO `TBLS` (`TBL_ID`,`CREATE_TIME`,`CREATION_METADATA_MV_CREATION_METADATA_ID_OID`,`DB_ID`,`LAST_ACCESS_TIME`,`OWNER`,`RETENTION`,`IS_REWRITE_ENABLED`,`SD_ID`,`TBL_NAME`,`TBL_TYPE`,`VIEW_EXPANDED_TEXT`,`VIEW_ORIGINAL_TEXT`) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)"" failed : Unknown column 'CREATION_METADATA_MV_CREATION_METADATA_ID_OID' in 'field list'
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)
        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:729)
        at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:749)
        at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:1125)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
        at com.sun.proxy.$Proxy36.createTable(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1506)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1412)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1614)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 
 Cannot create table: ""message:Exception thrown when executing query : SELECT DISTINCT..""",13136658,"These tests started failing against hadoop 3 after HBASE-19841 
 Remove MCreationMetadata from MTable class",yes
13316116,"The saveVersion.sh file does not parse thesrcChecksum correctly during the releasing process when dev-support/create-release/hbase-rm/Dockerfile is used. This results in missingSource Checksum.
Master UI displays this:



HBase Source Checksum
(stdin)=



 
 Incorrect checksum calculation in saveVersion.sh",13315716,"The web ui has a ""Software Attribute"" section with a table entry called ""HBase Source Checksum"", supposed to be a sha512 checksum according to the description. Instead, the value is just ""(stdin)="". 
 Source checksum is missing from web ui",yes
13289579,"The parent issue upped the parallelism of the flakey reruns – see the second panel on this page https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2/lastSuccessfulBuild/artifact/dashboard.html This upped the flakie list length. It also may have overcommitted the test-running machine. Let me down the rate to something more mild. 
 [Flakey Tests] Down the flakies re-run ferocity; it makes for too many fails.",13160843,"There is hadoop version mismatch between hive and storage-api and hence different transitive dependency versions gets pulled. 
 sync up hadoop version used by storage-api with hive",No
13250189,"
2019-08-11 09:15:58,092 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Can't replicate block BP-725378529-10.0.0.8-1410027444173:blk_13276745777_1112363330268 because on-disk length 175085 is shorter than NameNode recorded length 9223372036854775807.

If the block length is Long.MAX_VALUE, means file belongs to this block is deleted from the namenode and DN got the command after deletion of file. In this case command should be ignored. 
 DataNode shouldn't report block as bad block if the block length is Long.MAX_VALUE.",13256922,"Was unable to fetch status for a STOPPED app due to the below error in RM logs.


ERROR webapp.ApiServer (ApiServer.java:getService(213)) - Get service failed: {}
java.io.EOFException: Read of hdfs://my-cluster:8020/user/appuser/.yarn/services/my-service/my-service.json finished prematurely
        at org.apache.hadoop.yarn.service.utils.JsonSerDeser.load(JsonSerDeser.java:188)
        at org.apache.hadoop.yarn.service.utils.ServiceApiUtil.loadService(ServiceApiUtil.java:360)
        at org.apache.hadoop.yarn.service.client.ServiceClient.getAppId(ServiceClient.java:1409)
        at org.apache.hadoop.yarn.service.client.ServiceClient.getStatus(ServiceClient.java:1235)
        at org.apache.hadoop.yarn.service.webapp.ApiServer.lambda$getServiceFromClient$3(ApiServer.java:749)


This seems to happen when the json file my-service.json is larger than 128KB in my cluster. 
 YARN Service fails to fetch status for Stopped apps with bigger spec files",No
13248391,"Email from Oskars Vegeris:

During internal infrastructure testing it was discovered that the Hadoop Yarn REST endpoint /app/v1/services contains a command injection vulnerability.

The services endpoint's normal use-case is for launching containers (e.g. Docker images/apps), however by providing an argument with special shell characters it is possible to execute arbitrary commands on the Host server - this would allow to escalate privileges and access.

The command injection is possible in the parameter for JVM options - ""yarn.service.am.java.opts"". It's possible to enter arbitrary shell commands by using sub-shell syntax `cmd` or $(cmd). No shell character filtering is performed.

The ""launch_command"" which needs to be provided is meant for the container and if it's not being run in privileged mode or with special options, host OS should not be accessible.

I've attached a minimal request sample with an injected 'ping' command. The endpoint can also be found via UI @http://yarn-resource-manager:8088/ui2/#/yarn-services

If no auth, or ""simple auth"" (username) is enabled, commands can be executed on the host OS. I know commands can also be ran by the ""new-application"" feature, however this is clearly not meant to be a way to touch the host OS. 
 Yarn REST API, services endpoint remote command ejection",13130654,"Appy's dashboard for 2.0 https://builds.apache.org/job/HBase-Find-Flaky-Tests-branch2.0/lastSuccessfulBuild/artifact/dashboard.html and a GCE run last night showed how bad of a state our unit tests are in on tip of branch-2. This issue does triage fixing and disabling so I can push out the beta-1. Its done as a subtask of HBASE-19694 because a few of the tests were broke by this commit. 
 Fix or disable tests broken in branch-2 so can cut beta-1",No
13146665,"ALTER TABLE ADD CONSTRAINT should be able to add CHECK constraint (table level) 
 ALTER TABLE ADD CONSTRAINT support for CHECK constraint",13172994,"In https://issues.apache.org/jira/browse/HDFS-13723,increasing the amount of time in sleep() helps but the problem still appears, which is annoying.

Solution:
Use a loop to allow the test case to failmaxTrials times before declaring failure. Wait 50 ms between each retry.
 
 Still occasional ""Should be different group"" failure in TestRefreshUserMappings#testGroupMappingRefresh",No
13170591,"port HBASE-20695 to branch-1 
 port HBASE-20695 to branch-1",13315903,"LLAP IO supports caching but currently this is only done via LlapRecordReader / using splits, aka good old mapreduce way.
At certain times it would worth to leverage the caching of files on certain paths, that are not necessarily associated with a record reader directly. An example of this could be the caching of ACID delete delta files, as they are currently being read without caching.
With this patch we'd extend the LLAP API and offer another entry point for retrieving metadata of ORC files. 
 LLAP - add API to look up ORC metadata for certain Path",No
13191611,"Miscellaneous additions to hbck2 over in the hbase-operator-tools project.

Emit version
Fail if going against an hbase that doesn't support hbck2.
Add override to assigns and unassigns
Have bypass match changing its force option to be override
Add recursive to bypass so can pass a parent and it will go find all child procedures.

 
 [hbck2] Add version, version handling, and misc override to assigns/unassigns",13318439,"HIVE-21784 uses a new WriterOptions instead of the field in OrcRecordUpdater:
https://github.com/apache/hive/commit/f62379ba279f41b843fcd5f3d4a107b6fcd04dec#diff-bb969e858664d98848960a801fd58b5cR580-R583
so in this scenario, the overwrite creates an empty bucket file, which is fine as that was the intention of that patch, but it creates that with invalid schema:


CREATE TABLE test_table (
   cda_id             int,
   cda_run_id         varchar(255),
   cda_load_ts        timestamp,
   global_party_id    string)
PARTITIONED BY (
   cda_date           int,
   cda_job_name       varchar(12))
CLUSTERED BY (cda_id) 
INTO 2 BUCKETS
STORED AS ORC;


INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id
UNION ALL
SELECT 2 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id;

ALTER TABLE test_table ADD COLUMNS (group_id string) CASCADE ;

INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id, 'group_id' as group_id;


because of HIVE-21784, the new empty bucket_00000 shows this schema in orc dump:


Type: struct<_col0:int,_col1:varchar(255),_col2:timestamp,_col3:string,_col4:string>


instead of:


Type: struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<cda_id:int,cda_run_id:varchar(255),cda_load_ts:timestamp,global_party_id:string,group_id:string>>


and this could lead to problems later, when hive tries to look into the file during split generation 
 Empty bucket files are inserted with invalid schema after HIVE-21784",No
13273576,"Distcp over S3A always copies all source files no matter the files are changed or not. This is opposite to the statement in the doc below.
http://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html

And to use -update to only copy changed files.


CopyMapper compares file length as well as block size before copying. While the file length should match, the block size does not. This is apparently because the returned block size from S3A is always 32MB.
https://github.com/apache/hadoop/blob/release-3.2.0-RC1/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L348
I'd suppose we should update the documentation or make code change. 
 distcp -update to S3A; abfs, etc always overwrites due to block size mismatch",13326822,"We usedistcp with -update option to copy a dir from hdfs to S3. When we run distcp jobonce more, it will overwrite S3 dir directly, rather than skip the same files.
 
 Test Case:
Run twice the following cmd, the modify time of S3 files will be modified every time.
 hadoop distcp -update/test/ s3a://${s3_buckect}/test/

Check code in CopyMapper.java andS3AFileSystem.java
(1) For the first time, distcp job will create files in S3, but blockSize isunused!


(2)For the second time, the distcp job will compare fileSize and blockSize between hdfs file and S3 file


(3) blockSize is unused, when get blockSize of S3 file, it return a default value.
In S3AFileSystem.java, we find that the default value offs.s3a.block.size is 32 * 1024 * 1024.



 
The blockSize of HDFS seems invalid in Object Store, like S3. So I think there's no need to compare blockSize when distcp with -update option. 
 DistCp -update option will be invalid when distcp files from hdfs to S3",yes
13191840,"Once weget pluggable device framework mature, we can port existingGPU related code into this new framework. 
 Documentation of the pluggable device framework",13339929,"Ranger Replication fallback to updateIfExists
Add dummy resource as workaround while creating the deny policy to avoid it from overriding the actual policy 
 Ranger Replication fallback to updateIfExists",No
13151016,"Hi,
I compiled a draft document for the HBase incompatibilities from the raw source content that was available in HBase Beta 1 site. Can someone please review and provide a feedbackorshare your comments on this document?
Appreciate your support and time.

Best Regards,
Triguna 
 Document incompatibilities between HBase 1.x and HBase 2.0",13155987,"This could be limited to assert exceptions; but might interfere with other exceptions...discovered while ""fixing"" testreopt after HIVE-19269


create table tu(id_uv int,id_uw int,u int);
create table tv(id_uv int,v int);
create table tw(id_uw int,w int);

insert into tu values (10,10,10),(1,1,1),(2,2,2),(3,3,3),(4,4,4),(5,5,5),(6,6,6);
insert into tv values (10,10),(1,1),(2,2),(3,3);
insert into tw values (10,10),(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9);

set zzz=0;
set hive.vectorized.execution.enabled=false;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- fails as expected

set hive.vectorized.execution.enabled=true;
select assert_true(${hiveconf:zzz}>sum(1)) from tu join tv on (tu.id_uv=tv.id_uv) where u<10 and v>1;
-- there is a result set

 
 Vectorization: assert_true HiveException erroneously gets suppressed to NULL",No
13162375,"Hadoop package build fails with java doc error


[ERROR] /opt/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceCalculator.java:263: error: bad use of '>'
[ERROR]    * included) has a >0 value.
[ERROR]                      ^
[ERROR] /opt/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceCalculator.java:266: error: bad use of '>'

 
 Javadoc error in ResourceCalculator",13162203,"
$ mvn javadoc:javadoc --projects hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common
...
[ERROR] /hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceCalculator.java:263: error: bad use of '>'
[ERROR]    * included) has a >0 value.
[ERROR]                      ^
[ERROR] /hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceCalculator.java:266: error: bad use of '>'
[ERROR]    * @return returns true if any resource is >0
[ERROR]                                              ^

 
 Javadoc build failed due to ""bad use of '>'""",yes
13230476,"Fails intermittently with diff:


Client Execution succeeded but contained differences (error code = 1) after executing cbo_rp_limit.q 
11c11
<  1  4 2
---
>  1 4 2

 
 Use special pause for CallQueueTooBigException",13209008,"When running org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore integration test, I got the following stack trace:


[ERROR] org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore  Time elapsed: 0.333 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3a.s3guard.ITestDynamoDBMetadataStore.beforeClassSetup(ITestDynamoDBMetadataStore.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


The NPE happened here:


assertTrue(""Test DynamoDB table name: '""
  + S3GUARD_DDB_TEST_TABLE_NAME_KEY + ""' and production table name: '""
  + S3GUARD_DDB_TABLE_NAME_KEY + ""' can not be the same."",
  !conf.get(S3GUARD_DDB_TABLE_NAME_KEY).equals(testDynamoDBTableName));


The problem is that though we check previously whether the variable testDynamoDBTableName (fs.s3a.s3guard.ddb.test.table config) is not null, but we don't do the same for fs.s3a.s3guard.ddb.table (S3GUARD_DDB_TABLE_NAME_KEY) before calling the .equals(), thus causing an NPE.
Since we don't need the fs.s3a.s3guard.ddb.table config for the test, we should check first whether that config is given or not, and only comparing the two configs if they both exist. 
 NPE in ITestDynamoDBMetadataStore when fs.s3a.s3guard.ddb.table is not set",No
13347124,"Vulnerability fixes needed for Jetty hadoop dependency library
The jetty jars where CVEs are found are ,
================ =====
Jetty[version 9.4.20.v20190813 ]
jetty-server-9.4.20.v20190813.jar
CVE details :- [ CVE-2020-27216 ]
================ =====
Jetty-http[version 9.4.20.v20190813 ]
jetty-http-9.4.20.v20190813.jar
CVE details :- [ CVE-2020-27216 ] 
 Update Jetty hadoop dependency",13339673,"The Hadoop 3 branches are on 9.4.20. We should update to the latest version: 9.4.34 
 Bump Jetty to the latest version 9.4.35",yes
13287659,"Use user test which belong to group hadoop to run distcp with-pbugpaxt , cp a directory which owner is super, distcp runs failed error log as follows:
2020-02-26 11:17:02,755 INFO [IPC Server handler 5 on 27101] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1582635453769_0003_m_000021_0: Error: org.apache.hadoop.security.AccessControlException: User super is not a super user (non-super user cannot change owner).
 at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:85)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1927)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:870)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:566)
 at
.......
Currnet user is test, not super, the log print wrong user info
 
 ERROR log print wrong user info when run distcp",13162319,"when do the chown operation on target file /tmp/test with user 'root' to user 'hive', the log displays 'User hive is not a super user' ;Thisappropriate log here should be 'User root is not a super user'


[root@lhccmh1 ~]# hdfs dfs -ls /tmp/test
rw-rr- 3 root hdfs 0 2018-05-28 10:33 /tmp/test
[root@lhccmh1 ~]# hdfs dfs -chown hive /tmp/test
chown: changing ownership of '/tmp/test': User hive is not a super user (non-super user cannot change owner).

The last version patch of issue HDFS-10455use username but not pc.getUser() in logs;



       if (!pc.isSuperUser()) {
         if (username != null && !pc.getUser().equals(username)) {
-          throw new AccessControlException(""Non-super user cannot change owner"");
+          throw new AccessControlException(""User "" + username
+              + "" is not a super user (non-super user cannot change owner)."");
         }
         if (group != null && !pc.isMemberOfGroup(group)) {
-          throw new AccessControlException(""User does not belong to "" + group);
+          throw new AccessControlException(
+              ""User "" + username + "" does not belong to "" + group);
         }
       }

 
 Fix incorrect username when deny the setOwner operation",yes
13251506,"When use user_permission command to show global permissions, the following exception occurred:


hbase(main):001:0> user_permission
User Namespace,Table,Family,Qualifier:Permission

ERROR: failed to coerce org.apache.hadoop.hbase.security.access.GlobalPermission to org.apache.hadoop.hbase.security.access.TablePermission

For usage try 'help ""user_permission""'

Took 1.1249 seconds
hbase(main):002:0>

 
 [Backport] HBASE-22871 to branch-1",13251370,"The current implementation is a bit ugly, the onConfigurationChange is an instance method but it writes to a static field and cause a findbugs warning. I think we can just put the DirScanPool in HMaster and pass it when constructing Cleaners... 
 Move the DirScanPool out and do not use static field",yes
13312685,"On master branch we have AsyncMetaTableAccessor which is used at client side and MetaTableAccessor has lots of internal methods for implementing assignment, which is not part of our client code.
So let's move it to hbase-server, and in the future, maybe in hbase-balancer? 
 Move MetaTableAccessor out of hbase-client",13312683,"Move some common methods for accessing root/meta to this util class.
Notice that, root table is stored in a master local region, so the access API is a bit different comparing to a table.
Maybe we could make use of the RegionAsTable class under the test code to unify the API? 
 Introduce a CatalogAccessor",yes
13150397,"All the metastore metadata tables related to Acid use dbname/table name.  There is no logic to update these tables when table is renamed. 
 Transactional tables can't handle 'rename' table",13139601,"ACID implementation uses metatables such as TXN_COMPONENTS, COMPLETED_TXN_COMPONENTS, COMPACTION_QUEUE, COMPLETED_COMPCTION_QUEUE etc to manage ACID operations.
Per table write ID implementation (HIVE-18192) introduces couple of metatables such as NEXT_WRITE_ID and TXN_TO_WRITE_ID to manage write ids allocated per table.
Now, when we rename any tables, it is necessary to update the corresponding table names in these metatables as well. Otherwise, ACID table operations won't work properly.
Since, this change is significant and have other side-effects, we propose to disable rename tables on ACID tables until a fixisfigured out. 
 Rename table impacts the ACID behavior as table names are not updated in meta-tables.",yes
13193952,"Reference:
https://builds.apache.org/job/PreCommit-HDFS-Build/25322/testReport/junit/org.apache.hadoop.hdfs.server.blockmanagement/TestPendingReconstruction/testPendingAndInvalidate/
Error Message :


java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReconstruction.testPendingAndInvalidate(TestPendingReconstruction.java:457)


 
 TestPendingReconstruction.testPendingAndInvalidate fails",13213502,"The replace flag is initially added inHIVE-16197. It would be good to have some tests in TestDbNotificationListener to validate if the flagis set as expected. 
 Tests for replace flag in insert event messages in Metastore notifications.",No
13206543,"Fails about 50% of time in the GCE builds... https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2.0/lastSuccessfulBuild/artifact/dashboard.html
We are disabling table to delete at end of test when multi regions and we don't complete... pid=21:


 2018-12-26 20:52:17,179 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] assignment.RegionTransitionProcedure(297): Received report CLOSED seqId=-1, pid=27, ppid=21, state=RUNNABLE:REGION_TRANSITION_DISPATCH, locked=true; UnassignProcedure                      table=testWithMapReduce, region=b66d77b9dd3eaed5b4f8c89872bb56ca, server=192.168.1.51,55250,1545886322135; rit=CLOSING, location=192.168.1.51,55250,1545886322135
 2018-12-26 20:52:17,179 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] procedure.MasterProcedureScheduler(339): Add TableQueue(testWithMapReduce, xlock=false sharedLock=8 size=1) to run queue because: pid=27, ppid=21, state=RUNNABLE:                          REGION_TRANSITION_FINISH, locked=true; UnassignProcedure table=testWithMapReduce, region=b66d77b9dd3eaed5b4f8c89872bb56ca, server=192.168.1.51,55250,1545886322135 has lock
 2018-12-26 20:52:17,179 DEBUG [RpcServer.default.FPBQ.Fifo.handler=1,queue=0,port=55246] assignment.RegionTransitionProcedure(297): Received report CLOSED seqId=-1, pid=26, ppid=21, state=RUNNABLE:REGION_TRANSITION_DISPATCH, locked=true; UnassignProcedure                      table=testWithMapReduce, region=71231f8d1e9b8d4dc83519a6b2b0602a, server=192.168.1.51,55250,1545886322135; rit=CLOSING, location=192.168.1.51,55250,1545886322135
 2018-12-26 20:52:17,179 DEBUG [PEWorker-1] procedure.MasterProcedureScheduler(349): Remove TableQueue(testWithMapReduce, xlock=false sharedLock=8 size=0) from run queue because: queue is empty after polling out pid=27, ppid=21, state=RUNNABLE:REGION_TRANSITION_FINISH,         locked=true; UnassignProcedure table=testWithMapReduce, region=b66d77b9dd3eaed5b4f8c89872bb56ca, server=192.168.1.51,55250,1545886322135
 2018-12-26 20:52:17,179 DEBUG [PEWorker-1] assignment.RegionTransitionProcedure(394): Finishing pid=27, ppid=21, state=RUNNABLE:REGION_TRANSITION_FINISH, locked=true; UnassignProcedure table=testWithMapReduce, region=b66d77b9dd3eaed5b4f8c89872bb56ca, server=192.168.1.51,      55250,1545886322135; rit=CLOSING, location=192.168.1.51,55250,1545886322135
 2018-12-26 20:52:17,179 DEBUG [RpcServer.default.FPBQ.Fifo.handler=1,queue=0,port=55246] procedure.MasterProcedureScheduler(339): Add TableQueue(testWithMapReduce, xlock=false sharedLock=8 size=1) to run queue because: pid=26, ppid=21, state=RUNNABLE:                          REGION_TRANSITION_FINISH, locked=true; UnassignProcedure table=testWithMapReduce, region=71231f8d1e9b8d4dc83519a6b2b0602a, server=192.168.1.51,55250,1545886322135 has lock
 2018-12-26 20:52:17,180 DEBUG [PEWorker-14] procedure.MasterProcedureScheduler(349): Remove TableQueue(testWithMapReduce, xlock=false sharedLock=8 size=0) from run queue because: queue is empty after polling out pid=26, ppid=21, state=RUNNABLE:REGION_TRANSITION_FINISH,        locked=true; UnassignProcedure table=testWithMapReduce, region=71231f8d1e9b8d4dc83519a6b2b0602a, server=192.168.1.51,55250,1545886322135
 2018-12-26 20:52:17,180 DEBUG [PEWorker-14] assignment.RegionTransitionProcedure(394): Finishing pid=26, ppid=21, state=RUNNABLE:REGION_TRANSITION_FINISH, locked=true; UnassignProcedure table=testWithMapReduce, region=71231f8d1e9b8d4dc83519a6b2b0602a, server=192.168.1.51,     55250,1545886322135; rit=CLOSING, location=192.168.1.51,55250,1545886322135
 2018-12-26 20:52:17,405 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:17,911 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:18,917 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:20,922 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:24,928 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:34,932 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:44,934 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:52:54,938 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21
 2018-12-26 20:53:04,944 DEBUG [RpcServer.default.FPBQ.Fifo.handler=2,queue=0,port=55246] master.MasterRpcServices(1152): Checking to see if procedure is done pid=21


This happens about time of fail of last few pid=21 subprocs:


 ***** ABORTING region server 192.168.1.51,55250,1545886322135: Caught throwable while processing event M_RS_CLOSE_REGION *****
 Cause:
 java.lang.RuntimeException: java.io.IOException: The new max sequence id 6 is less than the old max sequence id 7
   at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:116)
   at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
   at java.base/java.lang.Thread.run(Thread.java:844)
 Caused by: java.io.IOException: The new max sequence id 6 is less than the old max sequence id 7
   at org.apache.hadoop.hbase.wal.WALSplitter.writeRegionSequenceIdFile(WALSplitter.java:684)
   at org.apache.hadoop.hbase.regionserver.HRegion.writeRegionCloseMarker(HRegion.java:1134)
   at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1662)
   at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1479)
   at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:104)
   ... 4 more


Fails for me locally. 
 Flakey TestTableSnapshotInputFormat; DisableTable not completing...",13203335,"see: https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/branch-2.0/lastSuccessfulBuild/artifact/dashboard.html
RS aborted because : 


2018-12-09 00:34:18,635 WARN  [RpcServer.default.FPBQ.Fifo.handler=1,queue=0,port=34270] master.MasterRpcServices(514): asf905.gq1.ygridcore.net,32908,1544315637411 reported a fatal error:
***** ABORTING region server asf905.gq1.ygridcore.net,32908,1544315637411: Unrecoverable exception while closing region testWithMapReduce,,1544315644043.97dff0ec285658ab3f73d5ca42a97b6e., still finishing close *****
Cause:
java.io.IOException: The new max sequence id 6 is less than the old max sequence id 7
        at org.apache.hadoop.hbase.wal.WALSplitter.writeRegionSequenceIdFile(WALSplitter.java:684)
        at org.apache.hadoop.hbase.regionserver.HRegion.writeRegionCloseMarker(HRegion.java:1134)
        at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1662)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1479)
        at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:104)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

 
 The TestTableSnapshotInputFormat is flaky",yes
13230128,"attempting to build against current hadoop trunk for HBASE-22087 shows that hte byo-hadoop client is trying to package transitive dependencies from the hadoop dependencies that we expressly say we don't need to bring with us.
it's because we don't list those modules as provided, so all of their transitives are also in compile scope. The shading module does simple filtering when excluding things in a given scope, it doesn't e.g. make sure to also exclude the transitive dependencies of things it keeps out.
since we don't want to list all the transitive dependencies of hadoop in our shading exclusion, we should list the needed hadoop modules as provided. 
 shaded byo-hadoop client should list needed hadoop modules as provided scope to avoid inclusion of unnecessary transitive depednencies",13287980,"As a follow-up to HIVE-22918, this ticket is to investigate whether the empty bucket file creation mechanism can be removed safely when using MR as the engine.
For a bucketed table of N buckets, each insert will generate N bucket files in the delta directory, regardless of how many actual buckets are written to. As an example, if a table has 500 buckets, and we insert a single record, 499 empty bucket files are generated alongside the single bucket that contains the actual data. This makes the operation substantially slower in some cases. This behaviour only seems to happen when using MR as the execution engine.
Some components/parts of the code might depend on this behaviour though, so it needs to be verified that removing this logic does not interfere with anything. 
 Investigate possibility of removing empty bucket file creation mechanism in Hive-on-MR",No
13237953,"We can downgrade NN during roling upgrade (runned ""hdfs dfsadmin -rollingUpgrade prepare) with HDFS-8432 involved. But with HDFS-14172 if the image has any unrecogized section, it will throw IOException at
FSImageFormatProtobuf.java

private void loadInternal(......) {
    ......
    String n = s.getName();
    SectionName sectionName = SectionName.fromString(n);
    if (sectionName == null) {
      throw new IOException(""Unrecognized section "" + n);
    }
    ......
}


and throw NPE on Hadoop 2.x
FSImageFormatProtobuf.java

private void loadInternal(......) {
    ......
    String n = s.getName();
    switch (sectionName)
    ......
}


When we downgrade NN from 3.x to 2.x, NN may load the image saved by 3.x NN. Then the lack of SectionName.ERASURE_CODING can break 2.x NN.
We should just skip the unrecogized section instead of throwing exception. 
 NN throws NPE if downgrade it during rolling upgrade from 3.x to 2.x",13224611,"After fixing HDFS-13596, try to downgrade from 3.x to 2.x. But namenode can't start because exception occurs. The message follows


2019-01-23 17:22:18,730 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/data1/hadoopdata/hadoop-namenode/current/fsimage_0000000000000025310, cpktTxId=0000000000
000025310)
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)
        at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:179)
        at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:226)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:885)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:869)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:742)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:673)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:290)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:998)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:700)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:612)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:672)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:839)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1517)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1583)
2019-01-23 17:22:19,023 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
java.io.IOException: Failed to load FSImage file, see error(s) above for more info.
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:688)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:290)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:998)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:700)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:612)


This issue occurs because 3.x namenode saves image with EC fields during upgrade
Try to fix it 
 Failed to load image from FSImageFile when downgrade from 3.x to 2.x",yes
13155612,"Tensorflow could be run onYARN and could leverage YARN's distributed features.
This spec fill will help to run Tensorflow on yarn with GPU/docker

Please go toYARN-8135Submarine for deep learning framework support on YARN. 
 Running Tensorflow on YARN with GPU and Docker - Examples",13151199,"Description:
Goals:

Allow infra engineer / data scientist to run unmodified Tensorflow jobs on YARN.
Allow jobs easy access data/models in HDFS and other storages.
Can launch services to serve Tensorflow/MXNet models.
Support run distributed Tensorflow jobs with simple configs.
Support run user-specified Docker images.
Support specify GPU and other resources.
Support launch tensorboard if user specified.
Support customized DNS name for roles (like tensorboard.$user.$domain:6006)

Why this name?

Because Submarine is the only vehicle canlet human to explore deep places. B-)

Please refer to on-going design doc, and add your thoughts:https://docs.google.com/document/d/199J4pB3blqgV9SCNvBbTqkEoQdjoyGMjESV4MktCo0k/edit#

Submarine all document list in here:https://issues.apache.org/jira/browse/SUBMARINE-113

See Also:

Zeppelin integration with Submarine design: https://docs.google.com/document/d/16YN8Kjmxt1Ym3clx5pDnGNXGajUT36hzQxjaik1cP4A/edit#heading=h.4jov859x47qe

 
 Hadoop {Submarine} Project: Simple and scalable deployment of deep learning training / serving jobs on Hadoop",yes
13146058,"If hdfs dfsrouteradmin -setQuota specifies a nonexistent path in mount table, there is any error prompt message.
I think we should print error prompt message like:
hdfs dfsrouteradmin -setQuota /test -nsQuota 400 --ssQuota 200
/test not exist in mount table.
 
 RBF: Optimize setQuota error output",13146055,"If hdfs dfsrouteradmin -setQuota only set one of -nsQuota and -ssQuota, the another one will be cleared.
The following is the step:
1. set /test1 -nsQuota 400 -ssQuota 200
hdfs dfsrouteradmin -setQuota /test1 -nsQuota 400 -ssQuota 200
Successfully set quota for mount point /test1
2. after a moment, list /test1, everything is ok.
hdfs dfsrouteradmin -ls /test1
Source                    Destinations              Owner                     Group                     Mode                      Quota/Usage              
/test1                    ns1->/test1               hadp                      hadp                      rwxr-xr-x                 [NsQuota: 400/0, SsQuota: 200 B/0 B]
3. only set /test1 -nsQuota 600
hdfs dfsrouteradmin -setQuota /test1 -nsQuota 600
Successfully set quota for mount point /test1
4.  after a moment, list /test1, now ssQuota has been cleared.
hdfs dfsrouteradmin -ls /test1
Source                    Destinations              Owner                     Group                     Mode                      Quota/Usage              
/test1                    ns1->/test1               hadp                      hadp                      rwxr-xr-x                 [NsQuota: 600/0, SsQuota: -/-]
 HDFS-13307.001.patch  
 RBF: Improve the use of setQuota command",yes
13149648,"
hdfs dfs -ls


this is happeningbecausegetMountPointDates is not implemented


private Map<String, Long> getMountPointDates(String path) {
Map<String, Long> ret = new TreeMap<>();
// TODO add when we have a Mount Table
return ret;
}

 
 RBF: Wrong date information in list file(-ls) result",13316117,"Theupdate_releasenotes function does not remove previous RC's Release notes fromRELEASENOTES.md file.
CHANGES.md does not have this issue.
https://github.com/apache/hbase/blob/master/dev-support/create-release/release-util.sh#L493-L497 
 Previous RC Release notes are not removed",No
13193002,"By default Hive 3.1.0 depends on HBase 2.0.0-alpha4. HBase 2.0.2 migrated from hbase.util.Base64 to java.util.Base64 (HBASE-20884), which causes Hive 3.1.0 fails to build with HBase 2.0.2.

$ cd hbase-handler
$ mvn package -DskipTests -Dhbase.version=2.0.2
[ERROR] .../hive/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableSnapshotInputFormat.java:[29,36] cannot find symbol
[ERROR] symbol: class Base64
[ERROR] location: package org.apache.hadoop.hbase.util

To make Hive works with 2.0.2+ (and also older versions), we should consider migrating Hive to java.util.Base64. 
 Migrate hbase.util.Base64 to java.util.Base64",13172981,"HBase is removing their Base64 implementation because it never should have been public, so Hive should switch to a different provider. Hive already uses Commons-Codec Base64 in other places, so that would be a natural replacement. 
 Hive shouldn't use HBase's Base64 implementation",yes
13337397,"If the Cleaner didn't remove any files, don't mark the compaction queue entry as ""succeeded"" but instead leave it in ""ready for cleaning"" state for later cleaning. If it removed at least one file, then the compaction queue entry as ""succeeded"". This is a partial fix, HIVE-24291 is the complete fix. 
 compactor.Cleaner should not set state ""mark cleaned"" if it didn't remove any files",13193952,"Reference:
https://builds.apache.org/job/PreCommit-HDFS-Build/25322/testReport/junit/org.apache.hadoop.hdfs.server.blockmanagement/TestPendingReconstruction/testPendingAndInvalidate/
Error Message :


java.lang.ArrayIndexOutOfBoundsException: 1 at org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReconstruction.testPendingAndInvalidate(TestPendingReconstruction.java:457)


 
 TestPendingReconstruction.testPendingAndInvalidate fails",No
13164697,"When balancer runs, it tries to find least loaded server with better locality for current region. During this, we make debug level logging for each of those regions. It creates too much amount of logging at debug level , we should move this to trace level logging.


int getLeastLoadedTopServerForRegion (int region, int currentServer) {
...
    if (leastLoadedServerIndex != -1) {
        LOG.debug(""Pick the least loaded server "" + servers[leastLoadedServerIndex].getHostname()
        + "" with better locality for region "" + regions[region]);
    }
...
}

This was fixed in branch-2.0 as part of HBASE-14614 
 too much logging when balancer runs from BaseLoadBalancer",13344162,"We found a socket file descriptor leak when we tried to get the checksum of EC file with reconstruction happened during the operation.
The cause of the leak seems that the StripedBlockChecksumReconstructor does not close StripedReader. Making the reader closed, the CLOSE_WAIT connections are gone. 
 EC: Socket file descriptor leak in StripedBlockChecksumReconstructor",No
13136583,"The testcase TestMRJobs#testJobClassloaderWithCustomClasses fails consistently with this error:

[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.mapreduce.v2.TestMRJobs
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 54.325 s <<< FAILURE! - in org.apache.hadoop.mapreduce.v2.TestMRJobs
[ERROR] testJobClassloaderWithCustomClasses(org.apache.hadoop.mapreduce.v2.TestMRJobs)  Time elapsed: 10.531 s  <<< FAILURE!
java.lang.AssertionError: 
Job status: Application application_1517928628935_0001 failed 2 times due to AM Container for appattempt_1517928628935_0001_000002 exited with  exitCode: 1
Failing this attempt.Diagnostics: [2018-02-06 15:50:38.688]Exception from container-launch.
Container id: container_1517928628935_0001_02_000001
Exit code: 1

[2018-02-06 15:50:38.693]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
log4j:WARN No appenders could be found for logger (org.apache.hadoop.mapreduce.v2.app.MRAppMaster).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.


[2018-02-06 15:50:38.694]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
log4j:WARN No appenders could be found for logger (org.apache.hadoop.mapreduce.v2.app.MRAppMaster).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.


For more detailed output, check the application tracking page: http://ubuntu:46235/cluster/app/application_1517928628935_0001 Then click on links to logs of each attempt.
. Failing the application.
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testJobClassloader(TestMRJobs.java:529)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testJobClassloaderWithCustomClasses(TestMRJobs.java:477)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


Today I found the offending commit with git bisect and this failure is caused by YARN-2185.
The application master fails because of the following error:

2018-02-05 17:15:18,530 DEBUG [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
1: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ExitUtil.terminate(ExitUtil.java:265)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1694)
Caused by: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:550)
        ... 11 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 12 more
Caused by: java.lang.ExceptionInInitializerError: incorrect classloader used
        at org.apache.hadoop.mapreduce.v2.TestMRJobs$CustomOutputFormat.verifyClassLoader(TestMRJobs.java:551)
        at org.apache.hadoop.mapreduce.v2.TestMRJobs$CustomOutputFormat.<init>(TestMRJobs.java:535)
        ... 17 more


The test expects the class CustomOutputFormat to be loaded by ApplicationClassLoader. After YARN-2185 however, the class is loaded by the system classloader. This class was supposed to be loaded from job.jar but apparently is picked from a different place. 
 Testcase TestMRJobs#testJobClassloaderWithCustomClasses fails ",13135687,"YARN-2185 added the ability to localize jar files as a stream instead of copying to local disk and then extracting. ZipInputStream does not need the end of the file. Let's read it out. This helps with an additional TeeInputStream on the input. 
 Localized jars that are expanded after localization are not fully copied",yes
13160803,"The jobSubmitDir directory is owned by root and is being cleaned up as the submitting user, which appears to be why it is failing to clean up.

2018-05-21 19:46:15,124 WARN  [DeletionService #0] privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 255. Privileged Execution Operation Stderr:

Stdout: main : command provided 3
main : run as user is ebadger
main : requested yarn user is ebadger
failed to unlink /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/jobSubmitDir/job.split: Permission denied
failed to unlink /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/jobSubmitDir/job.splitmetainfo: Permission denied
failed to rmdir jobSubmitDir: Directory not empty
Error while deleting /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001: 39 (Directory not empty)

Full command array for failed execution:
[/hadoop-3.2.0-SNAPSHOT/bin/container-executor, ebadger, ebadger, 3, /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001]
2018-05-21 19:46:15,124 ERROR [DeletionService #0] nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(848)) - DeleteAsUser for /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001 returned with exit code: 255
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=255:
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:206)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.deleteAsUser(LinuxContainerExecutor.java:844)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask.run(FileDeletionTask.java:135)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: ExitCodeException exitCode=255:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        ... 10 more



[foo@bar hadoop]$ ls -l /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/
total 4
drwxr-sr-x 2 root users 4096 May 21 19:45 jobSubmitDir

 
 Privileged docker containers' jobSubmitDir does not get successfully cleaned up",13136979,"Since they will be running as some other user than themselves, the NM likely won't be able to clean up after them because of permissions issues. So, to prevent this, we should make these directories read-only. 
 Privileged, trusted containers should be supported only in ENTRYPOINT mode",yes
13256922,"Was unable to fetch status for a STOPPED app due to the below error in RM logs.


ERROR webapp.ApiServer (ApiServer.java:getService(213)) - Get service failed: {}
java.io.EOFException: Read of hdfs://my-cluster:8020/user/appuser/.yarn/services/my-service/my-service.json finished prematurely
        at org.apache.hadoop.yarn.service.utils.JsonSerDeser.load(JsonSerDeser.java:188)
        at org.apache.hadoop.yarn.service.utils.ServiceApiUtil.loadService(ServiceApiUtil.java:360)
        at org.apache.hadoop.yarn.service.client.ServiceClient.getAppId(ServiceClient.java:1409)
        at org.apache.hadoop.yarn.service.client.ServiceClient.getStatus(ServiceClient.java:1235)
        at org.apache.hadoop.yarn.service.webapp.ApiServer.lambda$getServiceFromClient$3(ApiServer.java:749)


This seems to happen when the json file my-service.json is larger than 128KB in my cluster. 
 YARN Service fails to fetch status for Stopped apps with bigger spec files",13207986,"Found the problem when implementing HBASE-21671, TestRegionReplicaFailover is easy to fail. In this test we set REGION_REPLICA_WAIT_FOR_PRIMARY_FLUSH_CONF_KEY to true, which means the secondary replicas can not be read until the primary region has been successfully flushed. But in RegionReplicaFlushHandler, there is only one place where we call region.setReadsEnabled(true), this is incorrect. 
 Reset readsEnabled flag after successfully flushing the primary region",No
13160843,"There is hadoop version mismatch between hive and storage-api and hence different transitive dependency versions gets pulled. 
 sync up hadoop version used by storage-api with hive",13130139,"Fix the remaining Checkstyle errors in the hbase-backup module and enable Checkstyle to fail on violations. 
 Fix Checkstyle errors in hbase-backup",No
13240995,"Whilecolocating the recovered edits directory with hbase.wal.dir, BASE_NAMESPACE_DIR got missed. This results in recovered edits being put in a separate directory rather than the default region directory even if the hbase.wal.dir is not overridden. Eg. if data is stored in /hbase/data/namespace/table1, recovered edits are put in /hbase/namespace/table1. This also messes up the regular cleaner chores which never operate on this new directory and these directories will never be deleted, even for split parents or dropped tables. We should change the default back to have the base namespace directory in path. 
 Recovered WAL directories not getting cleaned up",13204251,"If HBASE_WAL_DIR if not configured, then 
recovered.edits dir path should be old method only.
If user is creating x no. of tables, in different namespaces, then all are creating in the ""hbase.rootdir"" path only.


/<hbase.rootdir>/data/<namespace>/<table>/<regionDir>/recovered.edits
eg:
/hbase/data/default/testTable/eaf343d35d3e66e6e5fd38106ba61c62/recovered.edits


But the format is currently. 


/<hbase.rootdir>/<namespace>/<table>/<regionDir>/recovered.edits
eg:
/hbase/default/testTable/eaf343d35d3e66e6e5fd38106ba61c62/recovered.edits

 
 HBASE_WAL_DIR if not configured, recovered.edits directory's are sidelined from the table dir path.",yes
13216563,"If external tables are enabled for replication on an existing repl policy, then bootstrapping of external tables are combined with incremental dump.
If incremental bootstrap load fails with non-retryable error for which user will have to manually drop all the external tables before trying with another bootstrap dump. For full bootstrap, to retry with different dump, we suggested user to drop the DB but in this case they need to manually drop all the external tables which is not so user friendly. So, need to handle it in Hive side as follows.
REPL LOAD takes additional config (passed by user in WITH clause) that says, drop all the tables which are bootstrapped from previous dump. 
hive.repl.clean.tables.from.bootstrap=<previous_bootstrap_dump_dir>
Hive will use this config only if the current dump is combined bootstrap in incremental dump.
Caution to be taken by user that this config should not be passed if previous REPL LOAD (with bootstrap) was successful or any successful incremental dump+load happened after ""previous_bootstrap_dump_dir"". 
 Hive should support clean-up of previously bootstrapped tables when retry from different dump.",13150057,"HBASE-10092 deserves a note in the upgrade section, even if we believe that existing out-of-the-box log4j configuration should work seemlessly. 
 [DOC] upgrade section should warn about logging changes",No
13153782,"Hive replication uses Hadoop distcp to copy files from primary to replica warehouse. If the HDFS block size is different across clusters, it cause file copy failures.


2018-04-09 14:32:06,690 ERROR [main] org.apache.hadoop.tools.mapred.CopyMapper: Failure in copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0
java.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0
 at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)
 at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)
 at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)
 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)
 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0
 at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
 at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)
 ... 10 more
Caused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)
 at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)
 at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)
 at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)
 at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
 ... 11 more


Distcp failed as the CM path for the file doesn't point to source file system. So, it is needed to get the qualified cm root URI as part of files listed in dump.
Also,REPL LOAD returns success even if distcp jobs failed.
CopyUtils.doCopyRetry doesn't throw error if copy failed even after maximum attempts.
So, need to perform 2 things.

Ifcopy of multiple files fail for some reason,then retry with same set of files again but need to set CM path if original source file is missing or modified based on checksum. Let distcp to skip the properly copied files. FileUtil.copy will always overwrite the files.
If source path is moved to CM path, then delete the incorrectly copied files.
If copy fails for maximum attempt, then throw error.

 
 REPL LOAD couldn't copy file from source CM path and also doesn't throw error if file copy fails.",13253234,"When we do source package of hadoop, it should not contain submarine project/code. 
 Exclude submarine from hadoop source build",No
13140340,"https://github.com/apache/hadoop/blob/69fa81679f59378fd19a2c65db8019393d7c05a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java


  private ThreadPoolExecutor renewerService;

  private void processDelegationTokenRenewerEvent(
      DelegationTokenRenewerEvent evt) {
    serviceStateLock.readLock().lock();
    try {
      if (isServiceStarted) {
        renewerService.execute(new DelegationTokenRenewerRunnable(evt));
      } else {
        pendingEventQueue.add(evt);
      }
    } finally {
      serviceStateLock.readLock().unlock();
    }
  }

  @Override
  protected void serviceStop() {
    if (renewalTimer != null) {
      renewalTimer.cancel();
    }
    appTokens.clear();
    allTokens.clear();
    this.renewerService.shutdown();




2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)


What I think is going on here is that theserviceStopmethod is not setting theisServiceStartedflag to 'false'.
Please update so that theserviceStopmethod grabs theserviceStateLockand setsisServiceStartedtofalse, before shutting down therenewerServicethread pool,to avoid this condition. 
 Race Condition When Stopping DelegationTokenRenewer causes RM crash during failover",13140339,"https://github.com/apache/hadoop/blob/69fa81679f59378fd19a2c65db8019393d7c05a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java


  private ThreadPoolExecutor renewerService;

  private void processDelegationTokenRenewerEvent(
      DelegationTokenRenewerEvent evt) {
    serviceStateLock.readLock().lock();
    try {
      if (isServiceStarted) {
        renewerService.execute(new DelegationTokenRenewerRunnable(evt));
      } else {
        pendingEventQueue.add(evt);
      }
    } finally {
      serviceStateLock.readLock().unlock();
    }
  }

  @Override
  protected void serviceStop() {
    if (renewalTimer != null) {
      renewalTimer.cancel();
    }
    appTokens.clear();
    allTokens.clear();
    this.renewerService.shutdown();




2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)


What I think is going on here is that the serviceStop method is not setting the isServiceStarted flag to 'false'.
Please update so that the serviceStop method grabs the serviceStateLock and sets isServiceStarted to false, before shutting down the renewerService thread pool,to avoid this condition. 
 Race Condition When Stopping DelegationTokenRenewer",yes
13224052,"I test like this
HQL=hdfs://hacluster/tmp/ff.hql
if hadoop fs -test -f ${HQL}
then
 beeline -f ${HQL}
fi
test ${HQL} ok, but beeline report ${HQL} no such file or directory 
 beeline -f report no such file if file is not on local fs",13147402,"I test like this
HQL=hdfs://hacluster/tmp/ff.hql
if hadoop fs -test -f ${HQL}
then
 beeline -f ${HQL}
fi
test ${HQL} ok, but beeline report ${HQL} no such file or directory 
 hadoop fs test can check srcipt ok, but beeline -f report no such file",yes
13337328,"TestBPOfferService.testMissBlocksWhenReregister  is flaky. It fails randomly when the 
following expression is not true:


      assertTrue(fullBlockReportCount == totalTestBlocks ||
          incrBlockReportCount == totalTestBlocks);


There is a race condition here that relies once more on ""time"" to synchronize between concurrent threads. The code below is is causing the non-deterministic execution.
On a slow server, addNewBlockThread may not be done by the time the main thread reach the assertion call.


      // Verify FBR/IBR count is equal to generate number.
      assertTrue(fullBlockReportCount == totalTestBlocks ||
          incrBlockReportCount == totalTestBlocks);
    } finally {
      addNewBlockThread.join();
      bpos.stop();
      bpos.join();


Therefore, the correct implementation should wait for the thread to finish


     // the thread finished execution.
     addNewBlockThread.join();
      // Verify FBR/IBR count is equal to generate number.
      assertTrue(fullBlockReportCount == totalTestBlocks ||
          incrBlockReportCount == totalTestBlocks);
    } finally {
      bpos.stop();
      bpos.join();


DataNodeFaultInjector needs to have a longer wait_time too. 1 second is not enough to satisfy the condition.


      DataNodeFaultInjector.set(new DataNodeFaultInjector() {
        public void blockUtilSendFullBlockReport() {
          try {
            GenericTestUtils.waitFor(() -> {
              if(count.get() > 2000) {
                return true;
              }
              return false;
            }, 100, 10000); // increase that waiting time to 10 seconds.
          } catch (Exception e) {
            e.printStackTrace();
          }
        }
      });




Stacktrace
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testMissBlocksWhenReregister(TestBPOfferService.java:350)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Standard Output
2020-10-26 07:38:56,442 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-10-26 07:38:56,454 [main] INFO  datanode.DataNode (SimulatedFSDataset.java:registerMBean(1370)) - Registered FSDatasetState MBean
2020-10-26 07:38:56,455 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-10-26 07:38:56,460 [Thread-103] INFO  datanode.DataNode (BPServiceActor.java:run(846)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:0 starting to offer service
2020-10-26 07:38:56,461 [Thread-104] INFO  datanode.DataNode (BPServiceActor.java:run(846)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:1 starting to offer service
2020-10-26 07:38:56,463 [Thread-103] DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(245)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:0 received versionRequest response: lv=-65;cid=fake cluster;nsid=1;c=0;bpid=fake bpid
2020-10-26 07:38:56,464 [Thread-104] DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(245)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:1 received versionRequest response: lv=-65;cid=fake cluster;nsid=1;c=0;bpid=fake bpid
2020-10-26 07:38:56,466 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.DataNode (BPServiceActor.java:register(787)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 beginning handshake with NN
2020-10-26 07:38:56,466 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.DataNode (BPServiceActor.java:register(787)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 beginning handshake with NN
2020-10-26 07:38:56,467 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.DataNode (BPServiceActor.java:register(815)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 successfully registered with NN
2020-10-26 07:38:56,467 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.DataNode (BPServiceActor.java:offerService(644)) - For namenode 0.0.0.0/0.0.0.0:0 using BLOCKREPORT_INTERVAL of 21600000msecs CACHEREPORT_INTERVAL of 10000msecs Initial delay: 0msecs; heartBeatInterval=3000
2020-10-26 07:38:56,467 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.DataNode (BPServiceActor.java:register(815)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 successfully registered with NN
2020-10-26 07:38:56,469 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.DataNode (BPServiceActor.java:offerService(644)) - For namenode 0.0.0.0/0.0.0.0:1 using BLOCKREPORT_INTERVAL of 21600000msecs CACHEREPORT_INTERVAL of 10000msecs Initial delay: 0msecs; heartBeatInterval=3000
2020-10-26 07:38:56,469 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:38:56,470 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:38:56,471 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=1
2020-10-26 07:38:56,471 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=2
2020-10-26 07:38:56,474 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.DataNode (BPServiceActor.java:blockReport(437)) - Successfully sent block report 0x7688fa248c41cdeb,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msecs to generate and 0 msecs for RPC and NN processing. Got back no commands.
2020-10-26 07:38:56,475 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.DataNode (BPServiceActor.java:blockReport(437)) - Successfully sent block report 0x7fd3be5ab5d82bff,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 1 msecs to generate and 0 msecs for RPC and NN processing. Got back no commands.
2020-10-26 07:38:56,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:38:56,670 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:38:56,671 [Command processor] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActor(674)) - DatanodeCommand action : DNA_REGISTER from 0.0.0.0/0.0.0.0:0 with standby state
2020-10-26 07:38:56,673 [Command processor] DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(245)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 received versionRequest response: lv=-65;cid=fake cluster;nsid=1;c=0;bpid=fake bpid
2020-10-26 07:38:56,674 [Command processor] INFO  datanode.DataNode (BPServiceActor.java:register(787)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 beginning handshake with NN
2020-10-26 07:38:56,675 [Command processor] INFO  datanode.DataNode (BPServiceActor.java:register(815)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 successfully registered with NN
2020-10-26 07:38:56,770 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:38:56,770 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:38:59,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:38:59,670 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=3
2020-10-26 07:38:59,681 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.DataNode (BPServiceActor.java:blockReport(437)) - Successfully sent block report 0x7688fa248c41cdec,  containing 1 storage report(s), of which we sent 1. The reports had 1782 total blocks and used 1 RPC(s). This took 9 msecs to generate and 1 msecs for RPC and NN processing. Got back no commands.
2020-10-26 07:38:59,770 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:38:59,771 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:02,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:39:02,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:02,770 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:39:02,771 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:05,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:39:05,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:05,770 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:39:05,771 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:08,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:39:08,670 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:08,771 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:39:08,771 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:11,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
2020-10-26 07:39:11,669 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:11,771 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(522)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:39:11,772 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] INFO  datanode.TestBPOfferService (TestBPOfferService.java:answer(205)) - fullBlockReportLeaseId=0
2020-10-26 07:39:11,878 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-10-26 07:39:11,878 [Command processor] ERROR datanode.DataNode (BPServiceActor.java:processQueue(1329)) - Command processor encountered interrupt and exit.
2020-10-26 07:39:11,878 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-10-26 07:39:11,878 [Command processor] ERROR datanode.DataNode (BPServiceActor.java:processQueue(1329)) - Command processor encountered interrupt and exit.
2020-10-26 07:39:11,879 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:1] WARN  datanode.DataNode (BPServiceActor.java:run(889)) - Ending block pool service for: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1
2020-10-26 07:39:11,878 [fake bpid heartbeating to 0.0.0.0/0.0.0.0:0] WARN  datanode.DataNode (BPServiceActor.java:run(889)) - Ending block pool service for: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0
Standard Error
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
Timestamp: 2020-10-26 07:38:57,678

""nioEventLoopGroup-2-30""  prio=10 tid=190 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""fake bpid heartbeating to 0.0.0.0/0.0.0.0:1"" daemon prio=5 tid=260 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(IncrementalBlockReportManager.java:158)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:738)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:878)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-27""  prio=10 tid=187 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""Signal Dispatcher"" daemon prio=9 tid=4 runnable
java.lang.Thread.State: RUNNABLE
""nioEventLoopGroup-2-4""  prio=10 tid=164 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-10""  prio=10 tid=170 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-41""  prio=10 tid=201 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-19""  prio=10 tid=179 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-1""  prio=10 tid=108 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-40""  prio=10 tid=200 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-36""  prio=10 tid=244 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-20""  prio=10 tid=180 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-6""  prio=10 tid=214 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""process reaper"" daemon prio=10 tid=20 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
""Command processor"" daemon prio=5 tid=258 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1324)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1312)
""nioEventLoopGroup-2-21""  prio=10 tid=181 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-7""  prio=10 tid=215 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-5""  prio=10 tid=165 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-8""  prio=10 tid=168 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""GcTimeMonitor obsWindow = 60000, sleepInterval = 5000, maxGcTimePerc = 100"" daemon prio=5 tid=42 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.util.GcTimeMonitor.run(GcTimeMonitor.java:155)
""nioEventLoopGroup-3-5""  prio=10 tid=213 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""Finalizer"" daemon prio=8 tid=3 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
""nioEventLoopGroup-3-32""  prio=10 tid=240 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""surefire-forkedjvm-ping-30s"" daemon prio=5 tid=19 runnable
java.lang.Thread.State: RUNNABLE
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-3""  prio=10 tid=163 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-47""  prio=10 tid=207 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-2""  prio=10 tid=210 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-43""  prio=10 tid=251 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-21""  prio=10 tid=229 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-46""  prio=10 tid=206 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-34""  prio=10 tid=194 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-37""  prio=10 tid=197 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-33""  prio=10 tid=241 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-4""  prio=10 tid=212 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-38""  prio=10 tid=246 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-25""  prio=10 tid=233 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-39""  prio=10 tid=247 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-46""  prio=10 tid=254 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""main""  prio=5 tid=1 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:394)
        at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testMissBlocksWhenReregister(TestBPOfferService.java:340)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
""nioEventLoopGroup-2-43""  prio=10 tid=203 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-30""  prio=10 tid=238 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-28""  prio=10 tid=236 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-48""  prio=10 tid=256 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-8""  prio=10 tid=216 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""process reaper"" daemon prio=10 tid=140 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-42""  prio=10 tid=250 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-19""  prio=10 tid=227 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-38""  prio=10 tid=198 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-2-worker-11"" daemon prio=5 tid=148 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
""nioEventLoopGroup-2-14""  prio=10 tid=174 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-23""  prio=10 tid=183 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-16""  prio=10 tid=176 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-13""  prio=10 tid=221 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-9""  prio=10 tid=217 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-48""  prio=10 tid=208 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-3""  prio=10 tid=211 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""Reference Handler"" daemon prio=10 tid=2 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
""nioEventLoopGroup-3-16""  prio=10 tid=224 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-28""  prio=10 tid=188 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-14""  prio=10 tid=222 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""Command processor"" daemon prio=5 tid=257 runnable
java.lang.Thread.State: RUNNABLE
        at java.lang.Thread.dumpThreads(Native Method)
        at java.lang.Thread.getAllStackTraces(Thread.java:1610)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87)
        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73)
        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:401)
        at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService$1.blockUtilSendFullBlockReport(TestBPOfferService.java:295)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reRegister(BPServiceActor.java:927)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:676)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processCommand(BPServiceActor.java:1351)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.lambda$enqueue$2(BPServiceActor.java:1397)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread$$Lambda$14/1065143224.run(Unknown Source)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1325)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1312)
""nioEventLoopGroup-3-15""  prio=10 tid=223 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-40""  prio=10 tid=248 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-31""  prio=10 tid=191 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-25""  prio=10 tid=185 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-37""  prio=10 tid=245 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-45""  prio=10 tid=205 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-24""  prio=10 tid=232 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-2-worker-22"" daemon prio=5 tid=151 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
""nioEventLoopGroup-3-45""  prio=10 tid=253 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-35""  prio=10 tid=243 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-22""  prio=10 tid=230 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""LeaseRenewer:jenkins@localhost:35601"" daemon prio=5 tid=158 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:412)
        at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
        at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:308)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-10""  prio=10 tid=218 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-2-worker-18"" daemon prio=5 tid=147 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
""nioEventLoopGroup-2-12""  prio=10 tid=172 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-2""  prio=10 tid=162 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-44""  prio=10 tid=252 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-29""  prio=10 tid=237 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""client DomainSocketWatcher"" daemon prio=5 tid=132 runnable
java.lang.Thread.State: RUNNABLE
        at org.apache.hadoop.net.unix.DomainSocketWatcher.doPoll0(Native Method)
        at org.apache.hadoop.net.unix.DomainSocketWatcher.access$900(DomainSocketWatcher.java:52)
        at org.apache.hadoop.net.unix.DomainSocketWatcher$2.run(DomainSocketWatcher.java:503)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-44""  prio=10 tid=204 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-22""  prio=10 tid=182 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-36""  prio=10 tid=196 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-33""  prio=10 tid=193 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-6""  prio=10 tid=166 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-24""  prio=10 tid=184 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-23""  prio=10 tid=231 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-18""  prio=10 tid=178 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-15""  prio=10 tid=175 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-35""  prio=10 tid=195 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""Thread-105""  prio=5 tid=261 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.lambda$testMissBlocksWhenReregister$0(TestBPOfferService.java:319)
        at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService$$Lambda$80/374646930.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-12""  prio=10 tid=220 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner"" daemon prio=5 tid=95 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
        at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:3937)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-42""  prio=10 tid=202 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-31""  prio=10 tid=239 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-20""  prio=10 tid=228 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-17""  prio=10 tid=225 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-9""  prio=10 tid=169 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-32""  prio=10 tid=192 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""AsyncAppender-Dispatcher-Thread-72"" daemon prio=5 tid=118 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.log4j.AsyncAppender$Dispatcher.run(AsyncAppender.java:548)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-17""  prio=10 tid=177 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-18""  prio=10 tid=226 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-13""  prio=10 tid=173 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""fake bpid heartbeating to 0.0.0.0/0.0.0.0:0"" daemon prio=5 tid=259 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(IncrementalBlockReportManager.java:158)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:738)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:878)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-47""  prio=10 tid=255 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-27""  prio=10 tid=235 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-11""  prio=10 tid=219 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-26""  prio=10 tid=234 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-39""  prio=10 tid=199 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-1""  prio=10 tid=209 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-34""  prio=10 tid=242 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-11""  prio=10 tid=171 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-7""  prio=10 tid=167 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""AsyncAppender-Dispatcher-Thread-51"" daemon prio=5 tid=87 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at org.apache.log4j.AsyncAppender$Dispatcher.run(AsyncAppender.java:548)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-3-41""  prio=10 tid=249 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-2-worker-29"" daemon prio=5 tid=150 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
""nioEventLoopGroup-2-29""  prio=10 tid=189 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""nioEventLoopGroup-2-26""  prio=10 tid=186 timed_waiting
java.lang.Thread.State: TIMED_WAITING
        at java.lang.Thread.sleep(Native Method)
        at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:790)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:525)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-2-worker-25"" daemon prio=5 tid=146 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
""surefire-forkedjvm-command-thread"" daemon prio=5 tid=18 runnable
java.lang.Thread.State: RUNNABLE
        at java.io.FileInputStream.readBytes(Native Method)
        at java.io.FileInputStream.read(FileInputStream.java:255)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
        at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:390)
        at java.lang.Thread.run(Thread.java:748)
""ForkJoinPool-2-worker-4"" daemon prio=5 tid=149 in Object.wait()
java.lang.Thread.State: WAITING (on object monitor)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)


	at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:401)
	at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService$1.blockUtilSendFullBlockReport(TestBPOfferService.java:295)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reRegister(BPServiceActor.java:927)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:676)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processCommand(BPServiceActor.java:1351)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.lambda$enqueue$2(BPServiceActor.java:1397)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1325)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1312)

 
 TestBPOfferService#testMissBlocksWhenReregister fails intermittently",13300649,"TestBPOfferService.testMissBlocksWhenReregister fails intermittently in trunk branch, not sure about other branches. Example failures are

https://builds.apache.org/job/hadoop-multibranch/job/PR-1964/4/testReport/org.apache.hadoop.hdfs.server.datanode/TestBPOfferService/testMissBlocksWhenReregister/
https://builds.apache.org/job/PreCommit-HDFS-Build/29175/testReport/org.apache.hadoop.hdfs.server.datanode/TestBPOfferService/testMissBlocksWhenReregister/

Sample exception stack is:

Stacktrace
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testMissBlocksWhenReregister(TestBPOfferService.java:350)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) 
 TestBPOfferService#testMissBlocksWhenReregister is flaky",yes
13267218,"The actual issue stems to the different date parser used by various part of the engine.
Fetch task uses udfdatediff via 

 org.apache.hadoop.hive.ql.udf.generic.GenericUDFToDate

 while the vectorized llap execution uses 

VectorUDFDateDiffScalarCol

.
This fix is meant to be not very intrusive and will add more support to the GenericUDFToDate by enhancing the parser.
For the longer term will be better to use one parser for all the operators.
Thanks Rajkumar Singh for the repro example

 
create external table testdatediff(datetimecol string) stored as orc;
insert into testdatediff values ('2019-09-09T10:45:49+02:00'),('2019-07-24');
select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183;

set hive.ferch.task.conversion=none;
select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183;

 
 Hive datediff function provided inconsistent results when hive.fetch.task.conversion is set to none",13266308,"Hive datadiff function provided inconsistent results when hive.ferch.task.conversion to more
Below is output, whereas in Hive 1.2 the results are consistent
Note: Same query works well on Hive 3 when hive.ferch.task.conversion is set to none
 Steps to reproduce the problem.


0: jdbc:hive2://c1113-node2.squadron.support.> select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183;
INFO : Compiling command(queryId=hive_20191105103636_1dff22a1-02f3-48a8-b076-0b91272f2268): select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:datetimecol, type:string, comment:null)], properties:null)
INFO : Completed compiling command(queryId=hive_20191105103636_1dff22a1-02f3-48a8-b076-0b91272f2268); Time taken: 0.479 seconds
INFO : Executing command(queryId=hive_20191105103636_1dff22a1-02f3-48a8-b076-0b91272f2268): select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183
INFO : Completed executing command(queryId=hive_20191105103636_1dff22a1-02f3-48a8-b076-0b91272f2268); Time taken: 0.013 seconds
INFO : OK
+--------------+
| datetimecol |
+--------------+
| 2019-07-24 |
+--------------+
1 row selected (0.797 seconds)
0: jdbc:hive2://c1113-node2.squadron.support.>


After setting fetch task conversion as none.


0: jdbc:hive2://c1113-node2.squadron.support.> set hive.fetch.task.conversion=none;
No rows affected (0.017 seconds)
0: jdbc:hive2://c1113-node2.squadron.support.> set hive.fetch.task.conversion;
+----------------------------------+
| set |
+----------------------------------+
| hive.fetch.task.conversion=none |
+----------------------------------+
1 row selected (0.015 seconds)
0: jdbc:hive2://c1113-node2.squadron.support.> select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183;
INFO : Compiling command(queryId=hive_20191105103709_0c38e446-09cf-45dd-9553-365146f42452): select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183


+----------------------------+
| datetimecol |
+----------------------------+
| 2019-09-09T10:45:49+02:00 |
| 2019-07-24 |
+----------------------------+
2 rows selected (5.327 seconds)
0: jdbc:hive2://c1113-node2.squadron.support.>


Steps to reproduce


create external table testdatediff(datetimecol string) stored as orc;
insert into testdatediff values ('2019-09-09T10:45:49+02:00'),('2019-07-24');
select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183;

set hive.ferch.task.conversion=none;
select datetimecol from testdatediff where datediff(cast(current_timestamp as string), datetimecol)<183;

 
 Hive datadiff function provided inconsistent results when hive.ferch.task.conversion is set to none",yes
13143048,"At 200+ sessions on a single HS2, the DbLock impl fails to propagate mysql exceptions


2018-03-06T22:55:16,197 ERROR [HiveServer2-Background-Pool: Thread-12867]: ql.Driver (:()) - FAILED: Error in acquiring locks: null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.DatabaseProduct.isDeadlock(DatabaseProduct.java:56)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkRetryable(TxnHandler.java:2459)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:499)




    return e instanceof SQLTransactionRollbackException
        || ((dbProduct == MYSQL || dbProduct == POSTGRES || dbProduct == SQLSERVER)
            && e.getSQLState().equals(""40001""))
        || (dbProduct == POSTGRES && e.getSQLState().equals(""40P01""))
        || (dbProduct == ORACLE && (e.getMessage().contains(""deadlock detected"")
            || e.getMessage().contains(""can't serialize access for this transaction"")));

 
 ACID: NPE on unexplained mysql exceptions ",13342498,"https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2488/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn.txt

[ERROR] Failures: 
[ERROR]   TestRouterWebServicesREST.testAppAttemptXML:720->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppPriorityXML:796->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppQueueXML:846->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppStateXML:744->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppTimeoutXML:920->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppTimeoutsXML:896->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppXML:696->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testUpdateAppPriorityXML:832 expected:<200> but was:<500>
[ERROR]   TestRouterWebServicesREST.testUpdateAppQueueXML:882 expected:<200> but was:<500>
[ERROR]   TestRouterWebServicesREST.testUpdateAppStateXML:782 expected:<202> but was:<500>
[ERROR] Errors: 
[ERROR]   TestRouterWebServicesREST.testGetAppAttemptXML:1292->getAppAttempt:1464 Â» ClientHandler
[ERROR]   TestRouterWebServicesREST.testGetAppsMultiThread:1337->testGetContainersXML:1317->getAppAttempt:1464 Â» ClientHandler
[ERROR]   TestRouterWebServicesREST.testGetContainersXML:1317->getAppAttempt:1464 Â» ClientHandler 
 
 TestRouterWebServicesREST fails",No
13141707,"The MountTableResolver refreshEntries function have a bug when add a new mount table entry which already have location cache.The old location cache will never be invalid until this mount point change again.
Need to invalid the location cache when add the mount table entries. 
 RBF: Fix router location cache issue",13141644,"To reproduce this issue, run the following commands at Router 1:


$ hdfs dfsrouteradmin -add /test1 ns1 /ns1/test1
$ hdfs dfsrouteradmin -rm /test1
$ hdfs dfsrouteradmin -add /test1 ns1 /ns1/test1

""hdfs dfs -ls hdfs://Router1:8020/test1"" works well after step 1. After step 3 when we add /test1 back, Router 1 still returns ""No such file or directory"".
But after step 3, when we run cmd ""hdfs dfs -ls hdfs://Router2:8020/test1"" talking to another Router, it works well.
From Router logs, I can see StateStoreZookeeperImpl and MountTableResolver are updated correctly and in time. Not find the root case yet, still looking into it.

 
 RBF: Mount path not available after ADD-REMOVE-ADD",yes
13155194,"Allows external clients to consume output from LLAP daemons in Arrow stream format. 
 Arrow format for LlapOutputFormatService (umbrella)",13155204,"This is a sub-class of LlapBaseRecordReader that wraps the socket inputStream and produces Arrow batches for an external client. 
 Provide an Arrow stream reader for external LLAP clients ",yes
13171679,"TopNKey operator is implemented in HIVE-17896, but it needs more work in pushdown implementation. So this issue covers TopNKey pushdown implementation with proper tests. 
 TopNKey pushdown",13171577,"Follow-up for HIVE-17896, which introduces TopNKey operator.
 
 Introduce rule to pushdown TopNKey through plan",yes
13211975,"YARN Web UI 2 footer shows the started time about ""2019-01-25 16:39"" even if the ResourceManager started at ""2019-01-26 00:39:34""(UTC), and my PC's localtime is JST.(+09:00 GMT)
ResourceManager log is below: (Sever time is set as UTC). 


2019-01-26 00:39:34,619 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = XXXXX/XXXXX
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.2.0
(snip)


Web browser console outputs an error like below


TypeError: ""c[0].match(...) is null""
	i http://localhost:8088/ui2/assets/vendor.js:5598:40973
	l http://localhost:8088/ui2/assets/vendor.js:5598:41338
	p http://localhost:8088/ui2/assets/vendor.js:5598:42035
	q http://localhost:8088/ui2/assets/vendor.js:5598:42235
	getDefaultTimezone http://localhost:8088/ui2/assets/yarn-ui.js:378:464
	convertTimestampWithTz http://localhost:8088/ui2/assets/yarn-ui.js:379:220
	timeStampToDate http://localhost:8088/ui2/assets/yarn-ui.js:360:80
	dateFormatter http://localhost:8088/ui2/assets/yarn-ui.js:177:1011
	compute http://localhost:8088/ui2/assets/vendor.js:1052:780
	value http://localhost:8088/ui2/assets/vendor.js:1528:12
	read http://localhost:8088/ui2/assets/vendor.js:1544:58
	readArray http://localhost:8088/ui2/assets/vendor.js:1545:110
	compute http://localhost:8088/ui2/assets/vendor.js:1553:317
	value http://localhost:8088/ui2/assets/vendor.js:1528:12
	read http://localhost:8088/ui2/assets/vendor.js:1544:58
	getValue http://localhost:8088/ui2/assets/vendor.js:907:329
	attribute http://localhost:8088/ui2/assets/vendor.js:2547:54
	attribute http://localhost:8088/ui2/assets/vendor.js:2575:623
	populateNodes http://localhost:8088/ui2/assets/vendor.js:2610:334
	render http://localhost:8088/ui2/assets/vendor.js:2605:265
	render http://localhost:8088/ui2/assets/vendor.js:2579:122
	yieldTemplate http://localhost:8088/ui2/assets/vendor.js:2479:155
	ifUnless http://localhost:8088/ui2/assets/vendor.js:871:85
	ifHelper http://localhost:8088/ui2/assets/vendor.js:869:524
	compute http://localhost:8088/ui2/assets/vendor.js:1051:500
	value http://localhost:8088/ui2/assets/vendor.js:1528:12
	invokeHelper http://localhost:8088/ui2/assets/vendor.js:914:14
	continueBlock http://localhost:8088/ui2/assets/vendor.js:2504:214
	renderAndCleanup http://localhost:8088/ui2/assets/vendor.js:2661:189
	hostBlock http://localhost:8088/ui2/assets/vendor.js:2505:150
	continueBlock http://localhost:8088/ui2/assets/vendor.js:2504:83
	block http://localhost:8088/ui2/assets/vendor.js:2503:1
	block http://localhost:8088/ui2/assets/vendor.js:2572:288
	populateNodes http://localhost:8088/ui2/assets/vendor.js:2610:34
	render http://localhost:8088/ui2/assets/vendor.js:2605:265
	render http://localhost:8088/ui2/assets/vendor.js:2579:122
	_firstRender http://localhost:8088/ui2/assets/vendor.js:2658:245
	renderAndCleanup http://localhost:8088/ui2/assets/vendor.js:2661:189
	_firstRender http://localhost:8088/ui2/assets/vendor.js:2658:55
	invoke http://localhost:8088/ui2/assets/vendor.js:2657:203
	yieldKeyword http://localhost:8088/ui2/assets/vendor.js:991:888
	handleKeyword http://localhost:8088/ui2/assets/vendor.js:2512:40
	handleRedirect http://localhost:8088/ui2/assets/vendor.js:2509:4
	inline http://localhost:8088/ui2/assets/vendor.js:2528:62
	content http://localhost:8088/ui2/assets/vendor.js:2572:903
	populateNodes http://localhost:8088/ui2/assets/vendor.js:2610:181
	render http://localhost:8088/ui2/assets/vendor.js:2605:265
	render http://localhost:8088/ui2/assets/vendor.js:2579:122
	_firstRender http://localhost:8088/ui2/assets/vendor.js:2658:245
	renderAndCleanup http://localhost:8088/ui2/assets/vendor.js:2661:189
	_firstRender http://localhost:8088/ui2/assets/vendor.js:2658:55
	invoke http://localhost:8088/ui2/assets/vendor.js:2657:203
	ViewNodeManager_render_instrument http://localhost:8088/ui2/assets/vendor.js:1033:16
	instrument http://localhost:8088/ui2/assets/vendor.js:1067:25
	ViewNodeManager_render http://localhost:8088/ui2/assets/vendor.js:1031:376
	render http://localhost:8088/ui2/assets/vendor.js:972:160
	handleKeyword http://localhost:8088/ui2/assets/vendor.js:2518:34
	handleRedirect http://localhost:8088/ui2/assets/vendor.js:2509:4
	inline http://localhost:8088/ui2/assets/vendor.js:2528:62
	content http://localhost:8088/ui2/assets/vendor.js:2572:903
	populateNodes http://localhost:8088/ui2/assets/vendor.js:2610:181
	render http://localhost:8088/ui2/assets/vendor.js:2605:265
	render http://localhost:8088/ui2/assets/vendor.js:2579:122
	_firstRender http://localhost:8088/ui2/assets/vendor.js:2658:245
	renderAndCleanup http://localhost:8088/ui2/assets/vendor.js:2661:189
	_firstRender http://localhost:8088/ui2/assets/vendor.js:2658:55
	invoke http://localhost:8088/ui2/assets/vendor.js:2657:203
	ViewNodeManager_render_instrument http://localhost:8088/ui2/assets/vendor.js:1033:16
	instrument http://localhost:8088/ui2/assets/vendor.js:1067:25
	ViewNodeManager_render http://localhost:8088/ui2/assets/vendor.js:1031:376
	renderHTMLBarsBlock http://localhost:8088/ui2/assets/vendor.js:1071:1530
	renderBlock http://localhost:8088/ui2/assets/vendor.js:2303:8
	Renderer_prerenderTopLevelView http://localhost:8088/ui2/assets/vendor.js:1628:377
	Renderer_renderTopLevelView http://localhost:8088/ui2/assets/vendor.js:1628:622
	invoke http://localhost:8088/ui2/assets/vendor.js:626:188
	flush http://localhost:8088/ui2/assets/vendor.js:629:12
	flush http://localhost:8088/ui2/assets/vendor.js:619:457
	end http://localhost:8088/ui2/assets/vendor.js:642:385
	run http://localhost:8088/ui2/assets/vendor.js:648:288
	run http://localhost:8088/ui2/assets/vendor.js:1509:356
	success http://localhost:8088/ui2/assets/vendor.js:5118:108
	fire http://localhost:8088/ui2/assets/vendor.js:230:281
	fireWith http://localhost:8088/ui2/assets/vendor.js:235:198
	done http://localhost:8088/ui2/assets/vendor.js:555:86
	callback http://localhost:8088/ui2/assets/vendor.js:572:215


I think YARN UI 2 uses ""moment/moment-timezone"" to `guess` the timezone on a browser and convert the time with it, 
 but it has a bug which is might be related to this problem.
https://github.com/moment/moment-timezone/pull/302 
 YARN UI 2 footer shows the datetime in different timezone",13183096,"We deployed hadoop 3.1.1 on centos 7.2 serverswhose timezone isconfigured as GMT+8, the web browser time zone is GMT+8 too. yarn ui page loaded failed due to js error:


Themoment-timezone js component raised that error. This has been fixed inmoment-timezone v0.5.1([see|https://github.com/moment/moment-timezone/issues/294).]We need to updatemoment-timezone version accordingly 
 [UI2] YARN UI2 page loading failed due to js error under some time zone configuration",yes
13161982,"Hive Schema Tool can't create catalogs if the db is postgres due to trying to run an aggregated query in FOR UPDATE mode which is not supported. 
 Hive Schema Tool createCatalog command is not working with postgres DB",13161629,"Attempts to use the schematool --createCatalog option when the metastore is using Oracle result in


SQL Error code: 1786
org.apache.hadoop.hive.metastore.HiveMetaException: Failed to add catalog
at org.apache.hive.beeline.HiveSchemaTool.createCatalog(HiveSchemaTool.java:941)
at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1459)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.util.RunJar.run(RunJar.java:308)
at org.apache.hadoop.util.RunJar.main(RunJar.java:222)
Caused by: java.sql.SQLSyntaxErrorException: ORA-01786: FOR UPDATE of this query expression is not allowed

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)
at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)
at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)
at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)
at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)
at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)
at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)
at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:30)
at oracle.jdbc.driver.T4CStatement.executeForDescribe(T4CStatement.java:762)
at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)
at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)
at oracle.jdbc.driver.OracleStatement.executeQuery(OracleStatement.java:1309)
at oracle.jdbc.driver.OracleStatementWrapper.executeQuery(OracleStatementWrapper.java:422)
at org.apache.hive.beeline.HiveSchemaTool.createCatalog(HiveSchemaTool.java:926)
... 7 more
*** schemaTool failed ***
 
 schematool  --createCatalog option fails when using Oracle as the RDBMS",yes
13140634,"After YARN-7139, the new application can get correct queue name in its submission context. We need to do the same thing for application recovering. 


      if (isAppRecovering) {
        if (LOG.isDebugEnabled()) {
          LOG.debug(applicationId
              + "" is recovering. Skip notifying APP_ACCEPTED"");
        }
      } else {
        // During tests we do not always have an application object, handle
        // it here but we probably should fix the tests
        if (rmApp != null && rmApp.getApplicationSubmissionContext() != null) {
          // Before we send out the event that the app is accepted is
          // to set the queue in the submissionContext (needed on restore etc)
          rmApp.getApplicationSubmissionContext().setQueue(queue.getName());
        }
        rmContext.getDispatcher().getEventHandler().handle(
            new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));
      }


We can do it by moving the rmApp.getApplicationSubmissionContext().setQueue block out of the if-else block. cc Wilfred Spiegelenburg. 
 Reset the queue name in submission context while recovering an application",13302114,"How to reproduce:

create external table ts_pq (ts timestamp) stored as parquet;
insert into ts_pq values ('1998-10-03 09:58:31.231');
create external table ts_pq_2 (ts bigint) stored as parquet location '<location of ts_pq>';
select * from ts_pq_2;
The following exception occurs during the select:
Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.TimestampWritableV2 cannot be cast to org.apache.hadoop.io.LongWritable

 
 Remove unused resourcemanager test dependeny in hadoop-yarn-client ",No
13154975,"There is momentarily issue between app ACCEPTED to RUNNING duration. If AM launching is delayed, then state is not updated in ATS. 
 Application State is not updated to ATS if AM launching is delayed.",13190008,"When testing namespace not enabled account using Oauth, some tests were skipped. So need to update the tests. 
 ABFS: Enable some tests for namespace not enabled account using OAuth",No
13206658,"Looking at the Hadoop source code, there are a few places where the code assumes the user name can be acquired from Java's system property user.name.
For example,
FileSystem

/** Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   */
  public Path getHomeDirectory() {
    return this.makeQualified(
        new Path(USER_HOME_PREFIX + ""/"" + System.getProperty(""user.name"")));
  }


This is incorrect, as in a Kerberized environment, a user may login as a user principal different from its system login account.
It would be better to use UserGroupInformation.getCurrentUser().getShortUserName(), similar to HDFS-12485.
Unfortunately, I am seeing this improper use in Yarn, HDFS federation SFTPFilesystem and Ozone code (tests are ignored)
The impact should be small, since it only affects the case where system is Kerberized and that the user principal is different from system login account. 
 Replace incorrect use of system property user.name",13251158,"Reproducer


set hive.query.results.cache.enabled=false;
set hive.optimize.ppd.storage=true;
set hive.optimize.index.filter=true;

set hive.tez.bucket.pruning=true; 


CREATE TABLE `test_table`(                 
   `col_1` int,                                     
   `col_2` string,                                  
   `col_3` string)                                  
 CLUSTERED BY (                                     
   col_1)                                           
 INTO 4 BUCKETS;                                     

insert into test_table values(1, 'one', 'ONE'), (2, 'two', 'TWO'), (3,'three','THREE'),(4,'four','FOUR');

select * from test_table;

explain select col_1, col_2, col_3 from test_table where col_1 <> 2 order by col_2;
select col_1, col_2, col_3 from test_table where col_1 <> 2 order by col_2;



Above sql query produce zero rows. 
 Turning on hive.tez.bucket.pruning produce wrong results",No
13210925,"Environment/Configuration

hbase.wal.dir: Configured to be on hdfs
hbase.rootdir: Configured to be on s3

In replication scenario, while trying to get archived log dir (using method WALEntryStream.java#L314) we get the following exception:


2019-01-21 17:43:55,440 ERROR [RS_REFRESH_PEER-regionserver/host2:22222-1.replicationSource,2.replicationSource.wal-reader.host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1,2] regionserver.ReplicationSource: Unexpected exception in RS_REFRESH_PEER-regionserver/host2:22222-1.replicationSource,2.replicationSource.wal-reader.host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1,2 currentPath=hdfs://dummy_path/hbase/WALs/host2,22222,1548063439555/host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1.1548063492594
java.lang.IllegalArgumentException: Wrong FS: s3a://xxxxxx/hbase128/oldWALs/host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1.1548063492594, expected: hdfs://dummy_path
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:781)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:246)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1622)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1619)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1634)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:465)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1742)
	at org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.getArchivedLog(WALEntryStream.java:319)
	at org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.resetReader(WALEntryStream.java:404)
	at org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.reset(WALEntryStream.java:161)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader.run(ReplicationSourceWALReader.java:148)
2019-01-21 17:43:55,444 ERROR [RS_REFRESH_PEER-regionserver/host2:22222-1.replicationSource,2.replicationSource.wal-reader.host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1,2] regionserver.HRegionServer: ***** ABORTING region server host2,22222,1548063439555: Unexpected exception in RS_REFRESH_PEER-regionserver/host2:22222-1.replicationSource,2.replicationSource.wal-reader.host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1,2 *****
java.lang.IllegalArgumentException: Wrong FS: s3a://xxxxxx/hbase128/oldWALs/host2%2C22222%2C1548063439555.host2%2C22222%2C1548063439555.regiongroup-1.1548063492594, expected: hdfs://dummy_path
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:781)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:246)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1622)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1619)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1634)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:465)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1742)
	at org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.getArchivedLog(WALEntryStream.java:319)
	at org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.resetReader(WALEntryStream.java:404)
	at org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.reset(WALEntryStream.java:161)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader.run(ReplicationSourceWALReader.java:148)





 Current code is:


  private Path getArchivedLog(Path path) throws IOException {
    Path rootDir = FSUtils.getRootDir(conf);

    // Try found the log in old dir
    Path oldLogDir = new Path(rootDir, HConstants.HREGION_OLDLOGDIR_NAME);
    Path archivedLogLocation = new Path(oldLogDir, path.getName());
    if (fs.exists(archivedLogLocation)) {
      LOG.info(""Log "" + path + "" was moved to "" + archivedLogLocation);
      return archivedLogLocation;
    }
    .
    .
    .
    return path;
  }


It considers root dir while we should use wal dir. 
 RS aborts while performing replication with wal dir on hdfs, root dir on s3",13208179,"Scan and fix code base to use new way of instantiating WAL File System. 
https://issues.apache.org/jira/browse/HBASE-21457?focusedCommentId=16734688&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16734688 
 Address WAL filesystem issues",yes
13193505,"A real problem in our production cluster. A user point that his table's data can't be replicate to the peer cluster. Then we start to debug the reason. We checked the replication scope, checked the replication wal entry filter, and check the namespace,tablecfs config. But didn't found any problem. We enabled the RS's debug log to find the reason. Finally, we found use use put with skip wal to write data. But it taked a long time... Our replication use wal to replicate data. So the data can't be replicated to peer cluster. I thought throw a exception may be better for user if the table's replication scope is not 0. (as 0 means not replicated). 
 Throw exception when user put data with skip wal to a table which may be replicated",13177587,"When i click FSCK on Router Web UI Utilities, i got errors

HTTP ERROR 404
Problem accessing /fsck. Reason:
    NOT_FOUND
Powered by Jetty://
I deep into the source code and find that fsck is not supported currently, So i think we should remove FSCK from Router Web UI 
 RBF: Remove FSCK from Router Web UI",No
13134981,"TestRMWebServiceAppsNodelabel#testAppsRunning is failing since YARN-7817. 
 Fix UT failure TestRMWebServiceAppsNodelabel#testAppsRunning",13134314,"org.junit.ComparisonFailure: partition amused 
 Expected :{""memory"":1024,""vCores"":1}
 Actual :{""memory"":1024,""vCores"":1,""resourceInformations"":{""resourceInformation"":[

{""maximumAllocation"":9223372036854775807,""minimumAllocation"":0,""name"":""memory-mb"",""resourceType"":""COUNTABLE"",""units"":""Mi"",""value"":1024}

,{""maximumAllocation"":9223372036854775807,""minimumAllocation"":0,""name"":""vcores"",""resourceType"":""COUNTABLE"",""units"":"""",""value"":1}]}}
 <Click to see difference



 at org.junit.Assert.assertEquals(Assert.java:115)
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServiceAppsNodelabel.verifyResource(TestRMWebServiceAppsNodelabel.java:218)
  at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServiceAppsNodelabel.testAppsRunning(TestRMWebServiceAppsNodelabel.java:201)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:497)
  at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
  at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
  at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
  at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
  at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
  at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
  at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
  at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
  at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
  at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
  at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
  at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
  at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) 
 TestRMWebServiceAppsNodelabel.testAppsRunning is failing",yes
13157703,"

CREATE VIEW view_s1 AS select 1;

-- FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table

 
 Create View - Table not found _dummy_table",13255422,"Mentioned on HDFS-13596
Incompatible StringTable changes cause downgrade from 3.2.0 to 2.7.2 failed
commit message as follow, but issue not found

commit 8a41edb089fbdedc5e7d9a2aeec63d126afea49f
Author: Vinayakumar B <vinayakumarb@apache.org>
Date:   Mon Oct 15 15:48:26 2018 +0530
    Fix potential FSImage corruption. Contributed by Daryn Sharp.  
 Downgrade Failed from 3.2.0 to 2.7 because of incompatible stringtable ",No
13172391,"Could be reproduced by the following:


set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
set hive.optimize.bucketingsorting=true;

create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS;
insert into bucket1 values (1, 'abc'), (3, 'abc');
select * from bucket1;

+-------------+--------------+
| bucket1.id  | bucket1.val  |
+-------------+--------------+
| 3           | abc          |
| 1           | abc          |
+-------------+--------------+

create table bucket2 like bucket1;

insert overwrite table bucket2 select * from bucket1;
select * from bucket2;

+-------------+--------------+
| bucket2.id  | bucket2.val  |
+-------------+--------------+
| 1           | abc          |
+-------------+--------------+


 
 Inserting from bucketed table can cause data loss, if the source table contains empty buckets",13215437,"Missing quotes in sql string is causing sql execution error for postgres.



metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(201)) - MetaException(message:Unable to update transaction database org.postgresql.util.PSQLException: ERROR: relat
ion ""database_params"" does not exist
Position: 25
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2284)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2003)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:200)
at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:424)
at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:321)
at org.postgresql.jdbc.PgStatement.executeQuery(PgStatement.java:284)
at com.zaxxer.hikari.pool.ProxyStatement.executeQuery(ProxyStatement.java:108)
at com.zaxxer.hikari.pool.HikariProxyStatement.executeQuery(HikariProxyStatement.java)
at org.apache.hadoop.hive.metastore.txn.TxnHandler.updateReplId(TxnHandler.java:907)
at org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn(TxnHandler.java:1023)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.commit_txn(HiveMetaStore.java:7703)
at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
at com.sun.proxy.$Proxy39.commit_txn(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$commit_txn.getResult(ThriftHiveMetastore.java:18730)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$commit_txn.getResult(ThriftHiveMetastore.java:18714)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:636)
at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:631)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:631)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
)

 
 Hive replication to a target with hive.strict.managed.tables enabled is failing when used HMS on postgres.",No
13193654,"From parent issue, Allan Yang suggests that we not give up on assign unless there a change – an SCP triggers failure – or at the extreme, an operator intervenes. This jibes w/ how we're thinking about assign (or to put it another way, we have no handling for the case where we exhaust retries). 
 Set hbase.assignment.maximum.attempts to Long.MAX",13256532,"You can't run s3guard prune with role DTs as we don't create it with permissons to do so.
I think it may actually be useful to have an option where we don't restrict the role. This doesn't just help with debugging, it would let things like SQS integration pick up the creds from S3A. 
 IAM role created by S3A DT doesn't include DynamoDB scan",No
13253234,"When we do source package of hadoop, it should not contain submarine project/code. 
 Exclude submarine from hadoop source build",13261775,"Fair Scheduler supports limiting the number of applications that a particular user can submit:

<user name=""someuser"">
  <maxRunningApps>10</maxRunningApps>
</user>


Capacity Scheduler does not have an exact equivalent. 
 Capacity scheduler: add support for limiting maxRunningApps per user",No
13253812,"The same with what described in YARN-4624
Scenario:
 =======
if not configure all queue's capacity to nodelabel even the value is 0, start cluster and access capacityscheduler page.
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock.renderQueueCapacityInfo(CapacitySchedulerPage.java:163)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock.renderLeafQueueInfoWithPartition(CapacitySchedulerPage.java:108)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock.render(CapacitySchedulerPage.java:97)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:243)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)
        at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$LI.__(Hamlet.java:7709)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueBlock.render(CapacitySchedulerPage.java:342)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:243)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)
        at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$LI.__(Hamlet.java:7709)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock.render(CapacitySchedulerPage.java:513)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:243)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)
        at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)
        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)
        at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:216)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.scheduler(RmController.java:86)
 
 NPE while accessing Scheduler UI",13245938,"I found incomplete queue info shownonscheduler page and NPE in RM log when rendering the info table of leaf queue in non-accessible partitions.

Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock.renderQueueCapacityInfo(CapacitySchedulerPage.java:163)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock.renderLeafQueueInfoWithPartition(CapacitySchedulerPage.java:108)
        at org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock.render(CapacitySchedulerPage.java:97)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:243)


The direct cause is that PartitionQueueCapacitiesInfo of leaf queues in non-accessible partitions are incomplete(part of fields are null such as configuredMinResource/configuredMaxResource/effectiveMinResource/effectiveMaxResource) but some places in CapacitySchedulerPage don't consider that. 
 NPE when rendering the info table of leaf queue in non-accessible partitions",yes
13181876,"I've created a script that will put continuous put one record (size 2.5KB) and flush immediately – in middle am doing compaction at regular intervals. Rate of flushes are around 20flushes/sec. After some time, my RS aborted and never came up back
 with the following error


2018-08-29 11:34:34,183 DEBUG [flush-table-TestTable_client_1258196-thread-1] regionserver.RSRpcServices: Closing region operation on TestTable_client_1,32816,1535513244999.762a3e633b03e5f847f357aca28768d0.
2018-08-29 11:34:34,183 INFO  [RpcServer.FifoWFPBQ.default.handler=49,queue=4,port=16040] regionserver.RSRpcServices: flush table task succeed 1, failed 10.
2018-08-29 11:34:34,280 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=230, memsize=4.2 K, hasBloomFilter=false, into tmp file hdfs://hacluster/hbase/data/hbase/meta/1588230740/.tmp/1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,290 INFO  [MemStoreFlusher.0] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,291 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://hacluster/hbase/data/hbase/meta/1588230740/.tmp/1cf1deee293848b0bea08940696dbd2a as hdfs://hacluster/hbase/data/hbase/meta/1588230740/info/1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,304 INFO  [MemStoreFlusher.0] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 1cf1deee293848b0bea08940696dbd2a
2018-08-29 11:34:34,304 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://hacluster/hbase/data/hbase/meta/1588230740/info/1cf1deee293848b0bea08940696dbd2a, entries=13, sequenceid=230, filesize=6.6 K
2018-08-29 11:34:34,307 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: ABORTING region server host-xxxx,16040,1535454741321: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: hbase:meta,,1
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2578)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2255)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2217)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2108)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:2034)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:505)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:475)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:263)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:826)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:1117)
        at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1090)
        at org.apache.hadoop.hbase.regionserver.HStore.access$700(HStore.java:120)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2450)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2533)
        ... 9 more
2018-08-29 11:34:34,307 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.AccessController, org.apache.hadoop.hbase.

 
 NullPointerException in StoreScanner",13179687,"I see the following NPE in the region server log for a table that is taking heavy writes. 
I am not sure how the memStoreScanners variable gets set to null.


2018-08-17 19:59:23,682 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem - Committing store file ...
2018-08-17 19:59:23,684 INFO  [MemStoreFlusher.1] regionserver.HStore - Added hdfs://...., entries=919170, sequenceid=275114, filesize=22.6 M
2018-08-17 19:59:23,689 FATAL [MemStoreFlusher.1] regionserver.HRegionServer - ABORTING region server iotperf1dchbase1a-dnds22-2-prd.eng.sfdc.net,60020,1533915690501: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: ......
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2581)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2258)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2220)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2106)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:2031)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:508)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:478)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:76)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:264)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.updateReaders(StoreScanner.java:827)
        at org.apache.hadoop.hbase.regionserver.HStore.notifyChangedReadersObservers(HStore.java:1160)
        at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1133)
        at org.apache.hadoop.hbase.regionserver.HStore.access$900(HStore.java:120)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2487)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2536)
        ... 9 more
2018-08-17 19:59:23,692 FATAL [MemStoreFlusher.1] regionserver.HRegionServer - RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.AccessController, org.apache.phoenix.coprocessor.ScanRegionObserver, org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver, org.apache.phoenix.hbase.index.Indexer, org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver, org.apache.hadoop.hbase.security.token.TokenProvider, org.apache.phoenix.coprocessor.ServerCachingEndpointImpl]

 
 NPE in StoreScanner.updateReaders causes RS to crash ",yes
13181726,"Currently, every single call to NameNode will be synced, in the sense that NameNode will not process it until state id catches up. But in certain cases, we would like to bypass this check and allow the call to return immediately, even when the server id is not up to date. Onecase could be the to-be-added new API inHDFS-13749that request for current state id. Othersmay include calls that do not promise real timeresponses such as getContentSummary. This Jira is to add the mechanism to allow certain calls to bypass sync. 
 Add mechanism to allow certain RPC calls to bypass sync",13181465,"Currently the implementation of msync added in HDFS-13767 waits until the server has caught up to the client-specified transaction ID regardless of what the inbound RPC is. This particularly causes problems for ObserverReadProxyProvider (see HDFS-13779) when we try to fetch the state from an observer/standby; this should be a quick operation, but it has to wait for the node to catch up to the most current state. I initially thought all HAServiceProtocol methods should thus be excluded from the wait period, but actually I think the right approach is that only ClientProtocol methods should be subjected to the wait period. I propose that we can do this via an annotation on client protocol which can then be checked within ipc.Server. 
 Only some protocol methods should perform msync wait",yes
13244618,"in deleteSubtree(path), the DynamoDB metastore walks down the tree, returning elements to delete. But it will delete parent entries before children, so if an operation fails partway through, there will be orphans
Better: DescendantsIterator to return all the leaf nodes before their parents so the deletion is done bottom up
Also: push the deletions off into their own async queue/pool so that they don't become the bottleneck on the process 
 DynamoDBMetaStore deleteSubtree to delete leaf nodes first",13129736,"In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.
Simple test case to reproduce this issue

set hive.vectorized.execution.enabled=true;
create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;
insert into parquettable values (-104, 25), (-112, 24), (54, 9);
select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) < 50 order by diff desc;
+-------+-----+-------+
|  t1   | t2  | diff  |
+-------+-----+-------+
| -104  | 25  | 127   |
| -112  | 24  | 120   |
| 54    | 9   | 45    |
+-------+-----+-------+


When vectorization is turned off the same query produces only one row. 
 Vectorized execution handles overflows in a different manner than non-vectorized execution",No
13201414,"Container resource monitoring is not stopped during the reinitialization process, and this prevents the NM from obtaining updated process tree information when the container starts running again. I observed a reinitialized container go from RUNNING to REINITIALIZING to REINITIALIZING_AWAITING_KILL to SCHEDULED to RUNNING. Container monitoring was then started for a second time, but since the trackingContainers entry had already been initialized for the container, ContainersMonitor skipped finding the new PID and IP for the container. A possible solution would be to stop the container monitoring in the reinitialization process so that the process tree information would be initialized properly when monitoring is restarted. When the same container was stopped by the NM later, the NM did not kill the container, and the service AM received an unexpected event (stop at reinitializing). 
 NM and service AM don't have updated status for reinitialized containers",13197032,"I setup a single node cluster, then trying to add node-attributes with CLI,
first I tried:


./bin/yarn nodeattributes -add localhost:hostname(STRING)=localhost


this command returns exit code 0, however the node-attribute was not added.
Then I tried to replace ""localhost"" with the host ID, and it worked.
We need to ensure the command fails with proper error message when adding was not succeed.
Similarly, when I remove a node-attribute that doesn't exist, I still get return code 0. 
 Usability improvements node-attributes CLI",No
13225824,"In our test environment, we occasionally encounter this failure:

2019-04-03 12:49:32 [INFO] Running org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestApplicationLifetimeMonitor
2019-04-03 12:53:08 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 215.535 s <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestApplicationLifetimeMonitor
2019-04-03 12:53:08 [ERROR] testApplicationLifetimeMonitor[0](org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestApplicationLifetimeMonitor)  Time elapsed: 34.244 s  <<< FAILURE!
2019-04-03 12:53:08 java.lang.AssertionError: Application killed before lifetime value
2019-04-03 12:53:08 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestApplicationLifetimeMonitor.testApplicationLifetimeMonitor(TestApplicationLifetimeMonitor.java:218)
2019-04-03 12:53:08 


The root cause is the condition here:

        Assert.assertTrue(""Application killed before lifetime value"",
            totalTimeRun > maxLifetime);


However, there are two problems with this condition:
 1. Logically it's not correct. In fact, since the app should be killed after 30 seconds, one would expect to see totalTimeRun = maxLifetime. Due to some asynchronicity and rounding, most of the time totalTimeRun ends up being 31.
2. Sometimes the application is killed fast enough and totalTimeRun is 30, but this is correct, because in setUpCSQueue we set the queue lifetime:

    csConf.setMaximumLifetimePerQueue(
        CapacitySchedulerConfiguration.ROOT + "".default"", maxLifetime);
    csConf.setDefaultLifetimePerQueue(
        CapacitySchedulerConfiguration.ROOT + "".default"", defaultLifetime);


A more proper condition is:

Assert.assertTrue(""Application killed before lifetime value"",
            totalTimeRun >= maxLifetime);


The assertion message in the next line is also misleading:

        Assert.assertTrue(
            ""Application killed before lifetime value "" + totalTimeRun,
            totalTimeRun < maxLifetime + 10L);


If it false, it means that the application is killed after 40 seconds, which exceeds both the app's lifetime (40s) and that of the queue (30s).

        Assert.assertTrue(
            ""Application killed after queue/app lifetime value: "" + totalTimeRun,
            totalTimeRun < maxLifetime + 10L);


We can be even be stricter, since we expect a kill almost immediately after 30 seconds:

        Assert.assertTrue(
            ""Application killed too late: "" + totalTimeRun,
            totalTimeRun < maxLifetime + 2L);


where we allow a 2 second tolerance. 
 Flaky test testApplicationLifetimeMonitor",13223370,"TestApplicationLifetimeMonitor#testApplicationLifetimeMonitor fails intermittent. 


[ERROR] testApplicationLifetimeMonitor[0](org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestApplicationLifetimeMonitor) Time elapsed: 34.75 s <<< FAILURE! java.lang.AssertionError: Application killed before lifetime value at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.assertTrue(Assert.java:41) at org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestApplicationLifetimeMonitor.testApplicationLifetimeMonitor(TestApplicationLifetimeMonitor.java:209) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)


As per testcase logs, submittime is 1553240813597 and finishtime is 1553240844372. The testcase does (finishtime - submittime) / 1000 = 30775 / 1000 = 30 and loses the decimal, 775 ms.


2019-03-22 07:47:24,357 INFO  [Ping Checker] util.AbstractLivelinessMonitor (AbstractLivelinessMonitor.java:run(149)) - Expired:application_1553240811329_0004_LIFETIME Timed out after 0 secs

2019-03-22 07:47:24,384 INFO  [AsyncDispatcher event handler] resourcemanager.RMAppManager$ApplicationSummary (RMAppManager.java:logAppSummary(219)) - appId=application_1553240811329_0004,name=,user=jenkins,queue=default,state=KILLED,trackingUrl=http://869e1f448cdd:8088/cluster/app/application_1553240811329_0004,appMasterHost=N/A,submitTime=1553240813597,startTime=1553240813604,launchTime=0,finishTime=1553240844372,finalStatus=KILLED,memorySeconds=0,vcoreSeconds=0,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=YARN,resourceSeconds=0 MB-seconds\, 0 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds


Testcase succeeds only when the seconds taken is above 30L.


 long totalTimeRun =
            (app4.getFinishTime() - app4.getSubmitTime()) / 1000;
 Assert.assertTrue(""Application killed before lifetime value"",
            totalTimeRun > maxLifetime);

 
 TestApplicationLifetimeMonitor#testApplicationLifetimeMonitor fails intermittent",yes
13172824,"From a local UT check against 2.1.0-RC1, HMaster failed to initialize before time out. Checking the test log we could see below message:

2018-07-17 20:06:37,142 DEBUG [Thread-4003] client.RpcRetryingCallerImpl(131): Call exception, tries=6, retries=6, started=4173 ms ago, cancelled=false, msg=java.io.IOException: Inject error
        at org.apache.hadoop.hbase.master.procedure.TestProcedurePriority$MyCP.preGetOp(TestProcedurePriority.java:92)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:841)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:838)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult.callObserver(CoprocessorHost.java:540)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.execOperation(CoprocessorHost.java:614)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGet(RegionCoprocessorHost.java:838)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2520)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2460)
        at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:41998)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:409)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)
, details=row 'hbase:namespace' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=hdpdevm1.et2sqa.tbsite.net,59254,1531829189215, seqNum=-1, exception=java.io.IOException: java.io.IOException: Inject error
        at org.apache.hadoop.hbase.master.procedure.TestProcedurePriority$MyCP.preGetOp(TestProcedurePriority.java:92)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19.call(RegionCoprocessorHost.java:841)
        ...
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:386)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:360)
        at org.apache.hadoop.hbase.MetaTableAccessor.getTableState(MetaTableAccessor.java:1078)
        at org.apache.hadoop.hbase.MetaTableAccessor.tableExists(MetaTableAccessor.java:403)
        at org.apache.hadoop.hbase.master.TableNamespaceManager.start(TableNamespaceManager.java:94)


In current test code we will set FAIL to true w/o checking whether namespace manager is already up, and if not lucky we will run into the above case and get a timeout.
The fix will be quite straight forward. 
 Fix Intermittent failure on TestProcedurePriority",13140575,"http://hbase.apache.org/book.html#tricks.pre-split


hbase>create 't1','f',SPLITS => ['10','20',30']


Missing a quote before the 30./ 
 Clean up inputs in JDBC PreparedStatement",No
13347280,"  
SlowDelete & SlowPut metric value should use updateDelete & updatePut API
like this 
  
 SlowDelete & SlowPut metric value should use updateDelete & updatePut ",13189463,"The hdfs checksum command computes the checksum in a distributed manner, which would take into account the block size. In other words, the block size determines how the file will be broken up.
Therefore itcan happen that the checksum command produces different outputs for the exact same file only differing in the block size: checksum(fileABlock1) + checksum(fileABlock2) != checksum(fileABlock1 + fileABlock2)
I suggest to add an option to the hdfs dfs -checksum command which would displays the block sizealong with the output, and that could also be helpful in some other cases where this piece of information is needed. 
 hdfs dfs -checksum command should optionally show block size in output",No
13300407,"TestDelegationTokenWithEncryption andTestGenerateDelegationToken always fail.

Incidentally, they don't fail in branch-2.3 and branch-2.2.

I suspect there's a regression with delegation token code, because if I comment out the following code in the test, they pass:



try (Connection conn = ConnectionFactory.createConnection(TEST_UTIL.getConfiguration())) {
 Token<? extends TokenIdentifier> token = TokenUtil.obtainToken(conn);
 UserGroupInformation.getCurrentUser().addToken(token);
}


Effectively, use Kerberos to login instead of delegation token.
The tests fail all the time (100%) in the last 29 runs: 
https://builds.apache.org/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/dashboard.html
Initially I thought this was caused by pluggable authentication (HBASE-23347), but the tests don't fail in branch-2.3 so looks unlikely. 
 TestDelegationTokenWithEncryption always fails",13157703,"

CREATE VIEW view_s1 AS select 1;

-- FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table

 
 Create View - Table not found _dummy_table",No
13249050,"   Placement of application (RMAppManager.placeApplication) is called for all the jobs during recovery. This can be ignored for the terminated jobs.


    at org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule.getPlacementForApp(AppNameMappingPlacementRule.java:193)
        at org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager.placeApplication(PlacementManager.java:66)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.placeApplication(RMAppManager.java:867)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:421)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:410)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:637)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1536)


 
 [backport] HBASE-22778 upgrade jackson-databind to 2.9.9.2",13246710,"Avoid Jackson versions and dependencies with known CVEs 
 Upgrade jackson dependencies in branch-1",yes
13167227,"When a precommit run fails due to license issues, we get pointed to a file in our maven logs:

/testptch/hbase/hbase-assembly/target/maven-shared-archive-resources/META-INF/LICENSE


But we don't have that file saved, so we don't know what the actual failure was. So we should save that in our build artifacts. Or maybe we can print a snippet from that file directly into the maven log. Both would be acceptable. 
 precommit should archive generated LICENSE file",13149093,"This serves as a symbolic link under YARN-7055, to YARN-7213 where all sub-tasks are included. 
 Support HBase 2.0.0-beta1 as ATSv2 backend",No
13266717,"The test on setup because AbstractAbfsIntegrationTest assumes the container to be exists. 
 Fix test failure of ITestAzureBlobFileSystemCLI.testMkdirRootNonExistentContainer",13266504,"Test added as part ofHADOOP-16138 :testMkdirRootNonExistentContainer() fails at setup when run with a namespace enabled account (as GetAclStatus call gets called in setup. For a non-namespace account, GetAclStatus being an invalid operation will return 400 Bad request which is handled in base test class.)


Operation failed: ""The specified filesystem does not exist."", 404, HEAD, https://snvijayacontracttest.dfs.core.windows.net/abfs-testcontainer-998ff709-31b7-4d9e-bbaa-ae4c026d8500//?upn=false&action=getAccessControl&timeout=90Operation failed: ""The specified filesystem does not exist."", 404, HEAD, https://snvijayacontracttest.dfs.core.windows.net/abfs-testcontainer-998ff709-31b7-4d9e-bbaa-ae4c026d8500//?upn=false&action=getAccessControl&timeout=90
at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:138) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:516) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAclStatus(AbfsClient.java:499) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getIsNamespaceEnabled(AzureBlobFileSystemStore.java:216) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getIsNamespaceEnabled(AzureBlobFileSystem.java:1108) at org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest.setup(AbstractAbfsIntegrationTest.java:137) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)

Even when run over with a non namespace enabled account, the test teardown has failures, though not reported as a test failure.

Operation failed: ""The specified filesystem does not exist."", 404, DELETE, https://snvijayaabfsnons.dfs.core.windows.net/abfs-testcontainer-bf33194a-6fcb-46b9-a0fa-7f458e9a6564?resource=filesystem&timeout=90, FilesystemNotFound, ""The specified filesystem does not exist. RequestId:48c592a5-901f-0049-4671-940f4b000000 Time:2019-11-06T07:09:44.5098782Z""Operation failed: ""The specified filesystem does not exist."", 404, DELETE, https://snvijayaabfsnons.dfs.core.windows.net/abfs-testcontainer-bf33194a-6fcb-46b9-a0fa-7f458e9a6564?resource=filesystem&timeout=90, FilesystemNotFound, ""The specified filesystem does not exist. RequestId:48c592a5-901f-0049-4671-940f4b000000 Time:2019-11-06T07:09:44.5098782Z"" 
at org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:138) at org.apache.hadoop.fs.azurebfs.services.AbfsClient.deleteFilesystem(AbfsClient.java:225) at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.deleteFilesystem(AzureBlobFileSystemStore.java:342) at org.apache.hadoop.fs.azurebfs.AbstractAbfsIntegrationTest.teardown(AbstractAbfsIntegrationTest.java:171) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:748)

 
 ABFS: Fix testcase added for HADOOP-16138 for namespace enabled account",yes
13142021,"When i read the addErasureCodingPolicies()ofFSNamesystemclass in namenode, i found the following codeonly used last ec policy name for logAuditEvent, i think this audit log can'ttrack whole policies for theadd multiple erasure coding policies to the ErasureCodingPolicyManager.Thanks.
FSNamesystem.java

try {
      checkOperation(OperationCategory.WRITE);
      checkNameNodeSafeMode(""Cannot add erasure coding policy"");
      for (ErasureCodingPolicy policy : policies) {
        try {
          ErasureCodingPolicy newPolicy =
              FSDirErasureCodingOp.addErasureCodingPolicy(this, policy,
                  logRetryCache);
          addECPolicyName = newPolicy.getName();
          responses.add(new AddErasureCodingPolicyResponse(newPolicy));
        } catch (HadoopIllegalArgumentException e) {
          responses.add(new AddErasureCodingPolicyResponse(policy, e));
        }
      }
      success = true;
      return responses.toArray(new AddErasureCodingPolicyResponse[0]);
    } finally {
      writeUnlock(operationName);
      if (success) {
        getEditLog().logSync();
      }
      logAuditEvent(success, operationName,addECPolicyName, null, null);
    }


 
 Log audit event only used last EC policy name when add multiple policies from file ",13142020,"When i read the addErasureCodingPolicies()ofFSNamesystemclass in namenode, i found the following codeonly used last ec policy name for logAuditEvent, i think this audit log can'ttrack whole policies for theadd multiple erasure coding policies to the ErasureCodingPolicyManager.Thanks.
FSNamesystem.java

try {
      checkOperation(OperationCategory.WRITE);
      checkNameNodeSafeMode(""Cannot add erasure coding policy"");
      for (ErasureCodingPolicy policy : policies) {
        try {
          ErasureCodingPolicy newPolicy =
              FSDirErasureCodingOp.addErasureCodingPolicy(this, policy,
                  logRetryCache);
          addECPolicyName = newPolicy.getName();
          responses.add(new AddErasureCodingPolicyResponse(newPolicy));
        } catch (HadoopIllegalArgumentException e) {
          responses.add(new AddErasureCodingPolicyResponse(policy, e));
        }
      }
      success = true;
      return responses.toArray(new AddErasureCodingPolicyResponse[0]);
    } finally {
      writeUnlock(operationName);
      if (success) {
        getEditLog().logSync();
      }
      logAuditEvent(success, operationName,addECPolicyName, null, null);
    }


 
 Audit log all EC policy names during addErasureCodingPolicies",yes
13188585,"This issue is about adding a lock picker to the HbckService
Over the w/e I had interesting case where an enable failed – a subprocedure ran into an exclusive lock (I think) – and then the parent enabletabled tried rollback. The rollback threw CODE-BUG because some subprocedures were in unrollbackable states.... so we ended up skipping out of the enable table procedure. The enable table procedure was marked ROLLBACKED... so it got GC'd. But the exclusive lock it had on the table stayed in place.
The above has to be fixed but for the future, we need way to kill locks otherwise only alternative if removing master proc wal files – which is a bigger pain restoring good state. 
 [hbck2] AMv2 Lock Picker",13244776,"As discussed in the following thread, we can deprecate / remove OfflineMetaRepair.
https://lists.apache.org/thread.html/f122efdc79be541d678e22cf8cf573352ab468159596d820c38bf84b@%3Cdev.hbase.apache.org%3E
Maybe we can deprecate in 2.x and remove in 3.0. 
 Deprecate / Remove OfflineMetaRepair in hbase-2+",No
13160630,"Every time we invoke INodesInPath#toString(), we will trigger one time byte-to-string transform operation.


private String toString(boolean vaildateObject) {
    if (vaildateObject) {
      validate();
    }

    final StringBuilder b = new StringBuilder(getClass().getSimpleName())
        .append("": path = "").append(DFSUtil.byteArray2PathString(path))
        .append(""\n  inodes = "");
        ...
}


But actually we only need to do at most one time. In INodesInPath,it has defined one String type variable named pathname for storing the path string value. Here we can use getPath() to replace DFSUtil.byteArray2PathString(path). 
 Reduce unnecessary byte-to-string transform operation in INodesInPath#toString",13245654,"Only for 3.0.0. Remove the methods which mark deprecated inHBASE-22673. 
 Remove the deprecated methods in Hbck interface",No
13202193,"I was looking into some compile profiles for tables with lots of columns; and it turned out that thrift 0.9.3 is allocating a List during every hashcode calculation; but luckily THRIFT-2877 is improving on that - so I propose to upgrade to at least 0.10.0  
 Upgrade thrift to at least 0.10.0",13308116,"Hive has been using thrift 0.9.3 for a long time. We might be able to avail new features like deprecation support etc in the newer releases of thrift. But this impacts interoperability between older clients and newer servers. We need to assess what can break atleast for the purposes of documenting before we make this change. 
 Upgrade thrift version in hive",yes
13212599,"BackportHADOOP-15549 to branch-3.1 to fix IllegalArgumentException:

02:44:34.707 ERROR org.apache.hadoop.hive.ql.exec.Task: Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'
java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.
	at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:116)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:109)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:102)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:475)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:454)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:369)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:151)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:730)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:221)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:220)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:266)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:719)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:709)
	at org.apache.spark.sql.hive.StatisticsSuite.createNonPartitionedTable(StatisticsSuite.scala:719)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$2(StatisticsSuite.scala:822)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:284)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:283)
	at org.apache.spark.sql.StatisticsCollectionTestBase.withTable(StatisticsCollectionTestBase.scala:40)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1(StatisticsSuite.scala:821)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$testAlterTableProperties$1$adapted(StatisticsSuite.scala:820)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.hive.StatisticsSuite.testAlterTableProperties(StatisticsSuite.scala:820)
	at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$70(StatisticsSuite.scala:851)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:104)
	at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)
	at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)
	at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)
	at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)
	at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)
	at org.scalatest.Suite.run(Suite.scala:1147)
	at org.scalatest.Suite.run$(Suite.scala:1129)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)
	at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)
	at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)
	at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:53)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:53)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1340)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1334)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1031)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
	Suppressed: java.io.IOException: Failed to use org.apache.hadoop.mapred.LocalClientProtocolProvider due to error: 
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:148)
		... 78 more
	Caused by: org.apache.commons.configuration2.ex.ConfigurationRuntimeException: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:463)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:479)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:492)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResultInstance(BasicConfigurationBuilder.java:447)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.createResult(BasicConfigurationBuilder.java:417)
		at org.apache.commons.configuration2.builder.BasicConfigurationBuilder.getConfiguration(BasicConfigurationBuilder.java:285)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.loadFirst(MetricsConfig.java:119)
		at org.apache.hadoop.metrics2.impl.MetricsConfig.create(MetricsConfig.java:98)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.configure(MetricsSystemImpl.java:478)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:188)
		at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:163)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:62)
		at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:58)
		at org.apache.hadoop.mapred.LocalJobRunnerMetrics.create(LocalJobRunnerMetrics.java:45)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:771)
		at org.apache.hadoop.mapred.LocalJobRunner.<init>(LocalJobRunner.java:764)
		at org.apache.hadoop.mapred.LocalClientProtocolProvider.create(LocalClientProtocolProvider.java:42)
		at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:130)
		... 78 more
	Caused by: java.lang.IllegalArgumentException: Cannot invoke org.apache.commons.configuration2.AbstractConfiguration.setListDelimiterHandler on bean class 'class org.apache.commons.configuration2.PropertiesConfiguration' - argument type mismatch - had objects of type ""org.apache.commons.configuration2.convert.DefaultListDelimiterHandler"" but expected signature ""org.apache.commons.configuration2.convert.ListDelimiterHandler""
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2195)
		at org.apache.commons.beanutils.PropertyUtilsBean.setSimpleProperty(PropertyUtilsBean.java:2108)
		at org.apache.commons.beanutils.PropertyUtilsBean.setNestedProperty(PropertyUtilsBean.java:1914)
		at org.apache.commons.beanutils.PropertyUtilsBean.setProperty(PropertyUtilsBean.java:2021)
		at org.apache.commons.beanutils.BeanUtilsBean.setProperty(BeanUtilsBean.java:1018)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initProperty(BeanHelper.java:365)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBeanProperties(BeanHelper.java:273)
		at org.apache.commons.configuration2.beanutils.BeanHelper.initBean(BeanHelper.java:192)
		at org.apache.commons.configuration2.beanutils.BeanHelper$BeanCreationContextImpl.initBean(BeanHelper.java:669)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.initBeanInstance(DefaultBeanFactory.java:162)
		at org.apache.commons.configuration2.beanutils.DefaultBeanFactory.createBean(DefaultBeanFactory.java:116)
		at org.apache.commons.configuration2.beanutils.BeanHelper.createBean(BeanHelper.java:459)
		... 95 more
	Caused by: java.lang.IllegalArgumentException: argument type mismatch
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.commons.beanutils.PropertyUtilsBean.invokeMethod(PropertyUtilsBean.java:2127)
		... 106 more


 
 Backport HADOOP-15549 to branch-3.1",13167213,"HADOOP-13660 upgraded from commons-configuration 1.x to 2.x. commons-configuration is used when parsing the metrics configuration properties file. The new builder API used in the new version apparently makes use of a bunch of very bloated reflection and classloading nonsense to achieve the same goal, and this results in a regression of >100ms of CPU time as measured by a program which simply initializes DefaultMetricsSystem.
This isn't a big deal for long-running daemons, but for MR tasks which might only run a few seconds on poorly-tuned jobs, this can be noticeable. 
 Upgrade to commons-configuration 2.1 regresses task CPU consumption",yes
13308151,"if the job is waiting for a long to acquire the lock for a long time; it would be favourable to do a quick check after lock acqusition wether there are new triggers
https://github.com/apache/hive/pull/948#discussion_r432093713 
 special_character_in_tabnames_1.q is unstable",13159283,"Without closing ctx in testMarkSuspectBlock, testIgnoreMisplacedBlock, testAppendWhileScanning, some tests fail on Windows:
[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestBlockScanner
[ERROR] Tests run: 14, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 113.398 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestBlockScanner
[ERROR] testScanAllBlocksWithRescan(org.apache.hadoop.hdfs.server.datanode.TestBlockScanner) Time elapsed: 0.031 s <<< ERROR!
java.io.IOException: Could not fully delete E:\OSS\hadoop-branch-2\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\name1
 at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1047)
 at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883)
 at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514)
 at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473)
 at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner$TestContext.<init>(TestBlockScanner.java:102)
 at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testScanAllBlocksImpl(TestBlockScanner.java:366)
 at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testScanAllBlocksWithRescan(TestBlockScanner.java:435)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
...
[INFO]
[INFO] Results:
[INFO]
[ERROR] Errors:
[ERROR] TestBlockScanner.testAppendWhileScanning:899 ╗ IO Could not fully delete E:\OS...
[ERROR] TestBlockScanner.testCorruptBlockHandling:488 ╗ IO Could not fully delete E:\O...
[ERROR] TestBlockScanner.testDatanodeCursor:531 ╗ IO Could not fully delete E:\OSS\had...
[ERROR] TestBlockScanner.testMarkSuspectBlock:717 ╗ IO Could not fully delete E:\OSS\h...
[ERROR] TestBlockScanner.testScanAllBlocksWithRescan:435->testScanAllBlocksImpl:366 ╗ IO
[ERROR] TestBlockScanner.testScanRateLimit:450 ╗ IO Could not fully delete E:\OSS\hado...
[ERROR] TestBlockScanner.testVolumeIteratorWithCaching:261->testVolumeIteratorImpl:169 ╗ IO
[ERROR] TestBlockScanner.testVolumeIteratorWithoutCaching:256->testVolumeIteratorImpl:169 ╗ IO
[INFO]
[ERROR] Tests run: 14, Failures: 0, Errors: 8, Skipped: 0 
 Enable TestMiniLlapLocalCliDriver#special_character_in_tabnames_1.q",yes
13339879,"This is a missing part of the client library for sync client on branch-2, and it is necessary when implementing meta replicas read. 
 Support scan on a specific replica",13155701,"The HashSet comparison is not working for some reason:

java.lang.AssertionError: expected: java.util.HashSet<[hb]> but was: java.util.HashSet<[hb]>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.yarn.api.resource.TestPlacementConstraintTransformations.testCardinalityConstraint(TestPlacementConstraintTransformations.java:116)

 
 YARN precommit build failing in TestPlacementConstraintTransformations",No
13149508,"start-ozone.sh calls start-dfs.sh to start the NN and DN in a ozone cluster. Starting of datanode fails because of incomplete classpaths as datanode is unable to load all the plugins.
Setting the class path to the following values does resolve the issue:


export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/ozone/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/hdsl/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/ozone/lib/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/hdsl/lib/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/cblock/*:/opt/hadoop/hadoop-3.2.0-SNAPSHOT/share/hadoop/cblock/lib/*

 
 Ozone: start-ozone.sh fail to start datanode because of incomplete classpaths",13289924,"Continuation of HADOOP-15686
Add the same log4j property to disable error log in hadoop-hdfs. 
 Suppress bogus AbstractWadlGeneratorGrammarGenerator in KMS stderr in hdfs",No
13299694,"After HDFS-12979, the prepare step of rolling upgrade does not work. This is because it added additional check for sufficient time passing since last checkpoint. Since RU rollback image creation and upload can happen any time, uploading of rollback image never succeeds. For a new cluster deployed for testing, it might work since it never checkpointed before.
It was found that this check is disabled for unit tests, defeating the very purpose of testing. 
 HDFS rollingupgrade prepare never finishes",13272843,"Image transfer from Standby NameNode to  Active silently fails on Active, without any logging and not notifying the receiver side. 
 Active NameNode should not silently fail the image transfer",yes
13326367,"When adding Hedged Reads metrics athttps://issues.apache.org/jira/browse/HBASE-12220,hedgedReadOpsInCurThread was not included.
I wonder if there was any reason to not include them. If not, I will provide pull request to include them soon.

Hedged reads use same thread pool for the original read:
https://github.com/apache/hadoop/blob/0b8464d75227fcee2c6e7f2410377b3d53d3d5f8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java#L1349-L1357
 When the executor is full and rejects the task, it will be executed in current thread, increasing this metric:
https://github.com/apache/hadoop/blob/0b8464d75227fcee2c6e7f2410377b3d53d3d5f8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java#L2913-L2918
This metric is very useful to determine if the hedged reads thread pool (dfs.client.hedged.read.threadpool.size) is undersized. 
 Add hedgedReadOpsInCurThread metric",13134917,"Looks like in union case, some code path may pass directories in newFiles. Probably legacy copyData/moveData; both seem to assume that these paths are files, but do not actually enforce it. 
 insert DML event may attempt to calculate a checksum on directories",No
13289102,"Parsing of query fails when we use single or double quotes in from/to string of translate function in 2.3*/3.1.1 version of hive.Parsing of the same query is successful in 2.1.1 version of hive.
Steps to reproduce:



CREATE TABLE test_table (data string);
INSERT INTO test_table VALUES(""d\""a\""t\""a"");
select translate(data, '""', '') from test_table;




Parsing fails with the following exception:


 NoViableAltException(355@[157:5: ( ( Identifier LPAREN )=> partitionedTableFunction | tableSource | subQuerySource | virtualTableSource )])NoViableAltException(355@[157:5: ( ( Identifier LPAREN )=> partitionedTableFunction | tableSource | subQuerySource | virtualTableSource )]) at org.antlr.runtime.DFA.noViableAlt(DFA.java:158) at org.antlr.runtime.DFA.predict(DFA.java:116) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource0(HiveParser_FromClauseParser.java:2942) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromSource(HiveParser_FromClauseParser.java:2880) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.joinSource(HiveParser_FromClauseParser.java:1451) at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.fromClause(HiveParser_FromClauseParser.java:1341) at org.apache.hadoop.hive.ql.parse.HiveParser.fromClause(HiveParser.java:45811) at org.apache.hadoop.hive.ql.parse.HiveParser.atomSelectStatement(HiveParser.java:39699) at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:39951) at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:39597) at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:38786) at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:38674) at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2340) at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1369) at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:507) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1388) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1528) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1308) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1298) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:276) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:221) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:465) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:992) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:916) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:795) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:223) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:40 cannot recognize input near 'tt' ';' '<EOF>' in from source 0org.apache.hadoop.hive.ql.parse.ParseException: line 1:40 cannot recognize input near 'tt' ';' '<EOF>' in from source 0 at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:211) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:507) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1388) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1528) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1308) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1298) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:276) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:221) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:465) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:992) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:916) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:795) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:223) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


Parsing is successful when double quote is escaped.
The following query works:



SELECTtranslate(data, '\""', ' ') from test_table;



But the behaviour which looks buggy is when the double quote is not escaped in from/to string of translate function, it seems to be interpreting the double quote in from/to string of next translate function as ending quote.
Parsing is successful for the following queries.



select translate(data, '""', ''), translate(data, '""', '') from test_table;
select translate(data, '""', ''), translate(data, ' ', '""') from test_table;
select translate(data, ""'"", """"), translate(data, ""'"", """") from test_table;
select translate(data, ""'"", """"), translate(data, "" "", ""'"") from test_table;
select translate(data, '""', '""') from test_table;
select translate(data, ""'"", ""'"") from test_table;



The same behaviour exists when we use any other built-in functions or UDFs. 
 HiveParser misinterpretes quotes in parameters of built-in functions or UDFs",13167029,"HIVE-15297 tries to split the command by considering semicolon inside string, but it doesn't consider the case that quotes can also be inside string. 
For the following command insert into escape1 partition (ds='1', part='3') values (""abc' "");, it will fail with 

18/06/19 16:37:05 ERROR ql.Driver: FAILED: ParseException line 1:64 extraneous input ';' expecting EOF near '<EOF>'
org.apache.hadoop.hive.ql.parse.ParseException: line 1:64 extraneous input ';' expecting EOF near '<EOF>'
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:606)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1686)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1633)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1628)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:239)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:153)

 
 HiveCli is not splitting the command by semicolon properly if quotes are inside the string ",yes
13253074,"testCancelledDelegationToken[0] fails intermittently with the following error/stack trace:

java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8088/ws/v1/cluster/delegation-token
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication.cancelDelegationToken(TestRMWebServicesDelegationTokenAuthentication.java:446)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication.testCancelledDelegationToken(TestRMWebServicesDelegationTokenAuthentication.java:267)



I'll attach an stdout as well to this issue.
It seems that the test helper infrastructure does not come up correctly.  
 testCancelledDelegationToken fails intermittently",13226705,"The test TestRMWebServicesDelegationTokenAuthentication.testCancelledDelegationToken sometimes fails with the following error:

java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8088/ws/v1/cluster/delegation-token
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication.cancelDelegationToken(TestRMWebServicesDelegationTokenAuthentication.java:462)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication.testCancelledDelegationToken(TestRMWebServicesDelegationTokenAuthentication.java:283)


The problem is that for whatever reason, Jetty seems to execute the token cancellation REST call twice. First we get HTTP 200 OK, but the second request fails with HTTP 400 Bad Request.
The MockRM instance is static. Something could be a problem in this class and it turned out that using separate MockRM instances solves the flakiness. 
 TestRMWebServicesDelegationTokenAuthentication.testCancelledDelegationToken fails with HTTP 400",yes
13338114,"When applying HBASE-24200 to branch-1.4, we noticed that the PR build failed. Looks like it's broken currently. 
 Fix PreCommit builds for branch-1.4",13320897,"After move to ci-hadoop, branch-1 precommits are unable to checkout the source tree. 
https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/view/change-requests/job/PR-2194/1/console

04:55:42  Running in /home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-2194/yetus
[Pipeline] {
[Pipeline] checkout
04:55:42  No credentials specified
04:55:42  Cloning the remote Git repository
 > git rev-parse HEAD^{commit} # timeout=10
 > git config core.sparsecheckout # timeout=10
 > git checkout -f f9a427c6aff4f01bf06aa458a935249cfd6a5d30 # timeout=10
04:55:41  Cloning repository https://github.com/apache/yetus.git
04:55:41   > git init /home/jenkins/jenkins-home/workspace/Base-PreCommit-GitHub-PR_PR-2194/yetus # timeout=10
04:55:41  Fetching upstream changes from https://github.com/apache/yetus.git
04:55:41   > git --version # timeout=10
04:55:41   > git fetch --tags --progress -- https://github.com/apache/yetus.git +refs/heads/*:refs/remotes/origin/* # timeout=10
04:55:44  Checking out Revision 11add70671de39cd96b56e86e40c64c872b9282f (rel/0.11.1)
04:55:42   > git config remote.origin.url https://github.com/apache/yetus.git # timeout=10
04:55:42   > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/* # timeout=10
04:55:42   > git config remote.origin.url https://github.com/apache/yetus.git # timeout=10
04:55:42  Fetching upstream changes from https://github.com/apache/yetus.git
04:55:42   > git fetch --tags --progress -- https://github.com/apache/yetus.git +refs/heads/*:refs/remotes/origin/* # timeout=10
04:55:43   > git rev-parse rel/0.11.1^{commit} # timeout=10
04:55:43   > git rev-parse refs/remotes/origin/rel/0.11.1^{commit} # timeout=10
04:55:44  Commit message: ""YETUS-920. Stage version 0.11.1.""
04:55:44  First time build. Skipping changelog.
[Pipeline] }
[Pipeline] // dir
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (precommit-run)
[Pipeline] withCredentials
[Pipeline] // withCredentials
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Declarative: Post Actions)
[Pipeline] script
[Pipeline] {
[Pipeline] step
04:55:45  Archiving artifacts
[Pipeline] publishHTML
04:55:45  [htmlpublisher] Archiving HTML reports...

 
 Remove unused credential hbaseqa-at-asf-jira",yes
13293059,"Compiler gets it's PerfLogger again and again from SessionState, relying on the fact that it is returned from the same ThreadLocal. As this may change in the future it'd be better to use the same instance, obtained only once. 
 Remove Calls to printStackTrace in Module hive-service",13195023,"In YARN service API:
public class ComponentContainers

{ private static final long serialVersionUID = -1456748479118874991L; ... }

 seems should be
 public class ComponentContainers implements Serializable 
{

private static final long serialVersionUID = -1456748479118874991L; ... } 
 Add Serializable interface to ComponentContainers",No
13258278,"During upgrade from 2.6 to 3.1, we encountered a problem:


2019-09-23,19:29:05,303 WARN org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Lost container container_e35_1568719110875_6460_08_000001, status: RUNNING, execution type: null
2019-09-23,19:29:05,303 WARN org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Lost container container_e35_1568886618758_11172_01_000062, status: RUNNING, execution type: null
2019-09-23,19:29:05,303 WARN org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Lost container container_e35_1568886618758_11172_01_000063, status: RUNNING, execution type: null
2019-09-23,19:29:05,303 WARN org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Lost container container_e35_1568886618758_11172_01_000064, status: RUNNING, execution type: null
2019-09-23,19:29:05,303 WARN org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Lost container container_e35_1568886618758_30617_01_000006, status: RUNNING, execution type: null

for (ContainerStatus remoteContainer : containerStatuses) {
  if (remoteContainer.getState() == ContainerState.RUNNING
      && remoteContainer.getExecutionType() == ExecutionType.GUARANTEED) {
    nodeContainers.add(remoteContainer.getContainerId());
  } else {
    LOG.warn(""Lost container "" + remoteContainer.getContainerId()
            + "", status: "" + remoteContainer.getState()
            + "", execution type: "" + remoteContainer.getExecutionType());
  }
}​



The cause is that we has nm with version 2.6, which do not have executionType for container status.
We should check here make the upgrade process more tranparently 
 Make execution type check compatible",13232957,"

  @Override
  public synchronized ExecutionType getExecutionType() {
    ContainerStatusProtoOrBuilder p = viaProto ? proto : builder;
    if (!p.hasExecutionType()) {
      return null;
    }
    return convertFromProtoFormat(p.getExecutionType());
  }


ContainerStatusPBImpl executionType should return default as ExecutionType.GUARANTEED.
 
 ContainerStatusPBImpl default execution type is not returned",yes
13279500,"CVE-2019-20330 is reported and fixed in jackson-databind 2.9.10.2.
https://nvd.nist.gov/vuln/detail/CVE-2019-20330 
 Upgrade jackson-databind to 2.9.10.2",13247686,"Looking at logs of LocatedFileStatus/FileInputFormat scans; there's a needless call to getFileStatus whenever a S3AFileSystem.listLocatedStatus() call is made

S3AFileSystem.listLocatedStatus() does a getFileStatus call, returns the file status first
But if you look at all the uses in the MR code in FileInputFormat and LocatedFileStatusFetcher, they only call this method knowing the destination is a directory

Which means for every unguarded S3 path: two needless HEADS and a single entry LIST, before the real LIST is initiated.
If the S3A FS can assume that a dest is a non-empty directory, then it can go straight to the LIST operation, only falling back to the HEAD + HEAD +/ if that fails.
We could also think about doing the same for listStatus 
 Tune S3AFileSystem.listLocatedStatus",No
13210666,"Please see http://www.apache.org/dev/release-download-pages.html#links for the detail. 
 Do not use dist.apache.org for download link",13165789,"The download page currently links to https://dist.apache.org/ for the KEYS, hashes and sigs.
However that host is only intended as a staging area for use by developers.
Please can you change the links to use the ASF webserver instead?
i.e.. change
https://dist.apache.org/repos/dist/release/...
to
https://www.apache.org/dist/...
wherever it appears. 
 Download page must not link to dist.apache.org",yes
13336690,"a lot of hbase component usehtrace-core4, thishtrace-core4 shaded fasterxmljackson(version 2.4.0)
[INFO] | +- org.apache.hbase.thirdparty:hbase-shaded-miscellaneous:jar:2.2.1:compile
[INFO] | +- org.slf4j:slf4j-api:jar:1.7.29:compile
[INFO] | +- commons-io:commons-io:jar:2.6:compile
[INFO] | +- org.apache.htrace:htrace-core4:jar:4.2.0-incubating:compile
[INFO] | +- org.apache.commons:commons-crypto:jar:1.0.0:compile
[INFO] | +- com.github.stephenc.findbugs:findbugs-annotations:jar:1.3.9-1:compile
[INFO] | +- log4j:log4j:jar:1.2.17:compile
[INFO] | - org.apache.yetus:audience-annotations:jar:0.5.0:compile

as you known fasterxml jackson component is frequently coming out newvulnerabilities, like CVE-2016-7051、CVE-2016-3720、CVE-2018-5968、CVE-2018-11307、CVE-2018-7489、CVE-2019-14893、CVE-2019-14379、CVE-2020-14195、CVE-2020-14061、CVE-2020-8840、CVE-2019-14540、CVE-2020-10968、CVE-2020-11619、CVE-2019-17531、CVE-2019-16943、CVE-2020-14062、CVE-2020-14060、CVE-2020-11111、CVE-2019-16942、CVE-2020-9546、CVE-2020-9548、CVE-2019-12384、CVE-2020-10673、CVE-2020-24750、CVE-2019-16335、CVE-2019-14439、CVE-2020-10969、CVE-2020-11112、CVE-2019-12086、CVE-2019-20330、CVE-2019-17267、CVE-2020-9547、CVE-2020-11113、CVE-2020-10672、CVE-2020-11620、CVE-2020-24616、CVE-2018-19362、CVE-2018-19361、CVE-2018-19360、CVE-2018-14721、CVE-2018-14720、CVE-2018-14719、CVE-2018-14718、CVE-2018-1000873、CVE-2017-7525、CVE-2017-17485、CVE-2017-15095，CVE-2019-12814
htrace-core4 is closed 4 years ago, what about this component'svulnerabilities, did hbase have plan to do with this？


 
 about hbase introduced fasterxml‘s jackson versions and vulnerabilities ",13320258,"htrace-core4 is a retired project and even on the latest version they Shade Jackson databind version 2.4.0 which has the following CVEs:



cve
severity
cvss


CVE-2017-15095
critical
9.8


CVE-2018-1000873
medium
6.5


CVE-2018-14718
critical
9.8


CVE-2018-5968
high
8.1


CVE-2018-7489
critical
9.8


CVE-2019-14540
critical
9.8


CVE-2019-14893
critical
9.8


CVE-2019-16335
critical
9.8


CVE-2019-16942
critical
9.8


CVE-2019-16943
critical
9.8


CVE-2019-17267
critical
9.8


CVE-2019-17531
critical
9.8


CVE-2019-20330
critical
9.8


CVE-2020-10672
high
8.8


CVE-2020-10673
high
8.8


CVE-2020-10968
high
8.8


CVE-2020-10969
high
8.8


CVE-2020-11111
high
8.8


CVE-2020-11112
high
8.8


CVE-2020-11113
high
8.8


CVE-2020-11619
critical
9.8


CVE-2020-11620
critical
9.8


CVE-2020-14060
high
8.1


CVE-2020-14061
high
8.1


CVE-2020-14062
high
8.1


CVE-2020-14195
high
8.1


CVE-2020-8840
critical
9.8


CVE-2020-9546
critical
9.8


CVE-2020-9547
critical
9.8


CVE-2020-9548
critical
9.8




Our security team is trying to block us from using hbase because of this 
 make a drop-in compatible impl of htrace APIs that does not do anything",yes
13161841,"I'm upgrading from Hadoop 2.7.3 to 2.9.1. ResourceManager restart works fine for 2.7.3, but fails on 2.9.1.
I'm using LevelDB as the RM state store, the problem seems related toTimelineServiceV1Publisher. If I setyarn.resourcemanager.system-metrics-publisher.enabled to false, then recovery works fine. But if the option is set to true, RM fails to start with the following log:

2018-05-24 23:11:54,597 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started
2018-05-24 23:11:54,673 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.1
2018-05-24 23:11:54,688 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 12 RM delegation token master keys
2018-05-24 23:11:54,688 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation tokens
2018-05-24 23:11:54,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 2099 applications and 2100 application attempts
2018-05-24 23:11:54,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 reservations
2018-05-24 23:11:54,998 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
2018-05-24 23:11:55,003 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Recovering 2099 applications
2018-05-24 23:11:55,107 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Successfully recovered 0 out of 2099 applications
2018-05-24 23:11:55,108 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state
java.lang.NullPointerException
at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.appCreated(TimelineServiceV1Publisher.java:90)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.sendATSCreateEvent(RMAppImpl.java:1954)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:931)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:1061)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:1054)
at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)
at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)
at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)
at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:878)
at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:339)
at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:533)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1394)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:758)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1147)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1187)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1183)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1889)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1183)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1223)
at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1422) 
 ResourceManager restart fail to recover due to TimelineServiceV1Publisher NPE",13147553,"[TimelineServiceV1Publisher|eclipse-javadoc:%E2%98%82=hadoop-yarn-server-resourcemanager/src%5C/main%5C/java%3Corg.apache.hadoop.yarn.server.resourcemanager.metrics%7BTimelineServiceV1Publisher.java%E2%98%83TimelineServiceV1Publisher].appCreated will cause NPE as we use like below


entityInfo.put(ApplicationMetricsConstants.APPLICATION_PRIORITY_INFO,app.getApplicationPriority().getPriority());

We have to handle this case while recovery.
 
 Application Priority field causes NPE in app timeline publish when Hadoop 2.7 based clients to 2.8+",yes
13129042,"Insert statements create files of format ending with 0000_0, 0001_0 etc. However, the load data uses the input file name. That results in inconsistent naming convention which makes SMB joins difficult in some scenarios and may cause trouble for other types of queries in future.
We need consistent naming convention.
For non-bucketed table, hive renames all the files regardless of how they were named by the user. 
 load data should rename files consistent with insert statements (non bucketed tables only)",13184678,"fix java doc error from node attributes in yarn-api module. 
 Javadoc error in node attributes",No
13215505,"Steps to reproduce
1:set_quota TYPE => SPACE, TABLE => 'test25', LIMIT => '2M', POLICY => NO_WRITES
2:./hbase pe --table=""test25"" --nomapred --rows=300 sequentialWrite 10
3: Observe that after some time data usage is 3 mb and policy is in violation
4: now try to insert some data again in the table and observe that operation fails due toNoWritesViolationPolicyEnforcement
5: Now change the quota policy
set_quota TYPE => SPACE, TABLE => 'test25', LIMIT => '2M', POLICY => NO_WRITES_COMPACTIONS
6: Now again try to insert data once new policy takes effect
7: Observe that still operation fails but because of old policy and not new policy.

 
 New space quota policy doesn't take effect if quota policy is changed after violation",13162981,"Steps to reproduce

Create a table and set quota with SpaceViolationPolicy.DISABLE having limit say 2MB
Now put rows until space quota is violated and table gets disabled
Next, increase space quota with limit say 4MB on the table
Now try putting a row into the table



 private void testSetQuotaThenViolateAndFinallyIncreaseQuota() throws Exception {
    SpaceViolationPolicy policy = SpaceViolationPolicy.DISABLE;
    Put put = new Put(Bytes.toBytes(""to_reject""));
    put.addColumn(Bytes.toBytes(SpaceQuotaHelperForTests.F1), Bytes.toBytes(""to""),
      Bytes.toBytes(""reject""));

    // Do puts until we violate space policy
    final TableName tn = writeUntilViolationAndVerifyViolation(policy, put);

    // Now, increase limit
    setQuotaLimit(tn, policy, 4L);

    // Put some row now: should not violate as quota limit increased
    verifyNoViolation(policy, tn, put);
  }


Expected
We should be able to put data as long as newly set quota limit is not reached
Actual
We fail to put any new row even after increasing limit
Root cause
Increasing quota on a violated table triggers the table to be enabled, but since the table is already in violation, the system does not allow it to be enabled (may be thinking that a user is trying to enable it)
Relevant exception trace

2018-05-31 00:34:27,563 INFO  [regionserver/root1-ThinkPad-T440p:0.Chore.1] client.HBaseAdmin$14(844): Started enable of testSetQuotaAndThenIncreaseQuotaWithDisable0
2018-05-31 00:34:27,571 DEBUG [RpcServer.default.FPBQ.Fifo.handler=3,queue=0,port=42525] ipc.CallRunner(142): callId: 11 service: MasterService methodName: EnableTable size: 104 connection: 127.0.0.1:38030 deadline: 1527707127568, exception=org.apache.hadoop.hbase.security.AccessDeniedException: Enabling the table 'testSetQuotaAndThenIncreaseQuotaWithDisable0' is disallowed due to a violated space quota.
2018-05-31 00:34:27,571 ERROR [regionserver/root1-ThinkPad-T440p:0.Chore.1] quotas.RegionServerSpaceQuotaManager(210): Failed to disable space violation policy for testSetQuotaAndThenIncreaseQuotaWithDisable0. This table will remain in violation.
org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Enabling the table 'testSetQuotaAndThenIncreaseQuotaWithDisable0' is disallowed due to a violated space quota.
	at org.apache.hadoop.hbase.master.HMaster$6.run(HMaster.java:2275)
	at org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.submitProcedure(MasterProcedureUtil.java:131)
	at org.apache.hadoop.hbase.master.HMaster.enableTable(HMaster.java:2258)
	at org.apache.hadoop.hbase.master.MasterRpcServices.enableTable(MasterRpcServices.java:725)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:409)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.instantiateException(RemoteWithExtrasException.java:100)
	at org.apache.hadoop.hbase.ipc.RemoteWithExtrasException.unwrapRemoteException(RemoteWithExtrasException.java:90)
	at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.makeIOExceptionOfException(ProtobufUtil.java:360)
	at org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.handleRemoteException(ProtobufUtil.java:348)
	at org.apache.hadoop.hbase.client.MasterCallable.call(MasterCallable.java:101)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithRetries(RpcRetryingCallerImpl.java:107)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3061)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3053)
	at org.apache.hadoop.hbase.client.HBaseAdmin.enableTableAsync(HBaseAdmin.java:839)
	at org.apache.hadoop.hbase.client.HBaseAdmin.enableTable(HBaseAdmin.java:833)
	at org.apache.hadoop.hbase.quotas.policies.DisableTableViolationPolicyEnforcement.disable(DisableTableViolationPolicyEnforcement.java:62)
	at org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager.disableViolationPolicyEnforcement(RegionServerSpaceQuotaManager.java:208)
	at org.apache.hadoop.hbase.quotas.SpaceQuotaRefresherChore.chore(SpaceQuotaRefresherChore.java:110)
	at org.apache.hadoop.hbase.ScheduledChore.run(ScheduledChore.java:186)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl$JitteredRunnableScheduledFuture.run(JitterScheduledThreadPoolExecutorImpl.java:111)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.security.AccessDeniedException): org.apache.hadoop.hbase.security.AccessDeniedException: Enabling the table 'testSetQuotaAndThenIncreaseQuotaWithDisable0' is disallowed due to a violated space quota.
	at org.apache.hadoop.hbase.master.HMaster$6.run(HMaster.java:2275)
	at org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.submitProcedure(MasterProcedureUtil.java:131)
	at org.apache.hadoop.hbase.master.HMaster.enableTable(HMaster.java:2258)
	at org.apache.hadoop.hbase.master.MasterRpcServices.enableTable(MasterRpcServices.java:725)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:409)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)

	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.onCallFinished(AbstractRpcClient.java:387)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.access$100(AbstractRpcClient.java:95)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:410)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$3.run(AbstractRpcClient.java:406)
	at org.apache.hadoop.hbase.ipc.Call.callComplete(Call.java:103)
	at org.apache.hadoop.hbase.ipc.Call.setException(Call.java:118)
	at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.readResponse(NettyRpcDuplexHandler.java:161)
	at org.apache.hadoop.hbase.ipc.NettyRpcDuplexHandler.channelRead(NettyRpcDuplexHandler.java:191)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)
	at org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.hbase.thirdparty.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at org.apache.hbase.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at org.apache.hbase.thirdparty.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
	at org.apache.hbase.thirdparty.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:801)
	at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:404)
	at org.apache.hbase.thirdparty.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:304)
	at org.apache.hbase.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at org.apache.hbase.thirdparty.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more

 
 Increasing space quota on a violated table does not remove SpaceViolationPolicy.DISABLE enforcement",yes
13141181,"Create a timeline domain table in HBase for storing domain information 
 Add table for storing timeline domain information",13138593,"
Update the schema creator to create a domain table to store timeline entity domain info. 
 [atsv2 read acls] Include domain table creation as part of schema creator",yes
13208370,"Depending on Maven version the Findbugs plugin version 3.0.0 can produce this:
[ERROR] Failed to execute goal org.codehaus.mojo:findbugs-maven-plugin:3.0.0:findbugs (default) on project hbase: Unable to parse configuration of mojo org.codehaus.mojo:findbugs-maven-plugin:3.0.0:findbugs for parameter pluginArtifacts: Cannot assign configuration entry 'pluginArtifacts' with value '${plugin.artifacts}' of type java.util.Collections.UnmodifiableRandomAccessList to property of type java.util.ArrayList -> [Help 1]
Fix is to upgrade the plugin from 3.0.0 to 3.0.4. 
This looks like an issue only with the branch-1s.  
 Upgrade findbugs maven plugin to 3.0.4",13208142,"Trying to build any branch-1 using Maven 3.6.0 fails:


[ERROR] Failed to execute goal org.codehaus.mojo:findbugs-maven-plugin:3.0.0:findbugs (default) on project hbase: Unable to parse configuration of mojo org.codehaus.mojo:findbugs-maven-plugin:3.0.0:findbugs for parameter pluginArtifacts: Cannot assign configuration entry 'pluginArtifacts' with value '${plugin.artifacts}' of type java.util.Collections.UnmodifiableRandomAccessList to property of type java.util.ArrayList -> [Help 1]


also shows up on nightly in the ""check that we can make a source artifact"" test, because ""Maven (latest)"" recently updated to maven 3.6.0. 
 Update Findbugs Maven Plugin to 3.0.4 to work with Maven 3.6.0+ [branches-1]",yes
13258198,"It's a simple bug, caused by RouterClientProtocl#821, a broken path is passed to getMountPointStatus(). We should pass the full path of the child. 
The original test case TestRouterMountTable.testMountTablePermissionsNoDest() doesn't cover enough cases, so I extend it. Test case TestRouterMountTable.testGetMountPointStatusWithIOException() is also broken, userB and groupB should be set to entry /testA/testB. 
 RBF: RouterRpcServer.getListing() returns wrong owner, group and permission.",13343434,"
java.lang.AssertionError: expected:<COMMITTED> but was:<COMPLETE>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.hdfs.TestDecommission.testAllocAndIBRWhileDecommission(TestDecommission.java:1025)

 
 Fix intermittent falilure of TestDecommission#testAllocAndIBRWhileDecommission",No
13189174,"Problemdescription:
The master thread initializes the sessionHive object in HiveSessionImpl classwhenwe opena new session for a client connection and by default all queries from thisconnectionshares the same sessionHive object.
If the master thread executes asynchronousquery, it closes the sessionHive object (referred via thread local hiveDb) if Hive.isCompatible returns false and sets new Hive object in thread local HiveDb but doesn'tchange the sessionHive object in the session. Whereas, asynchronous query execution via asyncthreadsnever closes the sessionHive object and it just creates a new one if needed and sets it as their thread local hiveDb.
So, the problem can happenin the case whereanasynchronous query is beingexecuted by async threads refers to sessionHive object and the master thread receives a synchronous querythatclosesthe samesessionHive object.
Also, each query execution overwrites the thread local hiveDb object to sessionHive object which potentially leaks a metastore connection if the previous synchronous query execution re-created the Hive object.
Possible Fix:
The sessionHive object could be shared my multiple threads and so it shouldn't be allowed to be closed by any query execution threads when they re-create the Hive object due to changes in Hive configurations. But the Hive objects created by query execution threads should be closed when the thread exits.
So, it is proposed to have an isAllowCloseflag (default: true) in Hive object which should be set to false forsessionHive and would be forcefully closed when the session is closed or released.
Also, when we reset sessionHive object with new one due to changes in sessionConf, the old one should be closed when no async thread is referring to it. This can be done using ""finalize"" method of Hive object where we can close HMS connection when Hive object is garbage collected.
cc Peter Vary 
 Async query execution can potentially fail if shared sessionHive is closed by master thread.",13217362,"In the following HPL/SQL programs:
declare num = 1; print -num;
The expected result should be '-1'，but it print '1' . 
 Negative forms of variables are not supported in HPL/SQL",No
13128899,"mergejoin test fails with java.lang.IllegalStateException when run in MiniLlapLocal.
This is the query for which it fails,
[ERROR]   TestMiniLlapLocalCliDriver.testCliDriver:59 Client execution failed with error code = 2 running ""
select count from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value"" fname=mergejoin.q 
This is the stack trace,
failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: b initializer failed, vertex=vertex_1515180518813_0001_42_05 [Map 8], java.lang.RuntimeException: ORC split generation failed with exception: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_12_a_key_min
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1784)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1872)
    at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:499)
    at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:684)
    at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:196)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
    at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)
    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)
    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_12_a_key_min
    at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    at java.util.concurrent.FutureTask.get(FutureTask.java:192)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1778)
    ... 17 more
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value for RS_12_a_key_min
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:142)
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getJavaValue(DynamicValue.java:97)
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getLiteral(DynamicValue.java:93)
    at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.getLiteralList(SearchArgumentImpl.java:120)
    at org.apache.orc.impl.RecordReaderImpl.evaluatePredicateMinMax(RecordReaderImpl.java:553)
    at org.apache.orc.impl.RecordReaderImpl.evaluatePredicateRange(RecordReaderImpl.java:463)
    at org.apache.orc.impl.RecordReaderImpl.evaluatePredicate(RecordReaderImpl.java:440)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.isStripeSatisfyPredicate(OrcInputFormat.java:2163)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.pickStripesInternal(OrcInputFormat.java:2140)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.pickStripes(OrcInputFormat.java:2131)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.access$3000(OrcInputFormat.java:157)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.callInternal(OrcInputFormat.java:1476)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.access$2700(OrcInputFormat.java:1261)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1445)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator$1.run(OrcInputFormat.java:1442)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1442)
    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:1261)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    ... 3 more
Caused by: java.lang.IllegalStateException: Value does not exist in registry: RS_12_a_key_min
    at org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.getValue(DynamicValueRegistryTez.java:76)
    at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:137)
    ... 23 more 
 mergejoin fails with java.lang.IllegalStateException",13193352,"Currently ObserverReadProxyProviderWithIPFailover extends ObserverReadProxyProvider, and the only difference is changing the proxy factory to use IPFailoverProxyProvider. However this is not enough because when calling constructor of ObserverReadProxyProvider in super(...), the follow line:


nameNodeProxies = getProxyAddresses(uri,
        HdfsClientConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);


will try to resolve the all configured NN addresses to do configured failover. But in the case of IPFailover, this does not really apply.

A second issue closely related is about delegation token. For example, in current IPFailover setup, say we have a virtual host nn.xyz.com, which points to either of two physical nodes nn1.xyz.com or nn2.xyz.com. In current HDFS, there is always only one DT being exchanged, which has hostnamenn.xyz.com. Server only issues this DT, and client only knows the host nn.xyz.com, so all is good. But in Observer read, even with IPFailover, the client will no longer contactingnn.xyz.com, but will actively reaching to nn1.xyz.com andnn2.xyz.com. During this process, current code will look for DT associated with hostnamenn1.xyz.com ornn2.xyz.com, which is different from the DT given by NN. causing Token authentication to fail. This happens in AbstractDelegationTokenSelector#selectToken. New IPFailover proxy provider will need toresolve this as well. 
 ObserverReadProxyProviderWithIPFailover should work with HA configuration",No
13160803,"The jobSubmitDir directory is owned by root and is being cleaned up as the submitting user, which appears to be why it is failing to clean up.

2018-05-21 19:46:15,124 WARN  [DeletionService #0] privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 255. Privileged Execution Operation Stderr:

Stdout: main : command provided 3
main : run as user is ebadger
main : requested yarn user is ebadger
failed to unlink /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/jobSubmitDir/job.split: Permission denied
failed to unlink /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/jobSubmitDir/job.splitmetainfo: Permission denied
failed to rmdir jobSubmitDir: Directory not empty
Error while deleting /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001: 39 (Directory not empty)

Full command array for failed execution:
[/hadoop-3.2.0-SNAPSHOT/bin/container-executor, ebadger, ebadger, 3, /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001]
2018-05-21 19:46:15,124 ERROR [DeletionService #0] nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(848)) - DeleteAsUser for /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001 returned with exit code: 255
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=255:
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:206)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.deleteAsUser(LinuxContainerExecutor.java:844)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask.run(FileDeletionTask.java:135)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: ExitCodeException exitCode=255:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009)
        at org.apache.hadoop.util.Shell.run(Shell.java:902)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)
        ... 10 more



[foo@bar hadoop]$ ls -l /tmp/hadoop-local3/usercache/ebadger/appcache/application_1526931492976_0007/container_1526931492976_0007_01_000001/
total 4
drwxr-sr-x 2 root users 4096 May 21 19:45 jobSubmitDir

 
 Privileged docker containers' jobSubmitDir does not get successfully cleaned up",13284632,"Issue seen by a customer:
The failed requests we were seeing in the AbfsClient logging actually never made it out over the wire. We have found that there’s an issue with ADLS passthrough and the 8 read ahead threads that ADLSv2 spawns in ReadBufferManager.java. We depend on thread local storage in order to get the right JWT token and those threads do not have the right information in their thread local storage. Thus, when they pick up a task from the read ahead queue they fail by throwing anAzureCredentialNotFoundException exception in AbfsRestOperation.executeHttpOperation() where it calls client.getAccessToken(). This exception is silently swallowed by the read ahead threads in ReadBufferWorker.run(). As a result, every read ahead attempt results in a failedexecuteHttpOperation(), but still callsAbfsClientThrottlingIntercept.updateMetrics() and contributes to throttling (despite not making it out over the wire). After the read aheads fail, the main task thread performs the read with the right thread local storage information and succeeds, but first sleeps for up to 10 seconds due to the throttling. 
 ABFS: Send error back to client for Read Ahead request failure",No
13217415,"The FairScheduler's configuration has the following defaults (from the code: javadoc):

In new style resources, any resource that is not specified will be set to missing or 0%, as appropriate. Also, in the new style resources, units are not allowed. Units are assumed from the resource manager's settings for the resources when the value isn't a percentage. The missing parameter is only used in the case of new style resources without percentages. With new style resources with percentages, any missing resources will be assumed to be 100% because percentages are only used with maximum resource limits.


This is not documented in the hadoop yarn site FairScheduler.html. It is quite intuitive, but still need to be documented though. 
 Fair Scheduler configuration defaults are not documented in case of min and maxResources",13264090,"hive-druid-handler jar has shaded version of druid classes, druid-hdfs-storage also has non-shaded classes. 

 
[hive@hiveserver2-1 lib]$ ls |grep druid
calcite-druid-1.19.0.7.0.2.0-163.jar
druid-bloom-filter-0.15.1.7.0.2.0-163.jar
druid-hdfs-storage-0.15.1.7.0.2.0-163.jar
hive-druid-handler-3.1.2000.7.0.2.0-163.jar
hive-druid-handler.jar


Exception below - 


Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:177)
  ... 22 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:765)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:630)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)
  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)
  ... 3 more
Caused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:96)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:104)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:743)
  ... 6 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.copyFilesWithChecks(HdfsDataSegmentPusher.java:163)
  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.push(HdfsDataSegmentPusher.java:145)
  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$mergeAndPush$4(AppenderatorImpl.java:747)
  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86)

 
 Add ability to read Druid metastore password from jceks",No
13282352,"RM fails to start with SystemServiceManagerImpl failed to initialize.


2020-01-28 12:20:43,631 WARN  ha.ActiveStandbyElector (ActiveStandbyElector.java:becomeActive(900)) - Exception handling the winning of election
org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:146)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:896)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:476)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:636)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)
Caused by: org.apache.hadoop.ha.ServiceFailedException: Error when transitioning to Active mode
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:325)
        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)
        ... 4 more
Caused by: org.apache.hadoop.service.ServiceStateException: java.io.IOException: Filesystem closed
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
        at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:881)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1257)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1298)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1294)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1294)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)
        ... 5 more
Caused by: java.io.IOException: Filesystem closed
        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:475)
        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1645)
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1219)
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1235)
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1202)
        at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1181)
        at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1177)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusIterator(DistributedFileSystem.java:1189)
        at org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.list(SystemServiceManagerImpl.java:375)
        at org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.scanForUserServices(SystemServiceManagerImpl.java:282)
        at org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.serviceStart(SystemServiceManagerImpl.java:126)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        ... 16 more


This happens when reusing the FileSystem object which gets closed.
 
 SystemServiceManagerImpl fails to initialize ",13230868,"when starting RM, listing system services directory has failed as follows.


2019-04-30 17:18:25,441 INFO  client.SystemServiceManagerImpl (SystemServiceManagerImpl.java:serviceInit(114)) - System Service Directory is configured to /services
2019-04-30 17:18:25,467 INFO  client.SystemServiceManagerImpl (SystemServiceManagerImpl.java:serviceInit(120)) - UserGroupInformation initialized to yarn (auth:SIMPLE)
2019-04-30 17:18:25,467 INFO  service.AbstractService (AbstractService.java:noteFailure(267)) - Service ResourceManager failed in state STARTED
org.apache.hadoop.service.ServiceStateException: java.io.IOException: Filesystem closed
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
        at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: java.io.IOException: Filesystem closed
        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:473)
        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1639)
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1217)
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1233)
        at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:1200)
        at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1179)
        at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1175)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusIterator(DistributedFileSystem.java:1187)
        at org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.list(SystemServiceManagerImpl.java:375)
        at org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.scanForUserServices(SystemServiceManagerImpl.java:282)
        at org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl.serviceStart(SystemServiceManagerImpl.java:126)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        ... 13 more


it looks like due to the usage of filesystem cache.
this issue does not happen, when I add ""fs.hdfs.impl.disable.cache=true"" to yarn-site 
 RM failed to start due to system services",yes
13309232,"After release 2.3.0 is out, we need to backportHBASE-24492to all 2.x branches. 
 Backport HBASE-24492 to all 2.x branch",13128460,"Within a database, to filter for tables with the string 'abc' in its name, I can use something like:


hive> use my_database;
hive> show tables '*abc*';


It would be great if I can do something similar to search within the list of columns in a table.
I have a table with around 3200 columns. Searching for the column of interest is an onerous task after doing a describe on it. 
 Make it easier to search for column name in a table",No
13137348,"See https://builds.apache.org/job/HBase%20Nightly/job/branch-2/284/testReport/junit/org.apache.hadoop.hbase.client/TestAsyncRegionAdminApi/testMergeRegions_0_/

java.lang.AssertionError: expected:<2> but was:<3> at org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi.testMergeRegions(TestAsyncRegionAdminApi.java:359)

Merge regions not work. The table still have 3 regions after the MergeRegionsProcedure finished.
The master start balance region 9e2773ba1efba79a2defa276e9a26ed4. But because the MergeRegionsProcedure pid=138 start work first, so the balance need wait for the lock. But after merge regions finished, the MoveRegionProcedure pid=139 start work and assign 9e2773ba1efba79a2defa276e9a26ed4 to a new region server. This is not right. The MoveRegionProcedure should skip to assign a region which was marked as offline. Or we should clear the merged regions' procedure whenMergeRegionsProcedure finished.

Logs:
2018-02-08 16:24:44,608 INFO [master/cd4730e3eae2:0.Chore.1] master.HMaster(1454): balance hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318
2018-02-08 16:24:44,608 DEBUG [RpcServer.default.FPBQ.Fifo.handler=4,queue=0,port=37885] procedure2.ProcedureExecutor(868): Stored pid=138, state=RUNNABLE:MERGE_TABLE_REGIONS_PREPARE; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false
......
2018-02-08 16:24:50,111 INFO [PEWorker-13] procedure2.ProcedureExecutor(1249): Finished pid=138, state=SUCCESS; MergeTableRegionsProcedure table=testMergeRegions, regions=[9e2773ba1efba79a2defa276e9a26ed4, 8f8fd5cd032313e1aadb83e31e1b7479], forcibly=false in 5.5710sec
2018-02-08 16:24:50,113 INFO [PEWorker-13] procedure.MasterProcedureScheduler(813): pid=139, state=RUNNABLE:MOVE_REGION_UNASSIGN; MoveRegionProcedure hri=testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4., source=cd4730e3eae2,39077,1518106776411, destination=cd4730e3eae2,40578,1518106776318 testMergeRegions testMergeRegions,,1518107079782.9e2773ba1efba79a2defa276e9a26ed4.
 
 Fix flaky TestAsyncRegionAdminApi",13247686,"Looking at logs of LocatedFileStatus/FileInputFormat scans; there's a needless call to getFileStatus whenever a S3AFileSystem.listLocatedStatus() call is made

S3AFileSystem.listLocatedStatus() does a getFileStatus call, returns the file status first
But if you look at all the uses in the MR code in FileInputFormat and LocatedFileStatusFetcher, they only call this method knowing the destination is a directory

Which means for every unguarded S3 path: two needless HEADS and a single entry LIST, before the real LIST is initiated.
If the S3A FS can assume that a dest is a non-empty directory, then it can go straight to the LIST operation, only falling back to the HEAD + HEAD +/ if that fails.
We could also think about doing the same for listStatus 
 Tune S3AFileSystem.listLocatedStatus",No
13260102,"Contains also Kerberos related Zookeeper configuration changes after refactor. 
 Backport HBASE-23054 ""Remove synchronization block from MetaTableMetrics and fix LossyCounting algorithm"" to branch-1",13138770,"when you call flush() on a closed S3A output stream, you get a stack trace. 
This can cause problems in code with race conditions across threads, e.g. FLINK-8543. 
we could make it log@warn ""stream closed"" rather than raise an IOE. It's just a hint, after all. 
 S3ABlockOutputStream.flush() be no-op when stream closed",No
13155194,"Allows external clients to consume output from LLAP daemons in Arrow stream format. 
 Arrow format for LlapOutputFormatService (umbrella)",13158906,"Arrow batch serializer doesn't handle null values well in complex nested data types. 
 Null value error with complex nested data type in Arrow batch serializer",yes
13273630,"Distcp over S3A always copies all source files no matter the files are changed or not. This is opposite to the statement in the doc below.
http://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html

And to use -update to only copy changed files.


CopyMapper compares file length as well as block size before copying. While the file length should match, the block size does not. This is apparently because the returned block size from S3A is always 32MB.
https://github.com/apache/hadoop/blob/release-3.2.0-RC1/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L348
I'd suppose we should update the documentation or make code change. 
 Replace Base64 in accumulo-handler Package",13286361,"This fix went in to 2.2+. But I feel like it's applicable and could be used in 2.1 as well. 
 Backport HBASE-22040 to branch-2.1",No
13326378,"MetaFixer fails to fix overlaps when multiple tables have overlaps
Steps to reproduce from UT.

Create table t1 and t2 with split keys, [""bbb"", ""ccc"", ""ddd"", ""eee""]
Create extra region in both t1 and t2 with start key ""bbb"" and end key ""ddd""
Run catalog janitor, It will report total 4 overlaps, 2 from each table.
Run MetaFixer, wait for merges to finish.
Run the catalog janitor again and verify report, there should not be any overlap
Overlap still exists. Reproduced!!!

Analysis.

When I run the same scenario for just one table t1, overlaps are fixed successfully.
Seems problem with MetaFixer#calculateMerges.
I think merges should be calculated within a table. Across the table merge does not have significance.

 
 MetaFixer fails to fix overlaps when multiple tables have overlaps",13342498,"https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2488/1/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn.txt

[ERROR] Failures: 
[ERROR]   TestRouterWebServicesREST.testAppAttemptXML:720->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppPriorityXML:796->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppQueueXML:846->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppStateXML:744->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppTimeoutXML:920->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppTimeoutsXML:896->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testAppXML:696->performGetCalls:274 expected:<200> but was:<204>
[ERROR]   TestRouterWebServicesREST.testUpdateAppPriorityXML:832 expected:<200> but was:<500>
[ERROR]   TestRouterWebServicesREST.testUpdateAppQueueXML:882 expected:<200> but was:<500>
[ERROR]   TestRouterWebServicesREST.testUpdateAppStateXML:782 expected:<202> but was:<500>
[ERROR] Errors: 
[ERROR]   TestRouterWebServicesREST.testGetAppAttemptXML:1292->getAppAttempt:1464 Â» ClientHandler
[ERROR]   TestRouterWebServicesREST.testGetAppsMultiThread:1337->testGetContainersXML:1317->getAppAttempt:1464 Â» ClientHandler
[ERROR]   TestRouterWebServicesREST.testGetContainersXML:1317->getAppAttempt:1464 Â» ClientHandler 
 
 TestRouterWebServicesREST fails",No
13241639,"https://github.com/apache/hive/blob/967a1cc98beede8e6568ce750ebeb6e0d048b8ea/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java#L494-L503 


    int headerCount = 0;
    int footerCount = 0;
    if (table != null) {
      headerCount = Utilities.getHeaderCount(table);
      footerCount = Utilities.getFooterCount(table, conf);
      if (headerCount != 0 || footerCount != 0) {
        // Input file has header or footer, cannot be splitted.
        HiveConf.setLongVar(conf, ConfVars.MAPREDMINSPLITSIZE, Long.MAX_VALUE);
      }
    }


this piece of code makes the CSV (or any text files with header/footer) files not splittable if header or footer is present. 
If only header is present, we can find the offset after first line break and use that to split. Similarly for footer, may be read few KB's of data at the end and find the last line break offset. Use that to determine the data range which can be used for splitting. Few reads during split generation are cheaper than not splitting the file at all.   
 Set thread names for CapacityScheduler AsyncScheduleThread",13144312,"The test TestTaskAttempt#testReducerCustomResourceType can occasionally fail with the following error:

org.apache.hadoop.yarn.exceptions.ResourceNotFoundException: Unknown resource 'a-custom-resource'. Known resources are [name: memory-mb, units: Mi, type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807, name: vcores, units: , type: COUNTABLE, value: 0, minimum allocation: 0, maximum allocation: 9223372036854775807]
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt.createReduceTaskAttemptImplForTest(TestTaskAttempt.java:434)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt.testReducerCustomResourceTypes(TestTaskAttempt.java:1535)


The root cause seems to be an interference from previous tests that start instance(s) of FailingAttemptsMRApp or FailingAttemptsDuringAssignedMRApp. When I disabled these tests, testReducerCustomResourceTypes always passed. 
 Flaky test TestTaskAttempt#testReducerCustomResourceTypes",No
13184070,"resource-types.xml is copied in several tests to the test machine, but it is deleted only at the end of the test. In case the test fails the file will not be deleted and other tests will fail, because of the wrong configuration. 
 Copy of resource-types.xml is not deleted if test fails, causes other test failures",13160779,"JDBC standalone jar contains webapps static files which just adds to the jar size and are not required by the clients.  
 Remove webapps directory from standalone jar",No
13270830,"Utilities.removeTempOrDuplicateFiles
is very slow with cloud storage, as it executes listStatus twice and also runs in single threaded mode.
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L1629 
 Optimise Utilities.removeTempOrDuplicateFiles when moving files to final location",13132541,"
""nextInt % value"" may generate negative values:


int randIdx = rand.nextInt() % allInstances.size();
serviceInstance = allInstances.toArray(serviceInstanceArray)[randIdx];



 
 LlapBaseInputFormat - negative array index",No
13152600,"Upgrade to Hadoop 3.1.0 
 Upgrade  to Hadoop 3.1.0",13152597,"Upgrade to Hadoop 3.1.0 
 Upgrade to Hadoop 3.1.0",yes
13173820,"Using yarn logs command for a long running application which has been running longer than the configured timeline service ttlyarn.timeline-service.ttl-ms fails with the following exception.


Exception in thread ""main"" org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: The entity for application application_152347939332_00001 doesn't exist in the timeline store
at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore.getApplication(ApplicationHistoryManagerOnTimelineStore.java:670)
at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore.getContainers(ApplicationHistoryManagerOnTimelineStore.java:219)
at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService.getContainers(ApplicationHistoryClientService.java:211)
at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationHistoryProtocolPBServiceImpl.getContainers(ApplicationHistoryProtocolPBServiceImpl.java:172)
at org.apache.hadoop.yarn.proto.ApplicationHistoryProtocol$ApplicationHistoryProtocolService$2.callBlockingMethod(ApplicationHistoryProtocol.java:201)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2309)

at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)
at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationHistoryProtocolPBClientImpl.getContainers(ApplicationHistoryProtocolPBClientImpl.java:183)
at org.apache.hadoop.yarn.client.api.impl.AHSClientImpl.getContainers(AHSClientImpl.java:151)
at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getContainers(YarnClientImpl.java:720)
at org.apache.hadoop.yarn.client.cli.LogsCLI.getContainerReportsFromRunningApplication(LogsCLI.java:1089)
at org.apache.hadoop.yarn.client.cli.LogsCLI.getContainersLogRequestForRunningApplication(LogsCLI.java:1064)
at org.apache.hadoop.yarn.client.cli.LogsCLI.fetchApplicationLogs(LogsCLI.java:976)
at org.apache.hadoop.yarn.client.cli.LogsCLI.runCommand(LogsCLI.java:300)
at org.apache.hadoop.yarn.client.cli.LogsCLI.run(LogsCLI.java:107)
at org.apache.hadoop.yarn.client.cli.LogsCLI.main(LogsCLI.java:327)

 
 Fetching yarn logs fails for long running application if it is not present in timeline store",13193505,"A real problem in our production cluster. A user point that his table's data can't be replicate to the peer cluster. Then we start to debug the reason. We checked the replication scope, checked the replication wal entry filter, and check the namespace,tablecfs config. But didn't found any problem. We enabled the RS's debug log to find the reason. Finally, we found use use put with skip wal to write data. But it taked a long time... Our replication use wal to replicate data. So the data can't be replicated to peer cluster. I thought throw a exception may be better for user if the table's replication scope is not 0. (as 0 means not replicated). 
 Throw exception when user put data with skip wal to a table which may be replicated",No
13229927,"If hive.exec.parallel is set to true to parallelize MoveTasks or StatsTasks, the Tez task does not benefit from a new thread and will lose all the thread context of the current query.
Multiple threads even if they are spawned, will lock on SyncDagClient & make progress sequentially. 
 Tez: Prevent TezTasks from escaping thread logging context",13318439,"HIVE-21784 uses a new WriterOptions instead of the field in OrcRecordUpdater:
https://github.com/apache/hive/commit/f62379ba279f41b843fcd5f3d4a107b6fcd04dec#diff-bb969e858664d98848960a801fd58b5cR580-R583
so in this scenario, the overwrite creates an empty bucket file, which is fine as that was the intention of that patch, but it creates that with invalid schema:


CREATE TABLE test_table (
   cda_id             int,
   cda_run_id         varchar(255),
   cda_load_ts        timestamp,
   global_party_id    string)
PARTITIONED BY (
   cda_date           int,
   cda_job_name       varchar(12))
CLUSTERED BY (cda_id) 
INTO 2 BUCKETS
STORED AS ORC;


INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id
UNION ALL
SELECT 2 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id;

ALTER TABLE test_table ADD COLUMNS (group_id string) CASCADE ;

INSERT OVERWRITE TABLE test_table PARTITION (cda_date = 20200601 , cda_job_name = 'core_base')
SELECT 1 as cda_id,'cda_run_id' as cda_run_id, NULL as cda_load_ts, 'global_party_id' global_party_id, 'group_id' as group_id;


because of HIVE-21784, the new empty bucket_00000 shows this schema in orc dump:


Type: struct<_col0:int,_col1:varchar(255),_col2:timestamp,_col3:string,_col4:string>


instead of:


Type: struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<cda_id:int,cda_run_id:varchar(255),cda_load_ts:timestamp,global_party_id:string,group_id:string>>


and this could lead to problems later, when hive tries to look into the file during split generation 
 Empty bucket files are inserted with invalid schema after HIVE-21784",No
13140339,"https://github.com/apache/hadoop/blob/69fa81679f59378fd19a2c65db8019393d7c05a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java


  private ThreadPoolExecutor renewerService;

  private void processDelegationTokenRenewerEvent(
      DelegationTokenRenewerEvent evt) {
    serviceStateLock.readLock().lock();
    try {
      if (isServiceStarted) {
        renewerService.execute(new DelegationTokenRenewerRunnable(evt));
      } else {
        pendingEventQueue.add(evt);
      }
    } finally {
      serviceStateLock.readLock().unlock();
    }
  }

  @Override
  protected void serviceStop() {
    if (renewalTimer != null) {
      renewalTimer.cancel();
    }
    appTokens.clear();
    allTokens.clear();
    this.renewerService.shutdown();




2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)


What I think is going on here is that the serviceStop method is not setting the isServiceStarted flag to 'false'.
Please update so that the serviceStop method grabs the serviceStateLock and sets isServiceStarted to false, before shutting down the renewerService thread pool,to avoid this condition. 
 Race Condition When Stopping DelegationTokenRenewer",13261993,"my namenode metadata getting fullwhat is the solution 
 Release 2.2.2",No
13229189,"https://issues.apache.org/jira/browse/HDFS-14259add the safemode metric, but it is not very straightforward when open the UI of router. We should add in the page somewhere about the safemode is on. 
 RBF: Add safemode to Router UI",13208277,"Status value in ""Router Information"" and in Overview tab, is not matching for ""SAFEMODE"" condition. 
 RBF: Remove hard coded router status from FederationMetrics.",yes
13160616,"It happens rather frequently.
https://builds.apache.org/job/PreCommit-HIVE-Build/11072/testReport/junit/org.apache.hadoop.hive.ql.exec.tez/TestWorkloadManager/testApplyPlanQpChanges/
https://builds.apache.org/job/PreCommit-HIVE-Build/11096/testReport/junit/org.apache.hadoop.hive.ql.exec.tez/TestWorkloadManager/testApplyPlanQpChanges/
We have seen timeouts in other tests frequently in test machines, usually it is sufficient to increase the timeout period.
Cc Sergey Shelukhin Prasanth Jayachandran 
 TestWorkload timeouts",13160141,"Seems like the tests randomly get stuck after the lines like

2018-05-17T01:54:27,111  INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-05-17T01:54:27,603  INFO [TriggerValidator] tez.PerPoolTriggerValidatorRunnable: Creating trigger validator for pool: llap
2018-05-17T01:54:37,090 DEBUG [Thread-28] conf.HiveConf: Found metastore URI of null


Then they get killed by timeout. Happened in the same manner, to random tests in a few separate runs. 
 TestWorkloadManager sometimes hangs",yes
13284719,"All pink bars are misses from cachedstore.




 
 Add caching of table constraints, foreignKeys in CachedStore",13246182,"Currently table constraints are not cached. Hive will pull all constraints from tables involved in query, which results multiple db reads (including get_primary_keys, get_foreign_keys, get_unique_constraints, etc). The effort to cache this is small as it's just another table component. 
 [CachedStore] Add table constraints in CachedStore",yes
13131328,"Clicking 'master container log' or 'Link' next to 'Log' under application's appAttempt goes to Old UI's Log link 
 [UI2] Clicking 'master container log' or 'Link' next to 'log' under application's appAttempt goes to Old UI's Log link",13183001,"If you configure OAuth and then use abfs:// instead of abfss:// it will fail, but it takes a very long time, and still isn't very clear why. Good place to put a good human-readable exception message and fail fast. 
 Fail-fast when using OAuth over http",No
13294364,"There are certain configurations that can be tuned to increase the speed of Decommissioning, But restarting namenode for it seems to be a heavy operation.
Propose to make these configurations, reconfigurable during runtime in order to speed-up decommissioning as per need.
Proposed Configs :

dfs.namenode.replication.work.multiplier.per.iteration
dfs.namenode.replication.max-streams-hard-limit
dfs.namenode.replication.max-streams

 
 Make Decommission related configurations runtime refreshable",13238734,"There are 3 key parameters that control the speed of block replication across the cluster:


dfs.namenode.replication.max-streams
dfs.namenode.replication.max-streams-hard-limit
dfs.namenode.replication.work.multiplier.per.iteration


These are used when decomissioning nodes and when under replicated blocks are being recovered across the cluster. There are times when it may be desirable to increase the speed of replication and then reduce it again (eg during off peak hours) without restarting the namenode.
Therefore it would be good to be able to reconfigure / refresh these parameters dynamically without a namenode restart.
This Jira is to allow these parameters to be refreshed at runtime without a restart. 
 Allow block replication parameters to be refreshable",yes
13274319,"when I am try to run the tests of HDFS, theTestFsck.testFsckListCorruptFilesBlocksand TestFsck.testFsckListCorruptSnapshotFiles tests are easy to fail, see:


06:26:38 [ERROR] Failures: 
06:26:38 [ERROR]   TestFsck.testFsckListCorruptFilesBlocks:1167
06:26:38 [ERROR]   TestFsck.testFsckListCorruptSnapshotFiles:2167
06:26:38 [INFO]
06:26:38 [ERROR] Tests run: 33, Failures: 2, Errors: 0, Skipped: 0


Both of these two tests failures are mainly because the tests will check the number of corrupt files after sleep 1000 ms and the number is not equal to expected. see:

blk_1073741825  /corruptData/8117051706407353421
blk_1073741825  /corruptData/.snapshot/mySnapShot/8117051706407353421
The filesystem under path '/corruptData' has 2 CORRUPT files2. bad fsck include snapshot out: The list of corrupt files under path '/corruptData' are:
blk_1073741825  /corruptData/8117051706407353421
blk_1073741825  /corruptData/.snapshot/mySnapShot/8117051706407353421
The filesystem under path '/corruptData' has 2 CORRUPT files2019-12-13 06:26:35,808 [Listener at localhost/44367] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(2067)) - Shutting down the Mini HDFS Cluster
2019-12-13 06:26:35,808 [Listener at localhost/44367] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2115)) - Shutting down DataNode 0


To fix these two tests, we need to enlarge the sleep time of 1000 ms, according to my testing, enlaging the time to 5000 ms can make the tests passed every times. 
 TestFsck.testFsckListCorruptFilesBlocks and TestFsck.testFsckListCorruptSnapshotFiles fail some times",13272975,"https://builds.apache.org/job/PreCommit-HDFS-Build/28481/testReport/

https://builds.apache.org/job/PreCommit-HDFS-Build/28482/testReport/ 
 TestFsck testFsckListCorruptSnapshotFiles is failing in trunk",yes
13155202,"Support pushing arrow batches through org.apache.arrow.vector.ipc.ArrowOutputStream in LllapOutputFormatService. 
 Support ArrowOutputStream in LlapOutputFormatService",13158584,"""You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."" 
 Arrow SerDe itest failure",yes
13256452,"During endurance testing, we found a race condition that cause an empty localDirs being passed to container-executor.
The problem is that DirectoryCollection.checkDirs() clears three collections:


    this.writeLock.lock();
    try {
      localDirs.clear();
      errorDirs.clear();
      fullDirs.clear();
      ...


This happens in critical section guarded by a write lock. When we start a container, we retrieve the local dirs by calling dirsHandler.getLocalDirs();which in turn invokesDirectoryCollection.getGoodDirs(). The implementation of this method is:


List<String> getGoodDirs() {
    this.readLock.lock();
    try {
      return Collections.unmodifiableList(localDirs);
    } finally {
      this.readLock.unlock();
    }
  }


So we're also in a critical section guarded by the lock. But Collections.unmodifiableList() only returns a view of the collection, not a copy. After we get the view, MonitoringTimerTask.run() might be scheduled to run and immediately clears localDirs.
This caused a weird behaviour in container-executor, which exited with error code 35 (COULD_NOT_CREATE_WORK_DIRECTORIES).
Therefore we can't just return a view, we must return a copy with ImmutableList.copyOf().
Credits to Szilard Nemeth for analyzing and determining the root cause. 
 Race condition when DirectoryCollection.checkDirs() runs during container launch",13185680,"We started using CGroups with LinuxContainerExecutor recently, running Apache Hadoop 3.0.0. Occasionally (once out of many millions of tasks) a yarn container will fail with a message like the following:


[2018-09-02 23:48:02.458691] 18/09/02 23:48:02 INFO container.ContainerImpl: Container container_1530684675517_516620_01_020846 transitioned from SCHEDULED to RUNNING
[2018-09-02 23:48:02.458874] 18/09/02 23:48:02 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1530684675517_516620_01_020846
[2018-09-02 23:48:02.506114] 18/09/02 23:48:02 WARN privileged.PrivilegedOperationExecutor: Shell execution returned exit code: 35. Privileged Execution Operation Stderr:
[2018-09-02 23:48:02.506159] Could not create container dirsCould not create local files and directories
[2018-09-02 23:48:02.506220]
[2018-09-02 23:48:02.506238] Stdout: main : command provided 1
[2018-09-02 23:48:02.506258] main : run as user is nobody
[2018-09-02 23:48:02.506282] main : requested yarn user is root
[2018-09-02 23:48:02.506294] Getting exit code file...
[2018-09-02 23:48:02.506307] Creating script paths...
[2018-09-02 23:48:02.506330] Writing pid file...
[2018-09-02 23:48:02.506366] Writing to tmp file /path/to/hadoop/yarn/local/nmPrivate/application_1530684675517_516620/container_1530684675517_516620_01_020846/container_1530684675517_516620_01_020846.pid.tmp
[2018-09-02 23:48:02.506389] Writing to cgroup task files...
[2018-09-02 23:48:02.506402] Creating local dirs...
[2018-09-02 23:48:02.506414] Getting exit code file...
[2018-09-02 23:48:02.506435] Creating script paths...



Looking at the container executor source it's traceable to errors here: https://github.com/apache/hadoop/blob/release-3.0.0-RC1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c#L1604
 And ultimately to https://github.com/apache/hadoop/blob/release-3.0.0-RC1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c#L672
The root failure seems to be in the underlying mkdir call, but that exit code / errno is swallowed so we don't have more details. We tend to see this when many containers start at the same time for the same application on a host, and suspect it may be related to some race conditions around those shared directories between containers for the same application.
For example, this is a typical pattern in the audit logs:


[2018-09-07 17:16:38.447654] 18/09/07 17:16:38 INFO nodemanager.NMAuditLogger: USER=root	IP=<> Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1530684675517_559126	CONTAINERID=container_1530684675517_559126_01_012871
[2018-09-07 17:16:38.492298] 18/09/07 17:16:38 INFO nodemanager.NMAuditLogger: USER=root	IP=<> Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1530684675517_559126	CONTAINERID=container_1530684675517_559126_01_012870
[2018-09-07 17:16:38.614044] 18/09/07 17:16:38 WARN nodemanager.NMAuditLogger: USER=root	OPERATION=Container Finished - Failed	TARGET=ContainerImpl	RESULT=FAILURE	DESCRIPTION=Container failed with state: EXITED_WITH_FAILURE	APPID=application_1530684675517_559126	CONTAINERID=container_1530684675517_559126_01_012871



Two containers for the same application starting in quick succession followed by the EXITED_WITH_FAILURE step (exit code 35).
We plan to upgrade to 3.1.x soon but I don't expect this to be fixed by this, the only major JIRAs that affected the executor since 3.0.0 seem unrelated (https://github.com/apache/hadoop/commit/bc285da107bb84a3c60c5224369d7398a41db2d8 and https://github.com/apache/hadoop/commit/a82be7754d74f4d16b206427b91e700bb5f44d56) 
 LinuxContainerExecutor fails sporadically in create_local_dirs",yes
13180570,"This is handled via write ID being invalid for the stats. 
 txn stats cleanup in compaction txn handler is unneeded",13167771,"YARN-8319 filter check for flows pages. The same behavior need to be added for all other REST API as long as ATS provides support for ACLs 
 Add basic ACL check for all ATS v2 REST APIs",No
13143977,"The service definition is persisted to diskprior to launching the application. Once the application is launched, the service definition is updated to include the application ID. If submit fails, the application ID is never added to the previously persisted service definition.
When this occurs, attempting to stop or destroy the application results in a NPEwhile trying to get the application ID from the service definition, making it impossible to clean up.


2018-03-02 18:28:05,512 INFO org.apache.hadoop.yarn.service.utils.ServiceApiUtil: Loading service definition from hdfs://y7001.yns.hortonworks.com:8020/user/hadoopuser/.yarn/services/skumpfcents/skumpfcents.json
2018-03-02 18:28:05,525 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
java.lang.NullPointerException
	at org.apache.hadoop.yarn.api.records.ApplicationId.fromString(ApplicationId.java:111)
	at org.apache.hadoop.yarn.service.client.ServiceClient.getAppId(ServiceClient.java:1106)
	at org.apache.hadoop.yarn.service.client.ServiceClient.actionStop(ServiceClient.java:363)
	at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:251)
	at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:243)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
 
 Validate the application ID has been persisted to the service definition prior to use",13141301,"The service records written to the registry are removed by ServiceClient on a destroy call, but not on a stop call. The service AM does have some code to clean up the registry entries when component instances are stopped, but if the AM is killed before it has a chance to perform the cleanup, these entries will be left in ZooKeeper. It would be better to clean these up in the stop call, so that RegistryDNS does not provide lookups for containers that don't exist.
Additional stop/destroy behavior improvements include fixing errors / unexpected behavior related to:

destroying a saved (not launched or started) service
destroying a stopped service
destroying a destroyed service
returning proper exit codes for destroy failures
performing other client operations on saved services (fixing NPEs)

 
 Delete registry entries from ZK on ServiceClient stop and clean up stop/destroy behavior",yes
13298387,"The following highlighted line seems to be incorrect in the test suite:
https://github.com/apache/hive/blob/master/ql/src/test/results/clientpositive/perf/tez/cbo_query95.q.out#L89
Note that the project takes all the columns from the table scan, yet it only needs a couple of them.
I did some very small debugging on this. When I removed the applyJoinOrderingTransform here:https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L1897
... the problem goes away. So presumably one of the rules in there is causing the problem.
Here is a slightly simplified version of the query which has the same problem (using the same tpc-ds database):
explain cbo with ws_wh as
(select ws1.ws_order_number
from web_sales ws1,web_returns wr2 
where ws1.ws_order_number = wr2.wr_order_number)
select
 ws_order_number
from
 web_sales ws1 
where
ws1.ws_order_number in (select wr_order_number
              from web_returns,ws_wh
              where wr_order_number = ws_wh.ws_order_number)
 
 Project not defined correctly after reordering a join",13151594,"added in HIVE-18976 
the property key druid.kafka.query.files doesn't exists in testconfiguration.properties.
because of this TestMiniDruidKafkaCliDriver tries to run all qtests...which time out...and produce 


TestMiniDruidKafkaCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=252)


 
 Fix TestMiniDruidKafkaCliDriver",No
13229576,"

[ERROR] Tests run: 36, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 40.165 s <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption
[ERROR] testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes[FairSharePreemption](org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption)  Time elapsed: 10.519 s  <<< FAILURE!
java.lang.AssertionError: Incorrect # of containers on the greedy app expected:<6> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyPreemption(TestFairSchedulerPreemption.java:296)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyRelaxLocalityPreemption(TestFairSchedulerPreemption.java:537)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes(TestFairSchedulerPreemption.java:473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

 
 TestFairSchedulerPreemption#testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes failed in Jenkins",13313461,"
The following 2 test cases are failing on unrelated patches very often:

hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler
hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption
Here is an example of both failures


[ERROR] Tests run: 105, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 27.481 s <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler
[ERROR] testNormalizationUsingQueueMaximumAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler)  Time elapsed: 0.178 s  <<< ERROR!
org.apache.hadoop.metrics2.MetricsException: Metrics source PartitionQueueMetrics,partition= already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics.getPartitionMetrics(QueueMetrics.java:360)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics.incrPendingResources(QueueMetrics.java:599)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.updatePendingResources(AppSchedulingInfo.java:399)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.internalAddResourceRequests(AppSchedulingInfo.java:331)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.internalAddResourceRequests(AppSchedulingInfo.java:358)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.updateResourceRequests(AppSchedulingInfo.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.updateResourceRequests(SchedulerApplicationAttempt.java:462)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:931)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler.allocateAppAttempt(TestFairScheduler.java:435)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler.testNormalizationUsingQueueMaximumAllocation(TestFairScheduler.java:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)




[ERROR] Tests run: 40, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 58.843 s <<< FAILURE! - in org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption
[ERROR] testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes[MinSharePreemption](org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption)  Time elapsed: 10.709 s  <<< FAILURE!
java.lang.AssertionError: Incorrect # of containers on the greedy app expected:<6> but was:<4>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyPreemption(TestFairSchedulerPreemption.java:289)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.verifyRelaxLocalityPreemption(TestFairSchedulerPreemption.java:542)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption.testRelaxLocalityPreemptionWithNoLessAMInRemainingNodes(TestFairSchedulerPreemption.java:478)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


 
 Flaky test cases in Fair Scheduler",yes
13142719,"In current ConnectionPool.getConnection(), it will return the first active connection:


for (int i=0; i<size; i++) {
  int index = (threadIndex + i) % size;
  conn = tmpConnections.get(index);
  if (conn != null && !conn.isUsable()) {
    return conn;
  }
}


Here ""!conn.isUsable()"" should be ""conn.isUsable()"". 
 RBF: ConnectionPool should return first usable connection",13231747,"We do failover test and throw a leak error, this is hard to reproduce.



2019-05-06 02:30:27,781 ERROR [AsyncFSWAL-0] util.ResourceLeakDetector: LEAK: ByteBuf.release() was not called before it's garbage-collected. See http://netty.io/wiki/reference-counted-objects.html for more information.
Recent access records:
Created at:
 org.apache.hbase.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:334)
 org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
 org.apache.hbase.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
 org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.flush0(FanOutOneBlockAsyncDFSOutput.java:494)
 org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.flush(FanOutOneBlockAsyncDFSOutput.java:513)
 org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter.sync(AsyncProtobufLogWriter.java:144)
 org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.sync(AsyncFSWAL.java:353)
 org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.consume(AsyncFSWAL.java:536)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 java.lang.Thread.run(Thread.java:748)



If FanOutOneBlockAsyncDFSOutput#endBlock throw Exception before call ""buf.release();"", this buf has not chance to release.
In CallRunner if the call skipped or Dropping timed out call, the call do not call cleanup. 
 ByteBuf LEAK ERROR",No
13290843,"Our tests can create thousands of threads all up in the one JVM. Using less means less memory, less contention, and hopefully, likelier passes.
I've been studying the likes of TestNamespaceReplicationWithBulkLoadedData to see what it does as it runs (this test puts up 4 clusters with replication between). It peaks at 2k threads. After some configuration and using less HDFS, can get it down to ~800 threads and about 1/2 the memory-used.
(HDFS is a profligate offender. DataXceivers (Server and Client), jetty threads, Volume threads (async disk 'worker' then another for cleanup...), image savers, ipc clients – new thread per incoming connection w/o bound (or reuse), block responder threads, anonymous threads, and so on. Many are not configurable or boundable or are hard-coded; e.g. each volume gets 4 workers. Biggest impact was to be had by downing the count of data nodes. TODO: a follow-on that turns down DN counts in all tests)
I've been using Java Flight Recorder during this study. Here is how you get a flight recorder for the a single test run:


MAVEN_OPTS="" -XX:StartFlightRecording=disk=true,dumponexit=true,filename=recording.jfr,settings=profile,path-to-gc-roots=true,maxsize=1024m""  mvn  test -Dtest=TestNamespaceReplicationWithBulkLoadedData -Dsurefire.firstPartForkCount=0 -Dsurefire.secondPartForkCount=0 

i.e. start recording on mvn launch, bound the size of the recording, and have the test run in the mvn context (DON'T fork). Useful is connecting to the running test at the same time from JDK Mission Control. We do the latter because the thread reporting screen is overwhelmed by the count of running threads and if you connect live, you can at least get a 'live threads' graph w/ count as the test progresses. Useful.
When the test finishes, it dumps a .jfr file which can be opened in JDK MC. I've been compiling w/ JDK8 and then running w/ JDK11 so I can use JDK MC Version 7, the non-commercial latest. Works pretty well.
Let me put up a patch for tests that cuts down thread counts where we can.

 
 Have test runs use less resources",13290844,"Our tests can create thousands of threads all up in the one JVM. Using less means less memory, less contention, likelier passes, and later, more possible parallelism.
I've been studying the likes of TestNamespaceReplicationWithBulkLoadedData to see what it does as it runs (this test puts up 4 clusters with replication between). It peaks at 2k threads. After some configuration and using less HDFS, its possible to get it down to ~800 threads and about 1/2 the memory-used. HDFS is a main offender. DataXceivers (Server and Client), jetty threads, Volume threads (async disk 'worker' then another for cleanup...), image savers, ipc clients – new thread per incoming connection w/o bound (or reuse), block responder threads, anonymous threads, and so on. Many are not configurable or boundable or are hard-coded; e.g. each volume gets 4 workers regardless. Biggest impact was just downing the count of data nodes. TODO: a follow-on that turns down DN counts in all tests.
I've been using Java Flight Recorder during this study. Here is how you get a flight recorder for the a single test run: {code:java} MAVEN_OPTS="" -XX:StartFlightRecording=disk=true,dumponexit=true,filename=recording.jfr,settings=profile,path-to-gc-roots=true,maxsize=1024m"" mvn test -Dtest=TestNamespaceReplicationWithBulkLoadedData -Dsurefire.firstPartForkCount=0 -Dsurefire.secondPartForkCount=0 {code} i.e. start recording on mvn launch, bound the size of the recording, and have the test run in the mvn context (DON'T fork). Useful is connecting to the running test at the same time from JDK Mission Control. We do the latter because the thread reporting screen is overwhelmed by the count of running threads and if you connect live, you can at least get a 'live threads' graph w/ count as the test progresses. Useful. When the test finishes, it dumps a .jfr file which can be opened in JDK MC.
I've been compiling w/ JDK8 and then running w/ JDK11 so I can use JDK MC Version 7, the non-commercial latest. Works pretty well. Let me put up a patch for tests that cuts down thread counts where we can.
Let me put up a patch that does first pass on curtailing resource usage. 
 Use less resources running tests",yes
13158689,"""guarded"" is misspelled as ""gaurded"" 
  ""guarded"" is misspelled as ""gaurded"" in FSPermissionChecker.java",13201213,"Each record writer registers a memory monitor with theMemoryMXBean but they aren't removed. So the listener objects/lambdas accumulate over time in the bean. 
 streaming/AbstractRecordWriter leaks HeapMemoryMonitor",No
13204251,"If HBASE_WAL_DIR if not configured, then 
recovered.edits dir path should be old method only.
If user is creating x no. of tables, in different namespaces, then all are creating in the ""hbase.rootdir"" path only.


/<hbase.rootdir>/data/<namespace>/<table>/<regionDir>/recovered.edits
eg:
/hbase/data/default/testTable/eaf343d35d3e66e6e5fd38106ba61c62/recovered.edits


But the format is currently. 


/<hbase.rootdir>/<namespace>/<table>/<regionDir>/recovered.edits
eg:
/hbase/default/testTable/eaf343d35d3e66e6e5fd38106ba61c62/recovered.edits

 
 HBASE_WAL_DIR if not configured, recovered.edits directory's are sidelined from the table dir path.",13241533,"If you call s3guard init s3a://name/ then the custom bucket options of fs.s3a.bucket.name are not picked up, instead the global value is used.
Fix: take the name of the bucket and use that to eval properties and patch the config used for the init command. 
 Port HBASE-22617 (Recovered WAL directories not getting cleaned up) to branch-1",yes
13136865,"I'm using latest master branch code and mysql as metastore.
Creating table hits this error:

2018-02-07T22:04:55,438 ERROR [41f91bf4-bc49-4a73-baee-e2a1d79b8a4e main] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MTable@28d16af8"" using statement ""INSERT INTO `TBLS` (`TBL_ID`,`CREATE_TIME`,`CREATION_METADATA_MV_CREATION_METADATA_ID_OID`,`DB_ID`,`LAST_ACCESS_TIME`,`OWNER`,`RETENTION`,`IS_REWRITE_ENABLED`,`SD_ID`,`TBL_NAME`,`TBL_TYPE`,`VIEW_EXPANDED_TEXT`,`VIEW_ORIGINAL_TEXT`) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)"" failed : Unknown column 'CREATION_METADATA_MV_CREATION_METADATA_ID_OID' in 'field list'
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)
        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:729)
        at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:749)
        at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:1125)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
        at com.sun.proxy.$Proxy36.createTable(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1506)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1412)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1614)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 
 Cannot create table: ""message:Exception thrown when executing query : SELECT DISTINCT..""",13294516,"java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createLocatedBlock(BlockManager.java:1362)
at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createLocatedBlocks(BlockManager.java:1501)
at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:179)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2047)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770) 
 ArrayIndexOfboundsException in BlockManager CreateLocatedBlock",No
13143667,"If you use distcp -update to an adl or wasb store, repeatedly, all the source files are copied up every time. In contrast, if you use hdfs:// or s3a:// as a destination, only the new ones are uploaded. hdfs uses checksums for a diff, but s3a is just returning file length and relying on distcp logic being ""if either src or dest doesn't do checksums, only compare file len""
somehow that's not kicking in. Tested for file:  and hdfs sources, wasb and adl dests 
 distcp -update to WASB and ADL copies up all the files, always",13326822,"We usedistcp with -update option to copy a dir from hdfs to S3. When we run distcp jobonce more, it will overwrite S3 dir directly, rather than skip the same files.
 
 Test Case:
Run twice the following cmd, the modify time of S3 files will be modified every time.
 hadoop distcp -update/test/ s3a://${s3_buckect}/test/

Check code in CopyMapper.java andS3AFileSystem.java
(1) For the first time, distcp job will create files in S3, but blockSize isunused!


(2)For the second time, the distcp job will compare fileSize and blockSize between hdfs file and S3 file


(3) blockSize is unused, when get blockSize of S3 file, it return a default value.
In S3AFileSystem.java, we find that the default value offs.s3a.block.size is 32 * 1024 * 1024.



 
The blockSize of HDFS seems invalid in Object Store, like S3. So I think there's no need to compare blockSize when distcp with -update option. 
 DistCp -update option will be invalid when distcp files from hdfs to S3",yes
13321894,"For a table with 3000+ partitions, analyze table takes a lot longer time as HiveMetaStoreAuthorizer tries to create HiveConf for every partition request.

https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java#L319

https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/metastore/HiveMetaStoreAuthorizer.java#L447 
 Optimise HiveMetaStoreAuthorizer.createHiveMetaStoreAuthorizer",13282563,"Stacktrace may not match exactly master branch. But in master as well, it is not very different.

at org.apache.hadoop.util.StringInterner.weakIntern(StringInterner.java:71)
	at org.apache.hadoop.conf.Configuration$Parser.handleEndElement(Configuration.java:3273)
	at org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3354)
	at org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3137)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3030)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2991)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2871)
	- locked <0x00000005cbe60ad0> (a org.apache.hadoop.mapred.JobConf)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1389)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1361)
	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:518)
	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:536)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:430)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5482)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5450)
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.createHiveMetaStoreAuthorizer(HiveMetaStoreAuthorizer.java:450)
	at org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.onEvent(HiveMetaStoreAuthorizer.java:100)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.firePreEvent(HiveMetaStore.java:3835)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database_req(HiveMetaStore.java:1655)
	at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
	at com.sun.proxy.$Proxy28.get_database_req(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database_req.getResult(ThriftHiveMetastore.java:15671)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_database_req.getResult(ThriftHiveMetastore.java:15655)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)

 
 HiveMetaStoreAuthorizer parses conf on every HMS invocation",yes
13142472,"RMcrashes with NPE during failover becauseACL configurations were changed as a result we no longer have a rights to submit anapplication to a queue.
Scenario:

Submit an application
Change ACL configuration for a queue that accepted the application so that an owner of the application will no longer have a rights to submit this application.
Restart RM.

As a result, we get NPE:
2018-02-27 18:14:00,968 INFO org.apache.hadoop.service.AbstractService: Service ResourceManager failed in state STARTED; cause: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplicationAttempt(FairScheduler.java:738)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1286)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:116)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1098)
	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1044)
	at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385 
 RM crashes with NPE during recovering if ACL configuration was changed",13137405,"There are edge cases when the application recovery fails with an exception.
Example failure scenario:

setup: a queue is a leaf queue in the primary RM's config and the same queue is a parent queue in the secondary RM's config.
When failover happens with this setup, the recovery will fail for applications on this queue, and an APP_REJECTED event will be dispatched to the async dispatcher. On the same thread (that handles the recovery), a NullPointerException is thrown when the applicationAttempt is tried to be recovered (https://github.com/apache/hadoop/blob/55066cc53dc22b68f9ca55a0029741d6c846be0a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java#L494). I don't see a good way to avoid the NPE in this scenario, because when the NPE occurs the APP_REJECTED has not been processed yet, and we don't know that the application recovery failed.

Currently the first exception will abort the recovery, and if there are X applications, there will be ~X passive -> active RM transition attempts - the passive -> active RM transition will only succeed when the last APP_REJECTED event is processed on the async dispatcher thread. 
 Improve error handling when application recovery fails with exception",yes
13245922,"I have managed to adapt SLS wrapper into FIFO scheduler. It is currently half-working (container allocations are not traced, but all else works). Going to upload patch shortly. 
 Add SLS support for FIFO Scheduler",13264261,"[impadmin@impetus-g031 ~]$ beeline
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://impetus-dsrv11.impetus.co.in:2181,ct-n0066.impetus.co.in:2181,ct-n0092.impetus.co.in:2181/default;principal=hive/_HOST@IMPETUS.CO.IN;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
19/10/24 19:03:42 [main]: INFO jdbc.HiveConnection: Connected to ct-n0092.impetus.co.in:10000
Connected to: Apache Hive (version 3.1.0.3.1.0.0-78)
Driver: Hive JDBC (version 3.1.0.3.1.0.0-78)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.0.3.1.0.0-78 by Apache Hive
0: jdbc:hive2://impetus-dsrv11.impetus.co.in:> select minute('12:58:59');
INFO : Compiling command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395): select minute('12:58:59')
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)
INFO : Completed compiling command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395); Time taken: 0.427 seconds
INFO : Executing command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395): select minute('12:58:59')
INFO : Completed executing command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395); Time taken: 0.003 seconds
INFO : OK
-------



 _c0 



-------



 NULL 



-------
1 row selected (0.739 seconds)
0: jdbc:hive2://impetus-dsrv11.impetus.co.in:> 
 UDF minute with time returns NULL",No
13258198,"It's a simple bug, caused by RouterClientProtocl#821, a broken path is passed to getMountPointStatus(). We should pass the full path of the child. 
The original test case TestRouterMountTable.testMountTablePermissionsNoDest() doesn't cover enough cases, so I extend it. Test case TestRouterMountTable.testGetMountPointStatusWithIOException() is also broken, userB and groupB should be set to entry /testA/testB. 
 RBF: RouterRpcServer.getListing() returns wrong owner, group and permission.",13250999,"


source
target namespace
destination
owner
group
permission


/mnt
ns0
/mnt
mnt
mnt_group
755


/mnt/test1
ns1
/mnt/test1
mnt_test1
mnt_test1_group
755


/test1
ns1
/test1
test1
test1_group
755



When do getListing(""/mnt""), the owner of /mnt/test1should be mnt_test1 instead of test1 in result.

And if the mount table as blew, we should support getListing(""/mnt"") instead of throw IOException whendfs.federation.router.default.nameservice.enable is false.



source
target namespace
destination
owner
group
permission


/mnt/test1
ns0
/mnt/test1
test1
test1
755


/mnt/test2
ns1
/mnt/test2
test2
test2
755




 
 RBF: LS command for mount point shows wrong owner and permission information.",yes
13261775,"Fair Scheduler supports limiting the number of applications that a particular user can submit:

<user name=""someuser"">
  <maxRunningApps>10</maxRunningApps>
</user>


Capacity Scheduler does not have an exact equivalent. 
 Capacity scheduler: add support for limiting maxRunningApps per user",13263804,"In FairScheduler, there has limitation for max running which will let application pending.
But in CapacityScheduler there has no feature like max running app.Only got max app,and jobs will be rejected directly on client.
This jira i want to implement this semantic for CapacityScheduler. 
 Support max running app logic for CapacityScheduler",yes
13207986,"Found the problem when implementing HBASE-21671, TestRegionReplicaFailover is easy to fail. In this test we set REGION_REPLICA_WAIT_FOR_PRIMARY_FLUSH_CONF_KEY to true, which means the secondary replicas can not be read until the primary region has been successfully flushed. But in RegionReplicaFlushHandler, there is only one place where we call region.setReadsEnabled(true), this is incorrect. 
 Reset readsEnabled flag after successfully flushing the primary region",13320092,"
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java


aggregationBuffer = allocateAggregationBuffer(); 

Flushed out aggregation buffers could be reused instead of allocating everytime here, to reduce GC pressure. 
 Reuse VectorAggregationBuffer to reduce GC pressure in VectorGroupByOperator",No
13294199,"Using S3A URL scheme while writing out data from Spark to S3 is creating many folder level delete markers.
Writing the same with S3 URL scheme, does not create any delete markers at all.

Spark - 2.4.4
Hadoop - 3.2.1
EMR version - 6.0.0
Write Mode - Append


[hadoop@ip-192-0-161-212 ~]$ spark-shell
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
20/03/27 07:37:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Spark context Web UI available at http://ip-192-0-161-212.ec2.internal:4040
Spark context available as 'sc' (master = yarn, app id = application_1585294390030_0003).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val df = spark.sql(""select 1 as a"")
df: org.apache.spark.sql.DataFrame = [a: int]

scala> df.write.mode(org.apache.spark.sql.SaveMode.Append).save(""s3://my-bucket/tmp/vijayant/test/s3/"")
                                                                                
scala> df.write.mode(org.apache.spark.sql.SaveMode.Append).save(""s3a://my-bucket/tmp/vijayant/test/s3a/"")
                                                                                
scala> 


Getting delete markers from `s3` write


aws s3api list-object-versions --bucket my-bucket --prefix tmp/vijayant/test/s3/
{
    ""Versions"": [
        {
            ""LastModified"": ""2020-03-27T07:38:17.000Z"", 
            ""VersionId"": ""V06OzeE7j221Tq7keSGj8bveCYyJFIcf"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3/_SUCCESS"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:38:16.000Z"", 
            ""VersionId"": ""dLYtHDugLhFIdw2YHLFmoFOxXkm.21Wo"", 
            ""ETag"": ""\""26e70a1e26c709e3e8498acd49cfaaa3-1\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3/part-00000-9d9a8925-f119-415d-b547-b742396e2ca7-c000.snappy.parquet"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 384
        }
    ]
} 


Getting delete markers from `s3a` write


aws s3api list-object-versions --bucket my-bucket --prefix tmp/vijayant/test/s3a/

{
    ""DeleteMarkers"": [
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""NJWRZMcb_eYYwCJh_isX4H1Ox6W362Wb"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""F0h0mLcVVwkMtcHxd95Hj7BACL4Up_Q9"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:10.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": "".sBcE6cXeggekOnSgZ4n7pyCDHnsLERK"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:10.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""nzm39jiUPC4H0ZaS.5Shp0FYPnR8wNf9"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""BPM65R1HkZngPDYtDL3zPZYPw_G_m9Ic"", 
            ""Key"": ""tmp/vijayant/test/s3a/"", 
            ""LastModified"": ""2020-03-27T07:39:08.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""LJt8_MVDOiD4UdgUqEMycxjvtinJlTNt"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""RqunJTn8Od0PgFR4yu44PX4kL54k6EDv"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""4vY8cnqUI5VJAk3VfEt_VD_KEczo3bmY"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:08.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""ln47YYy.yiE.k70cvqvfgYCEQoYFnKQW"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""5Bsrt7s1caM90mzGNgk0MsTU9q8UjTTA"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""pN3HzDfnmqIqrMwAL2jqKEBkvoHZALor"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""wg91poO1KXReXxvsZHzZXrHR1IgIX8t2"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""cv5Noykq3sMilQqJXAH3E.N7qAWnIBx7"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/"", 
            ""LastModified"": ""2020-03-27T07:39:11.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""VersionId"": ""6xzt9SxlCUJaOLD8krkE3yXfQU14rErX"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/"", 
            ""LastModified"": ""2020-03-27T07:39:09.000Z""
        }, 
        {
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""VersionId"": ""wGmJAo7x_gkLWAiHzxPGdPMVSus7Wcp1"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/part-00000-3923e1b1-406c-4202-b9a8-3bd7cb2d97b2-c000.snappy.parquet"", 
            ""LastModified"": ""2020-03-27T07:39:10.000Z""
        }
    ], 
    ""Versions"": [
        {
            ""LastModified"": ""2020-03-27T07:39:11.000Z"", 
            ""VersionId"": ""2py_ZXKl7yh6fwhzksAx8Os1BriDJCBb"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_SUCCESS"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:08.000Z"", 
            ""VersionId"": ""lDqTnLCqDYtjrOiY.V7E6AKTRQLKrqUT"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:10.000Z"", 
            ""VersionId"": ""g.rGoTDdmrGrNjrLchvwz3jMmGePkgiD"", 
            ""ETag"": ""\""d41d8cd98f00b204e9800998ecf8427e\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""Size"": 0
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:09.000Z"", 
            ""VersionId"": "".ZCpY2UW4hRlbLL87dFUJRuk021Hyq8p"", 
            ""ETag"": ""\""3def7238a0858c17c62d7045290175cf\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/_temporary/0/_temporary/attempt_20200327073907_0001_m_000000_1/part-00000-3923e1b1-406c-4202-b9a8-3bd7cb2d97b2-c000.snappy.parquet"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": false, 
            ""Size"": 384
        }, 
        {
            ""LastModified"": ""2020-03-27T07:39:10.000Z"", 
            ""VersionId"": ""JSNjTDHSQqe9zSAV93bc6TXPuqA.vDJE"", 
            ""ETag"": ""\""3def7238a0858c17c62d7045290175cf\"""", 
            ""StorageClass"": ""STANDARD"", 
            ""Key"": ""tmp/vijayant/test/s3a/part-00000-3923e1b1-406c-4202-b9a8-3bd7cb2d97b2-c000.snappy.parquet"", 
            ""Owner"": {
                ""DisplayName"": ""sysops+stage"", 
                ""ID"": ""08939105f417dc74b1fa237e211185ff2d9f528d54b1380501de07bd0657b5e1""
            }, 
            ""IsLatest"": true, 
            ""Size"": 384
        }
    ]
}



This in turn makes listing objects slow and we have even noticed timeouts due to too many delete markers. 
 S3A creating folder level delete markers",13283718,"Add support for doing ""less"" about directory markers. Make this an experimental option for now, to see what breaks.
Rationale

reduce throttling from deletes
reduce tombstone markers in versioned buckets

 
 add experimental optimization of s3a directory marker handling",yes
13138196,"Enable Hive on TEZ. (MR works fine).
STEP 1. Create test data


nano /home/test/users.txt


Add to file:


Peter,34
John,25
Mary,28




hadoop fs -mkdir /bug
hadoop fs -copyFromLocal /home/test/users.txt /bug
hadoop fs -ls /bug


EXPECTED RESULT:


Found 2 items                                                                   
-rwxr-xr-x   3 root root         25 2015-10-15 16:11 /bug/users.txt


STEP 2. Upload data to hive


create external table bug(name string, age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' LOCATION '/bug';
select * from bug;


EXPECTED RESULT:


OK
Peter   34
John    25
Mary    28




create external table bug1(name string, age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' LOCATION '/bug1';
insert overwrite table bug select * from bug1;
select * from bug;


EXPECTED RESULT:


OK
Time taken: 0.097 seconds


ACTUAL RESULT:


hive>  select * from bug;
OK
Peter	34
John	25
Mary	28
Time taken: 0.198 seconds, Fetched: 3 row(s)

 
 INSERT OVERWRITE TABLE doesn't clean the table directory before overwriting",13321632,"This was a bug we had seen multiple times on Hadoop 2.6.2. And the following analysis is based on the core dump, logs, and code in 2017 with Hadoop 2.6.2. We hadn't seen it after 2.9 in our env. However, it was because of the RPC retry policy change and other changes. There's still a possibility even with the current code if I didn't miss anything.
High-level description:
 We had seen a starving mapper issue several times. The MR job stuck in a live lock state and couldn't make any progress. The queue is full so the pending mapper can’t get any resource to continue, and the application master failed to preempt the reducer, thus causing the job to be stuck. The reason why the application master didn’t preempt the reducer was that there was a leaked container in assigned mappers. The node manager failed to report the completed container to the resource manager.
Detailed steps:


Container_1501226097332_249991_01_000199 was assigned to attempt_1501226097332_249991_m_000095_0 on 2017-08-08 16:00:00,417.


appmaster.log:6464:2017-08-08 16:00:00,417 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1501226097332_249991_01_000199 to attempt_1501226097332_249991_m_000095_0


The container finished on 2017-08-08 16:02:53,313.


yarn-mapred-nodemanager-.log.1:2017-08-08 16:02:53,313 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1501226097332_249991_01_000199 transitioned from RUNNING to EXITED_WITH_SUCCESS
yarn-mapred-nodemanager-.log.1:2017-08-08 16:02:53,313 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1501226097332_249991_01_000199


The NodeStatusUpdater go an exception in the heartbeat on 2017-08-08 16:07:04,238. In fact, the heartbeat request is actually handled by resource manager, however, the node manager failed to receive the response. Let’s assume the heartBeatResponseId=$hid in node manager. According to our current configuration, next heartbeat will be 10s later.


2017-08-08 16:07:04,238 ERROR org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Caught exception in status-updater
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ; destination host is: XXXXXXX
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
        at org.apache.hadoop.ipc.Client.call(Client.java:1472)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy33.nodeHeartbeat(Unknown Source)
        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.nodeHeartbeat(ResourceTrackerPBClientImpl.java:80)
        at sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy34.nodeHeartbeat(Unknown Source)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:597)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at java.io.FilterInputStream.read(FilterInputStream.java:133)
        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:513)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)


NodeStatusUpdaterImpl try to send another heart beart 10s after last exception. Container_1501226097332_249991_01_000199 was added to the heartbeat request as a completed container for the first time. We can confirm this point from the timestamp in recentlyStoppedContainers@NodeStatusUpdaterImpl.


Java.util.LinkedHashMap$Entry@0x7889caca0
http://localhost:7000/object/0x7889cacc8
Value 1502209034239 
Time add to recently stopped 
1502208434239 = 1502209034239 - 600000
$date -d@1502208434
Tue Aug  8 16:07:14 UTC 2017


RM thought the request as duplication heartbeat as the $hid is the same as the heartBeatId it received last time. So it returned the last response generated from previous heartbeat and didn’t handle the request, in other words, it didn't add container_1501226097332_249991_01_000199 as a completed container to its own data structure.


// 3. Check if it's a 'fresh' heartbeat i.e. not duplicate heartbeat
NodeHeartbeatResponse lastNodeHeartbeatResponse = rmNode.getLastNodeHeartBeatResponse();
if (remoteNodeStatus.getResponseId() + 1 == lastNodeHeartbeatResponse
   .getResponseId()) {
 LOG.info(""Received duplicate heartbeat from node ""
     + rmNode.getNodeAddress()+ "" responseId="" + remoteNodeStatus.getResponseId());
 return lastNodeHeartbeatResponse;


Node manager received the response and clear container_1501226097332_249991_01_000199 from pendingCompletedContainers@nodeStatusUpdaterImpl. However, container_1501226097332_249991_01_000199 is still in recentlyStoppedContainers@NodeStautsUpdaterImp. So the container won’t report container_1501226097332_249991_01_000199 again as a complete container in the heartbeat request.


             if (containerStatus.getState() == ContainerState.COMPLETE) {
 if (isApplicationStopped(applicationId)) {
   if (LOG.isDebugEnabled()) {
     LOG.debug(applicationId + "" is completing, "" + "" remove ""
         + containerId + "" from NM context."");
   }
   context.getContainers().remove(containerId);
   pendingCompletedContainers.put(containerId, containerStatus);
 } else {
   if (!isContainerRecentlyStopped(containerId)) {
     pendingCompletedContainers.put(containerId, containerStatus);
     // Adding to finished containers cache. Cache will keep it around at
     // least for #durationToTrackStoppedContainers duration. In the
     // subsequent call to stop the container will get removed from cache.
     addCompletedContainer(containerId);
   }
 }
} else {
 containerStatuses.add(containerStatus);
}


The application master relies on getResourses() to get completed containers and remove them from assignedRequests. As the container will never be reported to the RM by nodemanger again, thus application master can’t remove the attempt associated with the container. And the preemption calculation returns false due to following codes, thus causing the starving mapper.


boolean preemptReducesIfNeeded() {
 if (reduceResourceRequest.equals(Resources.none())) {
   return false; // no reduces
 }
 if (assignedRequests.maps.size() > 0) {
   // there are assigned mappers
   return false;
 }



 
 Flaky test AsyncResponseHandlerTest",No
13183125,"There are a number of javadoc warnings in trunk in classes under the nodemanager package. These should be addressed or suppressed.


[WARNING] Javadoc Warnings
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java:93: warning - Tag @see: reference not found: ContainerLaunch.ShellScriptBuilder#listDebugInformation
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:118: warning - YarnConfiguration#YARN_CONTAINER_SANDBOX (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:118: warning - YarnConfiguration#YARN_CONTAINER_SANDBOX_FILE_PERMISSIONS (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:118: warning - YarnConfiguration#YARN_CONTAINER_SANDBOX_POLICY (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:118: warning - YarnConfiguration#YARN_CONTAINER_SANDBOX_WHITELIST_GROUP (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:118: warning - YarnConfiguration#YARN_CONTAINER_SANDBOX_POLICY_GROUP_PREFIX (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:211: warning - YarnConfiguration#YARN_CONTAINER_SANDBOX_WHITELIST_GROUP (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/JavaSandboxLinuxContainerRuntime.java:211: warning - NMContainerPolicyUtils#SECURITY_FLAG (referenced by @value tag) is an unknown reference.
[WARNING] /testptch/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/TrafficControlBandwidthHandlerImpl.java:248: warning - @return tag has no arguments.

 
 Javadoc warnings within the nodemanager package",13182261,"Maven javadoc plugin doesn't support @value annotation, even though IntelliJ works.  There are only ~12 instances that need to be removed.  It is probably better to remove them before this snowball into a problem. 
 Remove @value javadoc annotation from YARN projects",yes
13266598,"for INodeReferences , space consumed was different in QuotaUsage and Content Summary 
 INodeReference Space Consumed was not same in QuotaUsage and ContentSummary",13220103,"GC thrash from an unexpected source in ReduceSinkOperator.


org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinarySerializeWrite.resetWithoutOutput(LazyBinarySerializeWrite.java:136)
        at org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinarySerializeWrite.reset(LazyBinarySerializeWrite.java:132)
        at org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.process(VectorReduceSinkUniformHashOperator.java:180)


GC space is getting thrashed by the 


    root = new Field(STRUCT);


for every row. 
 Vectorization: LazyBinarySerializeWrite allocates Field() within the loop",No
13148582,"HBASE-15965 Was trying to be helpful in reducing spam in console output printed back to the user.
However, Peter Somogyi and [~balazs.meszaros] made a good find that some shell commands (e.g. get_table) just don't work at all when the default return-values=false. This would be quite obtuse to try to explain.
Apekshit Sharma and I had mulled over whether we should flip the default from false to true. The above example makes me sure that we should for 2.0.0.
fyi Michael Stack 
 Flip default ""return-values"" in shell from false to true",13147649,"Feedback from Mike Drob on HBASE-19158:

Shell:
HBASE-19770. There was another issue opened where this was identified as a problem so maybe the shape will change further, but I can't find it now.
New commentary from Sean Busbey:
This was a follow on to HBASE-15965. That change effectively makes it so none of our ruby wrappers can be used to build expressions in an interactive REPL. This is a pretty severe change (most of my tips on HBASE-15611 will break, I think).
I think we should
a) Have a DISCUSS thread, spanning dev@ and user@
b) based on the outcome of that thread, either default to the new behavior or the old behavior
c) if we keep the HBASE-15965 behavior as  the default, flag it as incompatible, call it out in the hbase 2.0 upgrade section, and update docs (two examples: the output in the shell_exercises sections would be wrong, and the _table_variables section won't work)
d) In either case document the new flag in the ref guide 
 [shell] Revert shell REPL change and document",yes
13173277,"Fix log message in Client class.
Currently:

LOG.debug(""{} set to true. Will bind client sockets to wildcard ""
            + ""address."",
        CommonConfigurationKeys.IPC_CLIENT_BIND_WILDCARD_ADDR_KEY);

to

LOG.debug(""{} set to {}"", CommonConfigurationKeys.IPC_CLIENT_BIND_WILDCARD_ADDR_KEY, bindToWildCardAddress);
 
 Fix debug log for property IPC_CLIENT_BIND_WILDCARD_ADDR_KEY",13160124,"Fix debug log statement introduced in HADOOP-15250. 
 fix logging for split-dns multihome ",yes
13155515,"Now that we've branched for 3.0 we need to create SQL install and upgrade scripts for 3.1 
 Create metastore SQL install and upgrade scripts for 3.1",13151214,"Now that branch for hive 3.0.0 is cut and we have started preparing for hive 3.1.0 development we need to add metastore upgrade scripts to upgrade from 3.0.0 to 3.1.0 
 Update metastore upgrade scripts to prepare for 3.1.0 development",yes
13226813,"The start-build-env.sh creates a Docker image and creates a user within it which maps to the user ID from the host. In the case where the host UID is very large (> 1 billion or so, not uncommon in large AD deployments), the resultant image fails to build due to /var/log/lastlog and /var/log/faillog growing to consume all available disk space.
These files are not necessary for the build process and if they do not exist, they will not be grown.
 
 OptimizedSql is not shown when the expression contains CONCAT",13149648,"
hdfs dfs -ls


this is happeningbecausegetMountPointDates is not implemented


private Map<String, Long> getMountPointDates(String path) {
Map<String, Long> ret = new TreeMap<>();
// TODO add when we have a Mount Table
return ret;
}

 
 RBF: Wrong date information in list file(-ls) result",No
13181454,"Adding ALLOWSNAPSHOT and DISALLOWSNAPSHOT (since 2.8.0, HDFS-9057) to WebHDFS REST API doc.
Below are my examples of the APIs:


# ALLOWSNAPSHOT uses http method PUT.
curl -X ""PUT"" ""http://<SERVER>:<PORT>/webhdfs/v1/snaptest/?op=ALLOWSNAPSHOT&user.name=hdfs""

Response on success:

HTTP/1.1 200 OK
Content-Type: application/octet-stream




# DISALLOWSNAPSHOT uses http method PUT.
curl -X ""PUT"" ""http://<SERVER>:<PORT>/webhdfs/v1/snaptest/?op=DISALLOWSNAPSHOT&user.name=hdfs""

Response on success:

HTTP/1.1 200 OK
Content-Type: application/octet-stream


Note: GETSNAPSHOTDIFF and GETSNAPSHOTTABLEDIRECTORYLIST are already documented.


# GETSNAPSHOTDIFF uses GET.
curl ""http://<SERVER>:<PORT>/webhdfs/v1/snaptest/?op=GETSNAPSHOTDIFF&user.name=hdfs&oldsnapshotname=snap1&snapshotname=snap2""

Response on success (example):

HTTP/1.1 200 OK
Content-Type: application/json

{""SnapshotDiffReport"":{""diffList"":[{""sourcePath"":"""",""type"":""MODIFY""},{""sourcePath"":""newfile.txt"",""type"":""CREATE""}],""fromSnapshot"":""snapOld"",""snapshotRoot"":""/snaptest"",""toSnapshot"":""snapNew""}}




# GETSNAPSHOTTABLEDIRECTORYLIST uses GET.
curl ""http://<SERVER>:<PORT>/webhdfs/v1/snaptest/?op=GETSNAPSHOTTABLEDIRECTORYLIST&user.name=hdfs""

Response on success (example):

HTTP/1.1 200 OK
Content-Type: application/json

{""SnapshottableDirectoryList"":[{""dirStatus"":{""accessTime"":0,""blockSize"":0,""childrenNum"":0,""fileId"":16392,""group"":""supergroup"",""length"":0,""modificationTime"":1535151813500,""owner"":""hdfs"",""pathSuffix"":""snaptest"",""permission"":""755"",""replication"":0,""storagePolicy"":0,""type"":""DIRECTORY""},""parentFullPath"":""/"",""snapshotNumber"":2,""snapshotQuota"":65536}]}

 
 WebHDFS: Document ALLOWSNAPSHOT and DISALLOWSNAPSHOT API doc",13138970,"There is no Document for Allow/Disallow snapshots.
http://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/WebHDFS.html 
 Webhdfs : update the Document for allow/disallow snapshots",yes
