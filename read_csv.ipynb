{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of duplicated pairs in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "import sqlite3\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import sys\n",
    "from itertools import combinations\n",
    "import random\n",
    "import my_utils\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = './datasets/dup_br_detection_data/dup_train.csv'\n",
    "\n",
    "# Open the CSV file and create a CSV reader\n",
    "labels = []\n",
    "with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    # for row in csv_reader:\n",
    "    #     print(row)\n",
    "    line = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        labels += [row[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89027"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.count(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33270"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.count(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122298"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = './datasets/dup_br_detection_data/dup_test.csv'\n",
    "\n",
    "# Open the CSV file and create a CSV reader\n",
    "labels = []\n",
    "with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    # for row in csv_reader:\n",
    "    #     print(row)\n",
    "    line = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        labels += [row[4]]\n",
    "        line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = './datasets/dup_br_detection_data/dup_valid.csv'\n",
    "\n",
    "# Open the CSV file and create a CSV reader\n",
    "labels = []\n",
    "with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    # for row in csv_reader:\n",
    "    #     print(row)\n",
    "    line = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        labels += [row[4]]\n",
    "        line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15288"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"./dbrd.db\"\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "column_names = my_utils.get_column_names(conn, \"spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9579/9579 [00:00<00:00, 1806368.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 28041.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark has 513.0 duplicated_pairs\n",
      "Processing eclipse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27583/27583 [00:00<00:00, 1696505.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 959/959 [00:00<00:00, 9370.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse has 2792.0 duplicated_pairs\n",
      "Processing eclipse_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74376/74376 [00:00<00:00, 1600929.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3254/3254 [00:01<00:00, 2514.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_old has 28866.0 duplicated_pairs\n",
      "Processing hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14016/14016 [00:00<00:00, 1869648.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 31145.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop has 431.0 duplicated_pairs\n",
      "Processing hadoop_1day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14016/14016 [00:00<00:00, 1921909.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 27255.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop_1day has 431.0 duplicated_pairs\n",
      "Processing hadoop_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24083/24083 [00:00<00:00, 1793622.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 905/905 [00:00<00:00, 11204.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop_old has 1504.0 duplicated_pairs\n",
      "Processing kibana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17016/17016 [00:00<00:00, 1842146.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 388/388 [00:00<00:00, 24795.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kibana has 605.0 duplicated_pairs\n",
      "Processing mozilla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193587/193587 [00:00<00:00, 1488905.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 10702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10702/10702 [00:15<00:00, 695.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mozilla has 86787.0 duplicated_pairs\n",
      "Processing mozilla_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338155/338155 [00:00<00:00, 1536333.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 21554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21554/21554 [00:56<00:00, 381.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mozilla_old has 66116.0 duplicated_pairs\n",
      "Processing spark_1day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9579/9579 [00:00<00:00, 1779880.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 34396.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_1day has 513.0 duplicated_pairs\n",
      "Processing vscode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62092/62092 [00:00<00:00, 1634754.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2342/2342 [00:00<00:00, 3372.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vscode has 13931.0 duplicated_pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_dups = 0\n",
    "\n",
    "for table in my_utils.table_names:\n",
    "    union_find = my_utils.UnionFind()\n",
    "    union_find.process_project(conn, table, column_names)\n",
    "    \n",
    "    roots = union_find.get_roots()\n",
    "    print(\"Num roots\", len(roots))\n",
    "    num_dups += union_find.num_dups()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_find.processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202489.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Hadoop data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14016/14016 [00:00<00:00, 1756314.68it/s]\n"
     ]
    }
   ],
   "source": [
    "project_name = \"hadoop\"\n",
    "union_find = my_utils.UnionFind()\n",
    "union_find.process_project(conn, project_name, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_pairs = union_find.get_duplicated_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_pairs = my_utils.get_non_duplicated_pairs(union_find, conn, len(duplicated_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 862/862 [00:00<00:00, 3897.20it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for bug_id1, bug_id2 in tqdm(duplicated_pairs+non_duplicated_pairs):\n",
    "    text1 = my_utils.get_descriptions(conn, project_name, bug_id1)\n",
    "    text2 = my_utils.get_descriptions(conn, project_name, bug_id2)\n",
    "    label = \"yes\" if union_find.are_dups(bug_id1, bug_id2) else \"No\"\n",
    "    dataset += [(bug_id1, text1, bug_id2, text2, label)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_valid = train_test_split(dataset, train_size=0.8, random_state=42)\n",
    "test, valid = train_test_split(test_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder \"./datasets/dup_br_data\" has been created.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./datasets/dup_br_data\"\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    # If the folder doesn't exist, create it\n",
    "    os.makedirs(folder_path)\n",
    "    print(f'The folder \"{folder_path}\" has been created.')\n",
    "else:\n",
    "    print(f'The folder \"{folder_path}\" already exists.')\n",
    "\n",
    "file_path = os.path.join(folder_path, f'{project_name}_train.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in train:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "file_path = os.path.join(folder_path, f'{project_name}_test.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in test:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "                \n",
    "file_path = os.path.join(folder_path, f'{project_name}_test.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in test:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
