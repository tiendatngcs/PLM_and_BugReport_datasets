{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of duplicated pairs in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/t/tiendat.ng.cs/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "import sqlite3\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import sys\n",
    "from itertools import combinations\n",
    "import random\n",
    "import my_utils\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = './datasets/dup_br_detection_data/dup_train.csv'\n",
    "\n",
    "# Open the CSV file and create a CSV reader\n",
    "labels = []\n",
    "with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    # for row in csv_reader:\n",
    "    #     print(row)\n",
    "    line = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        labels += [row[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89027"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.count(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33270"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.count(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122298"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = './datasets/dup_br_detection_data/dup_test.csv'\n",
    "\n",
    "# Open the CSV file and create a CSV reader\n",
    "labels = []\n",
    "with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    # for row in csv_reader:\n",
    "    #     print(row)\n",
    "    line = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        labels += [row[4]]\n",
    "        line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15289"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = './datasets/dup_br_detection_data/dup_valid.csv'\n",
    "\n",
    "# Open the CSV file and create a CSV reader\n",
    "labels = []\n",
    "with open(csv_file_path, 'r', newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    # for row in csv_reader:\n",
    "    #     print(row)\n",
    "    line = 0\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        labels += [row[4]]\n",
    "        line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15288"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"./dbrd.db\"\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "column_names = my_utils.get_column_names(conn, \"spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9579/9579 [00:00<00:00, 1806368.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 28041.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark has 513.0 duplicated_pairs\n",
      "Processing eclipse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27583/27583 [00:00<00:00, 1696505.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 959/959 [00:00<00:00, 9370.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse has 2792.0 duplicated_pairs\n",
      "Processing eclipse_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74376/74376 [00:00<00:00, 1600929.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3254/3254 [00:01<00:00, 2514.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_old has 28866.0 duplicated_pairs\n",
      "Processing hadoop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14016/14016 [00:00<00:00, 1869648.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 31145.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop has 431.0 duplicated_pairs\n",
      "Processing hadoop_1day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14016/14016 [00:00<00:00, 1921909.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 27255.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop_1day has 431.0 duplicated_pairs\n",
      "Processing hadoop_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24083/24083 [00:00<00:00, 1793622.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 905/905 [00:00<00:00, 11204.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop_old has 1504.0 duplicated_pairs\n",
      "Processing kibana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17016/17016 [00:00<00:00, 1842146.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 388/388 [00:00<00:00, 24795.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kibana has 605.0 duplicated_pairs\n",
      "Processing mozilla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193587/193587 [00:00<00:00, 1488905.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 10702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10702/10702 [00:15<00:00, 695.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mozilla has 86787.0 duplicated_pairs\n",
      "Processing mozilla_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338155/338155 [00:00<00:00, 1536333.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 21554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21554/21554 [00:56<00:00, 381.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mozilla_old has 66116.0 duplicated_pairs\n",
      "Processing spark_1day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9579/9579 [00:00<00:00, 1779880.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 34396.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_1day has 513.0 duplicated_pairs\n",
      "Processing vscode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62092/62092 [00:00<00:00, 1634754.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num roots 2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2342/2342 [00:00<00:00, 3372.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vscode has 13931.0 duplicated_pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_dups = 0\n",
    "\n",
    "for table in my_utils.table_names:\n",
    "    union_find = my_utils.UnionFind()\n",
    "    union_find.process_project(conn, table, column_names)\n",
    "    \n",
    "    roots = union_find.get_roots()\n",
    "    print(\"Num roots\", len(roots))\n",
    "    num_dups += union_find.num_dups()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_find.processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202489.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Hadoop data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hadoop_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24083/24083 [00:00<00:00, 1496886.88it/s]\n"
     ]
    }
   ],
   "source": [
    "project_name = \"hadoop\"\n",
    "union_find = my_utils.UnionFind()\n",
    "union_find.process_project(conn, project_name, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_pairs = union_find.get_duplicated_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_pairs = my_utils.get_non_duplicated_pairs(union_find, conn, len(duplicated_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3008/3008 [00:00<00:00, 3451.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for bug_id1, bug_id2 in tqdm(duplicated_pairs+non_duplicated_pairs):\n",
    "    text1 = my_utils.get_descriptions(conn, project_name, bug_id1)\n",
    "    text2 = my_utils.get_descriptions(conn, project_name, bug_id2)\n",
    "    label = \"yes\" if union_find.are_dups(bug_id1, bug_id2) else \"No\"\n",
    "    dataset += [(bug_id1, text1, bug_id2, text2, label)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3008"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_valid = train_test_split(dataset, train_size=0.8, random_state=42)\n",
    "test, valid = train_test_split(test_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2406"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder \"./datasets/dup_br_data\" already exists.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./datasets/dup_br_data\"\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    # If the folder doesn't exist, create it\n",
    "    os.makedirs(folder_path)\n",
    "    print(f'The folder \"{folder_path}\" has been created.')\n",
    "else:\n",
    "    print(f'The folder \"{folder_path}\" already exists.')\n",
    "\n",
    "file_path = os.path.join(folder_path, f'{project_name}_train.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in train:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "file_path = os.path.join(folder_path, f'{project_name}_test.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in test:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "                \n",
    "file_path = os.path.join(folder_path, f'{project_name}_valid.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in valid:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Hadoop old data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hadoop_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24083/24083 [00:00<00:00, 1666580.16it/s]\n"
     ]
    }
   ],
   "source": [
    "project_name = \"hadoop_old\"\n",
    "union_find = my_utils.UnionFind()\n",
    "union_find.process_project(conn, project_name, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_pairs = union_find.get_duplicated_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_duplicated_pairs = my_utils.get_non_duplicated_pairs(union_find, conn, len(duplicated_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3008/3008 [00:00<00:00, 3931.44it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for bug_id1, bug_id2 in tqdm(duplicated_pairs+non_duplicated_pairs):\n",
    "    text1 = my_utils.get_descriptions(conn, project_name, bug_id1)\n",
    "    text2 = my_utils.get_descriptions(conn, project_name, bug_id2)\n",
    "    label = \"yes\" if union_find.are_dups(bug_id1, bug_id2) else \"No\"\n",
    "    dataset += [(bug_id1, text1, bug_id2, text2, label)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3008"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_valid = train_test_split(dataset, train_size=0.8, random_state=42)\n",
    "test, valid = train_test_split(test_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2406"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder \"./datasets/dup_br_data\" already exists.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./datasets/dup_br_data\"\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    # If the folder doesn't exist, create it\n",
    "    os.makedirs(folder_path)\n",
    "    print(f'The folder \"{folder_path}\" has been created.')\n",
    "else:\n",
    "    print(f'The folder \"{folder_path}\" already exists.')\n",
    "\n",
    "file_path = os.path.join(folder_path, f'{project_name}_train.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in train:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "file_path = os.path.join(folder_path, f'{project_name}_test.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in test:\n",
    "        csv_out.writerow(row)\n",
    "        \n",
    "                \n",
    "file_path = os.path.join(folder_path, f'{project_name}_valid.csv')\n",
    "with open(file_path,'w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['bug_id1','bug1','bug_id2','bug2','label'])\n",
    "    for row in valid:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of reports with code in each table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use bertOverflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 17:25:29.147787: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-15 17:25:29.150517: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-15 17:25:29.194473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-15 17:25:30.073116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerpipeline = pipeline(model=\"mrm8488/codebert-base-finetuned-stackoverflow-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9579 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9579/9579 [00:00<00:00, 1794187.38it/s]\n",
      "100%|██████████| 644/644 [01:09<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table spark has 0 reports with code out of \n",
      "Processing eclipse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27583/27583 [00:00<00:00, 1695138.20it/s]\n",
      "100%|██████████| 2406/2406 [03:42<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table eclipse has 0 reports with code out of \n",
      "Processing eclipse_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74376/74376 [00:00<00:00, 1589209.94it/s]\n",
      " 18%|█▊        | 1594/8987 [03:05<21:19,  5.78it/s]"
     ]
    }
   ],
   "source": [
    "for table in my_utils.table_names:\n",
    "    union_find = my_utils.UnionFind()\n",
    "    union_find.process_project(conn, table, column_names)\n",
    "    \n",
    "    bug_ids = []\n",
    "    roots = union_find.get_roots()\n",
    "    for root in roots:\n",
    "        bug_ids += union_find.get_children(root)\n",
    "    \n",
    "    reports_with_codes_count = 0\n",
    "    for bug_id in tqdm(bug_ids):\n",
    "        desc = my_utils.get_descriptions(conn, table, bug_id)\n",
    "        inf_out = nerpipeline(desc)\n",
    "        for entry in inf_out:\n",
    "            if \"Code_Block\" in entry[\"entity\"].lower():\n",
    "                reports_with_codes_count += 1\n",
    "                break\n",
    "    print(f\"Table {table} has {reports_with_codes_count} reports with code out of \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
