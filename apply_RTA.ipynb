{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AutoTokenizer, AutoModel\n",
    "\n",
    "import sqlite3\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import sys\n",
    "from itertools import combinations\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}  # Dictionary to store parent nodes\n",
    "        self.ranks = {}    # Dictionary to store rank (or size) of each set\n",
    "        self.processed = False\n",
    "        self.project_name = None\n",
    "\n",
    "    def find(self, x):\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.ranks[x] = 1\n",
    "            return x\n",
    "\n",
    "        # Path compression\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "\n",
    "        if root_x != root_y:\n",
    "            if self.ranks[root_x] < self.ranks[root_y]:\n",
    "                self.parent[root_x] = root_y\n",
    "                self.ranks[root_y] += self.ranks[root_x]\n",
    "            else:\n",
    "                self.parent[root_y] = root_x\n",
    "                self.ranks[root_x] += self.ranks[root_y]\n",
    "            \n",
    "    def process_project(self, conn, project_name):\n",
    "        cursor = conn.cursor()\n",
    "        self.project_name = project_name\n",
    "        \n",
    "        cursor.execute(f\"SELECT * FROM {project_name}\")\n",
    "        for row in cursor.fetchall():\n",
    "            dup_id = int(row[column_names.index(\"dup_id\")])\n",
    "            if dup_id == -1: continue\n",
    "            bug_id = int(row[column_names.index(\"bug_id\")])\n",
    "            if (dup_id == bug_id): continue\n",
    "            assert(dup_id != bug_id)\n",
    "            self.union(bug_id, dup_id)\n",
    "        self.processed = True\n",
    "            \n",
    "    def get_roots(self,):\n",
    "        assert(self.processed)\n",
    "        return list(set(self.parent.values()))\n",
    "    \n",
    "    def get_children(self, parent):\n",
    "        assert(self.processed)\n",
    "        parent = self.find(parent)\n",
    "        children = [key for key, value in self.parent.items() if value == parent]\n",
    "        return children\n",
    "    \n",
    "    def get_all_children(self, ):\n",
    "        return [key for key, value in self.parent.items()]\n",
    "    \n",
    "    def are_dups(this, bug_id1, bug_id2):\n",
    "        if (bug_id1 not in this.parent.keys() or bug_id2 not in this.parent.keys()):\n",
    "            return False\n",
    "        return this.parent[bug_id1] == this.parent[bug_id2]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bug_ids(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "    column_name = \"bug_id\"\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    cursor.execute(f\"SELECT DISTINCT {column_name} FROM {table_name} ORDER BY {column_name};\")\n",
    "    distinct_values_sorted = cursor.fetchall()\n",
    "\n",
    "    # Extract table names from the result\n",
    "    return [value[0] for value in distinct_values_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names(conn, table_name):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a query to get information about the columns in the specified table\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    columns_info = cursor.fetchall()\n",
    "\n",
    "    # Extract and return the column names\n",
    "    column_names = [column[1] for column in columns_info]\n",
    "    return column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_feature(conn, project_name, bug_id):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    query = f\"SELECT * FROM {project_name} WHERE bug_id = {bug_id};\"\n",
    "    # print(query)\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()[0]\n",
    "    return result[column_names.index(\"code_feature\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptions(conn, project_name, bug_id):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    query = f\"SELECT * FROM {project_name} WHERE bug_id = {bug_id};\"\n",
    "    # print(query)\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()[0]\n",
    "    desc = result[column_names.index(\"description\")]\n",
    "    short_desc = result[column_names.index(\"short_desc\")]\n",
    "\n",
    "    # Extract table names from the result\n",
    "    return (desc + \" \\n \" + short_desc).replace(\"\\\\'\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_feature_from_db(conn, project_name, bug_id, selected_columns):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch table names using SQL query\n",
    "    query = f\"SELECT * FROM {project_name} WHERE bug_id = {bug_id};\"\n",
    "    # print(query)\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()[0]\n",
    "    ret = \"\"\n",
    "    for selected_column in selected_columns:\n",
    "        ret += result[column_names.index(selected_columns)] + \" \"\n",
    "\n",
    "    # Extract table names from the result\n",
    "    return ret.replace(\"\\\\'\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(description, stride_len, chunk_size):\n",
    "    tokens = tokenizer.tokenize(description)\n",
    "    # if len og token array is < 32, we do nothing as there is not enough information\n",
    "    if (len(tokens) < chunk_size // 2): return None\n",
    "\n",
    "    # remember to add cls and sep token at each chunk\n",
    "    token_ids = tokenizer.convert_tokens_to_ids([tokenizer.cls_token]+tokens+[tokenizer.sep_token])\n",
    "\n",
    "    # divide token ids into batche of chunks\n",
    "    chunk_list=[]\n",
    "    for i in range(0, len(token_ids), stride_len):\n",
    "        chunk = token_ids[i:min(i+chunk_size, len(token_ids))]\n",
    "        assert(len(chunk) <= chunk_size)\n",
    "        if len(chunk) < chunk_size:\n",
    "            # keep going\n",
    "            continue\n",
    "            # if (len(chunk) < chunk_size // 2): continue\n",
    "            # pad_length = chunk_size - len(chunk)\n",
    "            # chunk += [tokenizer.pad_token_id]*pad_length\n",
    "        assert(len(chunk) == chunk_size)\n",
    "        # print(chunk)\n",
    "        chunk_list.append(chunk)\n",
    "\n",
    "    if(len(chunk_list) == 0): return None\n",
    "    chunk_arr = np.array(chunk_list)\n",
    "    # print(\"Chunk arr size{}\".format(chunk_arr.shape))\n",
    "    # context_embedding = model(torch.tensor(token_ids[:512])[None, :])[0]\n",
    "    context_embedding = model(torch.tensor(chunk_arr)[:, :])[0]\n",
    "    return context_embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicated_pairs(union_find):\n",
    "    roots = union_find.get_roots()\n",
    "    pairs = []\n",
    "    for root in roots:\n",
    "        group = union_find.get_children(root)\n",
    "        pairs += list(combinations(group, 2))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_duplicated_pairs(union_find, conn, size):\n",
    "    from_dup = union_find.get_all_children()\n",
    "    #sample in some other single reports\n",
    "    assert(union_find.processed)\n",
    "    samples = random.sample(get_bug_ids(conn, union_find.project_name), len(from_dup))\n",
    "    \n",
    "    pairs = []\n",
    "    count = 0\n",
    "    while (count < size):\n",
    "        pair = random.sample(samples, 2)\n",
    "        if pair[0] == pair[1] or union_find.are_dups(pair[0], pair[1]):\n",
    "            continue\n",
    "        pairs += [(pair[0], pair[1]),]\n",
    "        count += 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mislabels(union_find, bug_ids, anchor_bug_id, threshold):\n",
    "    assert(threshold >= 0 and threshold <= 1)\n",
    "    ret = []\n",
    "    for bug_id in tqdm(bug_ids):\n",
    "        if not union_find.are_dups(anchor_bug_id, bug_id):\n",
    "            sim_score = get_similarity_of_pair((anchor_bug_id, bug_id),)\n",
    "            if sim_score > threshold:\n",
    "                ret += [bug_id]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"./dbrd_w_tech_terms.db\"\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(database_path)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"eclipse\"\n",
    "\n",
    "bug_ids = get_bug_ids(conn, project_name)\n",
    "\n",
    "column_names = get_column_names(conn, project_name)\n",
    "\n",
    "union_find = UnionFind()\n",
    "union_find.process_project(conn, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_set = get_duplicated_pairs(union_find)\n",
    "positive_labels = [1]*len(positive_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_set = get_non_duplicated_pairs(union_find, conn, len(positive_set)*3)\n",
    "negative_labels = [0]*len(negative_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2792 8376\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_set), len(negative_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out training set is 80% of positive set and 80% of negative set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(positive_set+negative_set, positive_labels+negative_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8934"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2234"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 349/349 [00:00<00:00, 30.3kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 6.20MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 15.3MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 42.0MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 130kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 685/685 [00:00<00:00, 329kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 499M/499M [00:09<00:00, 54.6MB/s] \n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at Colorful/RTA and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Colorful/RTA\")\n",
    "model = AutoModel.from_pretrained(\"Colorful/RTA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class STSBertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(STSBertModel, self).__init__()\n",
    "\n",
    "        word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=128)\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "        self.sts_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    def forward(self, input_data):\n",
    "\n",
    "        output = self.sts_model(input_data)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X_train, y_train, selected_columns):\n",
    "\n",
    "        # similarity = [i['similarity_score'] for i in dataset]\n",
    "        self.label = y_train\n",
    "        self.sentence_1 = [get_text_feature_from_db(conn, project_name, bug_id1, selected_columns) for (bug_id1, bug_id2) in X_train]\n",
    "        self.sentence_2 = [get_text_feature_from_db(conn, project_name, bug_id2, selected_columns) for (bug_id1, bug_id2) in X_train]\n",
    "        self.text_cat = [[str(x), str(y)] for x,y in zip(self.sentence_1, self.sentence_2)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.text_cat)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.tensor(self.label[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "\n",
    "        return tokenizer(self.text_cat[idx], padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "\n",
    "def collate_fn(texts):\n",
    "\n",
    "  num_texts = len(texts['input_ids'])\n",
    "  features = list()\n",
    "  for i in range(num_texts):\n",
    "      features.append({'input_ids':texts['input_ids'][i], 'attention_mask':texts['attention_mask'][i]})\n",
    "  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
